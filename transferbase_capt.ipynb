{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFNeZ3QnzJrt",
        "outputId": "06bd1ce3-c660-464d-a9cf-6ae76e3e3b5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m961.5/961.5 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# STEP 1️⃣: Mount Drive and Install Libraries\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Install required libraries\n",
        "!pip install -q nltk torchmetrics\n",
        "\n",
        "# Download punkt tokenizer for BLEU (only once)\n",
        "import nltk\n",
        "nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Load Vocabulary and Dataset\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class VideoCaptionDataset(Dataset):\n",
        "    def __init__(self, feature_dir, json_path, vocab, max_caption_len=45):\n",
        "        self.feature_dir = feature_dir\n",
        "        self.vocab = vocab\n",
        "        self.max_len = max_caption_len\n",
        "\n",
        "        with open(json_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        self.video_ids = [v['video_id'] for v in data['videos']]\n",
        "        self.captions_map = {}\n",
        "        for s in data['sentences']:\n",
        "            self.captions_map.setdefault(s['video_id'], []).append(s['caption'])\n",
        "\n",
        "        # Filter only those video_ids that have feature file\n",
        "        self.video_ids = [\n",
        "            vid for vid in self.video_ids if os.path.exists(os.path.join(feature_dir, f\"{vid}.npy\"))\n",
        "        ]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        vid = self.video_ids[idx]\n",
        "        feat_path = os.path.join(self.feature_dir, f\"{vid}.npy\")\n",
        "        video_feat = torch.tensor(np.load(feat_path), dtype=torch.float32)\n",
        "\n",
        "        caption = np.random.choice(self.captions_map[vid])\n",
        "        tokens = [self.vocab['<SOS>']] + [\n",
        "            self.vocab.get(w, self.vocab['<UNK>']) for w in caption.lower().split()\n",
        "        ] + [self.vocab['<EOS>']]\n",
        "\n",
        "        if len(tokens) < self.max_len:\n",
        "            tokens += [self.vocab['<PAD>']] * (self.max_len - len(tokens))\n",
        "        else:\n",
        "            tokens = tokens[:self.max_len]\n",
        "\n",
        "        caption_tensor = torch.tensor(tokens, dtype=torch.long)\n",
        "        return video_feat, caption_tensor\n"
      ],
      "metadata": {
        "id": "S5vIAT3704LW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load Vocab and Create Dataloaders\n",
        "# Load vocab\n",
        "vocab_path = '/content/drive/MyDrive/msvd_split/vocab.json'\n",
        "with open(vocab_path, 'r') as f:\n",
        "    vocab = json.load(f)\n",
        "\n",
        "# Dataset paths\n",
        "train_feature_dir = '/content/drive/MyDrive/msvd_split/train/features'\n",
        "train_json = '/content/drive/MyDrive/msvd_split/train/train_captions.json'\n",
        "\n",
        "val_feature_dir = '/content/drive/MyDrive/msvd_split/val/features'\n",
        "val_json = '/content/drive/MyDrive/msvd_split/val/val_captions.json'\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = VideoCaptionDataset(train_feature_dir, train_json, vocab)\n",
        "val_dataset = VideoCaptionDataset(val_feature_dir, val_json, vocab)\n",
        "\n",
        "# Create loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "print(f\"✅ Train size: {len(train_dataset)}, Val size: {len(val_dataset)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFzBoll51yQP",
        "outputId": "49bba778-8c65-45ad-c678-2585ed24cfa5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Train size: 1251, Val size: 91\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 3: Define Transformer Decoder Model\n",
        "# Full Code Block for the Model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "# Positional Encoding\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)  # shape: [1, max_len, d_model]\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: [B, L, d_model]\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "# Transformer-based Decoder\n",
        "class VideoTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=512, nhead=8, num_layers=6, dim_ff=2048,\n",
        "                 max_len=45, dropout=0.1, input_feat_dim=2048):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.max_len = max_len\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Embedding\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_enc = PositionalEncoding(d_model, dropout, max_len)\n",
        "\n",
        "        # Project video feature dim to model dim\n",
        "        self.vid_fc = nn.Linear(input_feat_dim, d_model)\n",
        "\n",
        "        # Transformer decoder\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model, nhead, dim_ff, dropout, batch_first=True)\n",
        "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers)\n",
        "\n",
        "        # Output projection\n",
        "        self.output = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, video_feats, captions):\n",
        "        \"\"\"\n",
        "        video_feats: [B, T, 2048]\n",
        "        captions: [B, L] (tokenized)\n",
        "        \"\"\"\n",
        "        B, T, _ = video_feats.shape\n",
        "        L = captions.shape[1]\n",
        "\n",
        "        # Encode video\n",
        "        memory = self.vid_fc(video_feats)  # [B, T, d_model]\n",
        "\n",
        "        # Embed target captions\n",
        "        tgt_emb = self.embedding(captions) * math.sqrt(self.d_model)\n",
        "        tgt_emb = self.pos_enc(tgt_emb)  # [B, L, d_model]\n",
        "\n",
        "        # Generate subsequent mask for causal attention\n",
        "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(L).to(captions.device)\n",
        "\n",
        "        out = self.decoder(tgt=tgt_emb, memory=memory, tgt_mask=tgt_mask)\n",
        "        logits = self.output(out)  # [B, L, vocab_size]\n",
        "        return F.log_softmax(logits, dim=-1)\n"
      ],
      "metadata": {
        "id": "STgu9dJA1Ehy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Create folder if not exists\n",
        "folder = '/content/drive/MyDrive/transformer_model'\n",
        "os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "# Save model file\n",
        "path = os.path.join(folder, 'video_transformer.py')\n",
        "with open(path, 'w') as f:\n",
        "    f.write(model_code)\n",
        "\n",
        "print(f\"✅ Saved Transformer model class to: {path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkUGN6k5GUW0",
        "outputId": "316d7cb6-d005-4894-a240-048c8a972c59"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved Transformer model class to: /content/drive/MyDrive/transformer_model/video_transformer.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_code = '''import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class VideoTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=512, nhead=8, num_layers=6, dim_ff=2048,\n",
        "                 max_len=45, dropout=0.1, input_feat_dim=2048):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.max_len = max_len\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_enc = PositionalEncoding(d_model, dropout, max_len)\n",
        "        self.vid_fc = nn.Linear(input_feat_dim, d_model)\n",
        "\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model, nhead, dim_ff, dropout, batch_first=True)\n",
        "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers)\n",
        "        self.output = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, video_feats, captions):\n",
        "        B, T, _ = video_feats.shape\n",
        "        L = captions.shape[1]\n",
        "\n",
        "        memory = self.vid_fc(video_feats)\n",
        "        tgt_emb = self.embedding(captions) * math.sqrt(self.d_model)\n",
        "        tgt_emb = self.pos_enc(tgt_emb)\n",
        "\n",
        "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(L).to(captions.device)\n",
        "        out = self.decoder(tgt=tgt_emb, memory=memory, tgt_mask=tgt_mask)\n",
        "        logits = self.output(out)\n",
        "        return F.log_softmax(logits, dim=-1)\n",
        "'''\n",
        "\n",
        "# 🔽 Save to Drive\n",
        "path = '/content/drive/MyDrive/transformer_model/video_transformer.py'\n",
        "with open(path, 'w') as f:\n",
        "    f.write(model_code)\n",
        "\n",
        "print(f\"✅ Saved Transformer model class to: {path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCi4vsvfF3Il",
        "outputId": "d13a599d-4bb8-48ed-ca87-9e1a80c63f08"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved Transformer model class to: /content/drive/MyDrive/transformer_model/video_transformer.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/transformer_model')\n",
        "from video_transformer import VideoTransformer\n"
      ],
      "metadata": {
        "id": "I077U8JDGdg-"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sample Instantiation\n",
        "model = VideoTransformer(\n",
        "    vocab_size=len(vocab),\n",
        "    d_model=512,\n",
        "    nhead=8,\n",
        "    num_layers=3,\n",
        "    dim_ff=2048,\n",
        "    max_len=45,\n",
        "    dropout=0.1,\n",
        "    input_feat_dim=2048  # Since we're using ResNet/I3D features\n",
        ").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "7K-qokIK2J15"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 4: Training Loop with Label Smoothing\n",
        "# 1. Label Smoothing Loss Function\n",
        "def label_smoothing_loss(pred, target, vocab_size, smoothing=0.1, pad_idx=0):\n",
        "    \"\"\"\n",
        "    pred: [B*L, V]\n",
        "    target: [B*L]\n",
        "    \"\"\"\n",
        "    confidence = 1.0 - smoothing\n",
        "    true_dist = torch.zeros_like(pred)\n",
        "    true_dist.fill_(smoothing / (vocab_size - 2))\n",
        "    true_dist.scatter_(1, target.unsqueeze(1), confidence)\n",
        "    true_dist.masked_fill_((target == pad_idx).unsqueeze(1), 0)\n",
        "    pred = F.log_softmax(pred, dim=1)\n",
        "    loss = -torch.sum(true_dist * pred, dim=1)\n",
        "    mask = (target != pad_idx).float()\n",
        "    return torch.sum(loss * mask) / torch.sum(mask)\n"
      ],
      "metadata": {
        "id": "0ogR_hsU2NTq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Full Training Loop\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "def train_transformer(model, train_loader, val_loader, vocab,\n",
        "                      device, num_epochs=20, learning_rate=1e-4,\n",
        "                      checkpoint_dir='/content/drive/MyDrive/transformer_checkpoints'):\n",
        "\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
        "    pad_idx = vocab['<PAD>']\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        print(f\"\\n🔁 Epoch {epoch}/{num_epochs}\")\n",
        "\n",
        "        for video_feats, captions in tqdm(train_loader, desc=\"Training\"):\n",
        "            video_feats, captions = video_feats.to(device), captions.to(device)\n",
        "\n",
        "            inputs = captions[:, :-1]\n",
        "            targets = captions[:, 1:]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(video_feats, inputs)  # [B, L, V]\n",
        "            loss = label_smoothing_loss(\n",
        "                output.view(-1, output.size(-1)),\n",
        "                targets.reshape(-1),\n",
        "                vocab_size=len(vocab),\n",
        "                smoothing=0.1,\n",
        "                pad_idx=pad_idx\n",
        "            )\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "        print(f\"✅ Avg Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # === Validation ===\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for video_feats, captions in tqdm(val_loader, desc=\"Validating\"):\n",
        "                video_feats, captions = video_feats.to(device), captions.to(device)\n",
        "                inputs = captions[:, :-1]\n",
        "                targets = captions[:, 1:]\n",
        "                output = model(video_feats, inputs)\n",
        "                loss = label_smoothing_loss(\n",
        "                    output.view(-1, output.size(-1)),\n",
        "                    targets.reshape(-1),\n",
        "                    vocab_size=len(vocab),\n",
        "                    smoothing=0.1,\n",
        "                    pad_idx=pad_idx\n",
        "                )\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        print(f\"🧪 Avg Validation Loss: {avg_val_loss:.4f}\")\n",
        "        scheduler.step()\n",
        "\n",
        "        # Save checkpoint\n",
        "        ckpt_path = f\"{checkpoint_dir}/transformer_epoch_{epoch}.pt\"\n",
        "        torch.save(model.state_dict(), ckpt_path)\n",
        "        print(f\"💾 Saved model to {ckpt_path}\")\n"
      ],
      "metadata": {
        "id": "Q0FpsFUs2WtK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start Training\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train_transformer(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    vocab=vocab,\n",
        "    device=device,\n",
        "    num_epochs=20,\n",
        "    learning_rate=1e-4,\n",
        "    checkpoint_dir='/content/drive/MyDrive/transformer_checkpoints'\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRxbD0XZ2b1W",
        "outputId": "938d8bce-6f02-4076-e6af-26d56837014b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔁 Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 157/157 [09:19<00:00,  3.56s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Avg Training Loss: 5.9045\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 12/12 [00:39<00:00,  3.26s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧪 Avg Validation Loss: 5.4615\n",
            "💾 Saved model to /content/drive/MyDrive/transformer_checkpoints/transformer_epoch_1.pt\n",
            "\n",
            "🔁 Epoch 2/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 157/157 [00:08<00:00, 18.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Avg Training Loss: 4.9812\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 12/12 [00:00<00:00, 31.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧪 Avg Validation Loss: 4.8362\n",
            "💾 Saved model to /content/drive/MyDrive/transformer_checkpoints/transformer_epoch_2.pt\n",
            "\n",
            "🔁 Epoch 3/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 157/157 [00:08<00:00, 18.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Avg Training Loss: 4.7322\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 12/12 [00:00<00:00, 32.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧪 Avg Validation Loss: 4.6030\n",
            "💾 Saved model to /content/drive/MyDrive/transformer_checkpoints/transformer_epoch_3.pt\n",
            "\n",
            "🔁 Epoch 4/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 157/157 [00:07<00:00, 20.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Avg Training Loss: 4.5940\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 12/12 [00:00<00:00, 34.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧪 Avg Validation Loss: 4.5518\n",
            "💾 Saved model to /content/drive/MyDrive/transformer_checkpoints/transformer_epoch_4.pt\n",
            "\n",
            "🔁 Epoch 5/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 157/157 [00:08<00:00, 17.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Avg Training Loss: 4.4939\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 12/12 [00:00<00:00, 33.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧪 Avg Validation Loss: 4.2958\n",
            "💾 Saved model to /content/drive/MyDrive/transformer_checkpoints/transformer_epoch_5.pt\n",
            "\n",
            "🔁 Epoch 6/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 157/157 [00:08<00:00, 19.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Avg Training Loss: 4.3178\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 12/12 [00:00<00:00, 34.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧪 Avg Validation Loss: 4.2612\n",
            "💾 Saved model to /content/drive/MyDrive/transformer_checkpoints/transformer_epoch_6.pt\n",
            "\n",
            "🔁 Epoch 7/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 157/157 [00:08<00:00, 18.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Avg Training Loss: 4.3280\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 12/12 [00:00<00:00, 31.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧪 Avg Validation Loss: 4.3848\n",
            "💾 Saved model to /content/drive/MyDrive/transformer_checkpoints/transformer_epoch_7.pt\n",
            "\n",
            "🔁 Epoch 8/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 157/157 [00:08<00:00, 19.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Avg Training Loss: 4.2571\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 12/12 [00:00<00:00, 32.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧪 Avg Validation Loss: 4.4411\n",
            "💾 Saved model to /content/drive/MyDrive/transformer_checkpoints/transformer_epoch_8.pt\n",
            "\n",
            "🔁 Epoch 9/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 157/157 [00:08<00:00, 18.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Avg Training Loss: 4.2404\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 12/12 [00:00<00:00, 32.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧪 Avg Validation Loss: 4.3478\n",
            "💾 Saved model to /content/drive/MyDrive/transformer_checkpoints/transformer_epoch_9.pt\n",
            "\n",
            "🔁 Epoch 10/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 157/157 [00:07<00:00, 20.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Avg Training Loss: 4.1585\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 12/12 [00:00<00:00, 27.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧪 Avg Validation Loss: 4.2052\n",
            "💾 Saved model to /content/drive/MyDrive/transformer_checkpoints/transformer_epoch_10.pt\n",
            "\n",
            "🔁 Epoch 11/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 157/157 [00:08<00:00, 17.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Avg Training Loss: 4.1074\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 12/12 [00:00<00:00, 34.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧪 Avg Validation Loss: 4.1991\n",
            "💾 Saved model to /content/drive/MyDrive/transformer_checkpoints/transformer_epoch_11.pt\n",
            "\n",
            "🔁 Epoch 12/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 157/157 [00:08<00:00, 18.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Avg Training Loss: 4.1664\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 12/12 [00:00<00:00, 34.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧪 Avg Validation Loss: 4.3796\n",
            "💾 Saved model to /content/drive/MyDrive/transformer_checkpoints/transformer_epoch_12.pt\n",
            "\n",
            "🔁 Epoch 13/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 157/157 [00:08<00:00, 18.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Avg Training Loss: 4.1522\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 12/12 [00:00<00:00, 33.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧪 Avg Validation Loss: 4.2760\n",
            "💾 Saved model to /content/drive/MyDrive/transformer_checkpoints/transformer_epoch_13.pt\n",
            "\n",
            "🔁 Epoch 14/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 157/157 [00:07<00:00, 19.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Avg Training Loss: 4.0673\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 12/12 [00:00<00:00, 34.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧪 Avg Validation Loss: 4.4561\n",
            "💾 Saved model to /content/drive/MyDrive/transformer_checkpoints/transformer_epoch_14.pt\n",
            "\n",
            "🔁 Epoch 15/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 157/157 [00:08<00:00, 17.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Avg Training Loss: 4.0814\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 12/12 [00:00<00:00, 34.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧪 Avg Validation Loss: 4.1520\n",
            "💾 Saved model to /content/drive/MyDrive/transformer_checkpoints/transformer_epoch_15.pt\n",
            "\n",
            "🔁 Epoch 16/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 157/157 [00:08<00:00, 19.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Avg Training Loss: 4.0723\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 12/12 [00:00<00:00, 34.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧪 Avg Validation Loss: 4.1365\n",
            "💾 Saved model to /content/drive/MyDrive/transformer_checkpoints/transformer_epoch_16.pt\n",
            "\n",
            "🔁 Epoch 17/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 157/157 [00:08<00:00, 18.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Avg Training Loss: 4.1023\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 12/12 [00:00<00:00, 28.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧪 Avg Validation Loss: 4.2014\n",
            "💾 Saved model to /content/drive/MyDrive/transformer_checkpoints/transformer_epoch_17.pt\n",
            "\n",
            "🔁 Epoch 18/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 157/157 [00:08<00:00, 18.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Avg Training Loss: 4.1038\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 12/12 [00:00<00:00, 31.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧪 Avg Validation Loss: 4.2107\n",
            "💾 Saved model to /content/drive/MyDrive/transformer_checkpoints/transformer_epoch_18.pt\n",
            "\n",
            "🔁 Epoch 19/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 157/157 [00:08<00:00, 17.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Avg Training Loss: 3.9779\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 12/12 [00:00<00:00, 31.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧪 Avg Validation Loss: 4.2921\n",
            "💾 Saved model to /content/drive/MyDrive/transformer_checkpoints/transformer_epoch_19.pt\n",
            "\n",
            "🔁 Epoch 20/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 157/157 [00:08<00:00, 18.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Avg Training Loss: 3.9967\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 12/12 [00:00<00:00, 28.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧪 Avg Validation Loss: 4.0402\n",
            "💾 Saved model to /content/drive/MyDrive/transformer_checkpoints/transformer_epoch_20.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Replace XX with final epoch number if needed\n",
        "save_path = \"/content/drive/MyDrive/transformer_checkpoints/transformer_final.pt\"\n",
        "\n",
        "torch.save(model.state_dict(), save_path)\n",
        "print(f\"✅ Model saved at {save_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOUfyDhtFW9-",
        "outputId": "b8170147-9608-4e41-c776-442da73d168d"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model saved at /content/drive/MyDrive/transformer_checkpoints/transformer_final.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DgxxaqpqFf5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 5: Inference with Greedy Decoding\n",
        "#Greedy Decoding Function\n",
        "\n",
        "def greedy_decode(model, video_feat, vocab, max_len=45, device='cuda'):\n",
        "    \"\"\"\n",
        "    video_feat: [1, T, 2048] - single video\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    sos_id = vocab['<SOS>']\n",
        "    eos_id = vocab['<EOS>']\n",
        "    inv_vocab = {v: k for k, v in vocab.items()}\n",
        "\n",
        "    caption = [sos_id]\n",
        "    video_feat = video_feat.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_len):\n",
        "            input_tensor = torch.tensor(caption).unsqueeze(0).to(device)  # [1, L]\n",
        "            output = model(video_feat, input_tensor)  # [1, L, V]\n",
        "            next_word = output[0, -1].argmax(dim=-1).item()\n",
        "            caption.append(next_word)\n",
        "            if next_word == eos_id:\n",
        "                break\n",
        "\n",
        "    decoded = [inv_vocab.get(tok, '<UNK>') for tok in caption[1:] if tok != eos_id]\n",
        "    return ' '.join(decoded)\n"
      ],
      "metadata": {
        "id": "Unj_F2LK2yyc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = '/content/drive/MyDrive/msvd_split/test/features/_SNE2MYAotU_41_49.npy'\n",
        "video_feat = torch.tensor(np.load(video_path)).unsqueeze(0).float()\n",
        "\n",
        "# Generate caption\n",
        "caption = greedy_decode(model, video_feat, vocab, max_len=45, device=device)\n",
        "print(\"🎬 Generated Caption:\", caption)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOaBh3lU6sqE",
        "outputId": "2656d673-f884-4883-b582-fcd585370d07"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎬 Generated Caption: a monkey is eating a small animal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "feature_dir = '/content/drive/MyDrive/msvd_split/test/features'\n",
        "filename = np.random.choice([f for f in os.listdir(feature_dir) if f.endswith('.npy')])\n",
        "video_path = os.path.join(feature_dir, filename)\n",
        "\n",
        "video_feat = torch.tensor(np.load(video_path)).unsqueeze(0).float().to(device)\n",
        "caption = greedy_decode(model, video_feat, vocab, max_len=45, device=device)\n",
        "\n",
        "print(f\"🎬 File: {filename}\")\n",
        "print(\"📝 Caption:\", caption)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgYVJyhI6u9Y",
        "outputId": "69b398ed-b73a-4ce9-cb27-4da120d30239"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎬 File: NV6pq1W-I4g_7_16.npy\n",
            "📝 Caption: a woman is putting a piece of a piece of a woman\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1bwu0VbI61Wg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 6: BLEU Score Evaluation\n",
        "#BLEU Evaluation Code\n",
        "import os\n",
        "import json\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "def evaluate_bleu(model, feature_dir, caption_json, vocab, max_len=45, device='cuda'):\n",
        "    with open(caption_json, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    references = {}\n",
        "    for entry in data['sentences']:\n",
        "        vid = entry['video_id']\n",
        "        caption = entry['caption'].lower().split()\n",
        "        references.setdefault(vid, []).append(caption)\n",
        "\n",
        "    inv_vocab = {v: k for k, v in vocab.items()}\n",
        "    scores = []\n",
        "    smooth_fn = SmoothingFunction().method4\n",
        "\n",
        "    model.eval()\n",
        "    for vid in tqdm(references.keys(), desc=\"Evaluating BLEU\"):\n",
        "        feat_path = os.path.join(feature_dir, f\"{vid}.npy\")\n",
        "        if not os.path.exists(feat_path):\n",
        "            continue\n",
        "\n",
        "        video_feat = torch.tensor(np.load(feat_path)).unsqueeze(0).float().to(device)\n",
        "        pred = greedy_decode(model, video_feat, vocab, max_len, device)\n",
        "        pred_tokens = pred.lower().split()\n",
        "\n",
        "        bleu = sentence_bleu(references[vid], pred_tokens, smoothing_function=smooth_fn)\n",
        "        scores.append(bleu)\n",
        "\n",
        "    avg_bleu = sum(scores) / len(scores)\n",
        "    print(f\"\\n📊 Average BLEU-4 Score: {avg_bleu:.4f}\")\n",
        "    return avg_bleu\n"
      ],
      "metadata": {
        "id": "fg2VALqN3Lto"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run BLEU Evaluation\n",
        "feature_dir_test = '/content/drive/MyDrive/msvd_split/test/features'\n",
        "json_test = '/content/drive/MyDrive/msvd_split/test/test_captions.json'\n",
        "\n",
        "evaluate_bleu(\n",
        "    model=model,\n",
        "    feature_dir=feature_dir_test,\n",
        "    caption_json=json_test,\n",
        "    vocab=vocab,\n",
        "    max_len=45,\n",
        "    device=device\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5DYbKa93hqS",
        "outputId": "61fa6c7d-f70a-4b23-91c5-f4a8baacfd30"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating BLEU: 100%|██████████| 78/78 [00:13<00:00,  5.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Average BLEU-4 Score: 0.4351\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4350781727165359"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Save Predictions to JSON\n",
        "def save_predictions_to_json(model, feature_dir, output_path, vocab, max_len=45, device='cuda'):\n",
        "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "\n",
        "    inv_vocab = {v: k for k, v in vocab.items()}\n",
        "    predictions = {}\n",
        "\n",
        "    model.eval()\n",
        "    for filename in tqdm(os.listdir(feature_dir), desc=\"Saving Predictions\"):\n",
        "        if not filename.endswith('.npy'):\n",
        "            continue\n",
        "        vid = os.path.splitext(filename)[0]\n",
        "        feat_path = os.path.join(feature_dir, filename)\n",
        "\n",
        "        video_feat = torch.tensor(np.load(feat_path)).unsqueeze(0).float().to(device)\n",
        "        pred = greedy_decode(model, video_feat, vocab, max_len, device)\n",
        "        predictions[vid] = pred\n",
        "\n",
        "    with open(output_path, 'w') as f:\n",
        "        json.dump(predictions, f, indent=4)\n",
        "    print(f\"✅ Saved predictions to {output_path}\")\n"
      ],
      "metadata": {
        "id": "0WHtJsmd4R-M"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Usage\n",
        "save_predictions_to_json(\n",
        "    model=model,\n",
        "    feature_dir='/content/drive/MyDrive/msvd_split/test/features',\n",
        "    output_path='/content/drive/MyDrive/msvd_split/predictions_transformer.json',\n",
        "    vocab=vocab,\n",
        "    max_len=45,\n",
        "    device=device\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJAHQyjb4aHe",
        "outputId": "1c780594-e279-4dfe-ac4f-dd800d5c9624"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving Predictions: 100%|██████████| 548/548 [00:16<00:00, 32.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved predictions to /content/drive/MyDrive/msvd_split/predictions_transformer.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ppT-LWZc94w2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#MINTOR\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bazmd5Qn940S",
        "outputId": "04001f72-64d9-4404-a731-39edde1ed737"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/pycocoevalcap-1.2-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reuse from previous step\n",
        "refs = {\n",
        "    \"video123\": [\"a man is dancing\", \"a person is performing dance\"],\n",
        "    \"video456\": [\"a dog is running\", \"an animal is sprinting\"]\n",
        "}\n",
        "\n",
        "preds = {\n",
        "    \"video123\": [\"a man dancing\"],\n",
        "    \"video456\": [\"a dog is running\"]\n",
        "}\n"
      ],
      "metadata": {
        "id": "ktPboq7I-G5q"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.meteor_score import meteor_score\n",
        "\n",
        "def compute_meteor_score(refs, preds):\n",
        "    scores = []\n",
        "    for vid in preds:\n",
        "        pred = preds[vid][0]\n",
        "        references = refs.get(vid, [])\n",
        "        if references:\n",
        "            score = meteor_score(references, pred)\n",
        "            scores.append(score)\n",
        "    avg_score = sum(scores) / len(scores)\n",
        "    print(f\"🌟 METEOR Score: {avg_score:.4f}\")\n",
        "    return avg_score\n"
      ],
      "metadata": {
        "id": "t7UmkR15-J9Q"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load ground truth\n",
        "with open('/content/drive/MyDrive/msvd_split/test/test_captions.json', 'r') as f:\n",
        "    gt = json.load(f)\n",
        "\n",
        "refs = {}\n",
        "for s in gt['sentences']:\n",
        "    refs.setdefault(s['video_id'], []).append(s['caption'].lower())\n",
        "\n",
        "# Load predictions\n",
        "with open('/content/drive/MyDrive/msvd_split/predictions_transformer.json', 'r') as f:\n",
        "    pred_json = json.load(f)\n",
        "\n",
        "preds = {k: [v.lower()] for k, v in pred_json.items() if k in refs}\n",
        "\n",
        "# Evaluate METEOR\n",
        "compute_meteor_score(refs, preds)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avOiD55b-NYz",
        "outputId": "dc38577d-7cae-4032-80de-0c25c92fa619"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🌟 METEOR Score: 0.6517\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6516606132281912"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.meteor_score import meteor_score\n",
        "\n",
        "def compute_meteor_score(refs, preds):\n",
        "    scores = []\n",
        "    for vid in preds:\n",
        "        pred = preds[vid][0].split()  # 🔁 tokenized prediction\n",
        "        references = [ref.split() for ref in refs.get(vid, [])]  # 🔁 tokenized references\n",
        "        if references:\n",
        "            score = meteor_score(references, pred)\n",
        "            scores.append(score)\n",
        "    avg_score = sum(scores) / len(scores)\n",
        "    print(f\"🌟 METEOR Score: {avg_score:.4f}\")\n",
        "    return avg_score\n"
      ],
      "metadata": {
        "id": "Uue8US8M-k-u"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = {k: [v.lower()] for k, v in pred_json.items() if k in refs}\n"
      ],
      "metadata": {
        "id": "n1mT2fnX-mWv"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IwX1XvXa-pKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MlnZL7e9_HQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#give mp4 video and predict\n",
        "video_path = '/content/04.mp4'  # 👈 replace with actual path\n"
      ],
      "metadata": {
        "id": "xSqesoFu_HTS"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 2: Extract frames using ffmpeg\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "def extract_frames_from_mp4(video_path, out_dir, num_frames=40):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    # Extract 40 frames equally spaced\n",
        "    cmd = f\"ffmpeg -i {video_path} -vf fps=1 {out_dir}/%06d.jpg -hide_banner -loglevel error\"\n",
        "    subprocess.call(cmd, shell=True)\n",
        "\n",
        "# Example usage\n",
        "extract_frames_from_mp4(video_path, out_dir=\"/content/frames\")\n"
      ],
      "metadata": {
        "id": "R8zr2Vy6BRJA"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pretrainedmodels\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzDJZmx9CNDG",
        "outputId": "12f130bb-6710-4620-eb8b-29d4ebd6f0e7"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/pycocoevalcap-1.2-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting pretrainedmodels\n",
            "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from pretrainedmodels) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from pretrainedmodels) (0.21.0+cu124)\n",
            "Collecting munch (from pretrainedmodels)\n",
            "  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from pretrainedmodels) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->pretrainedmodels) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->pretrainedmodels) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->pretrainedmodels) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->pretrainedmodels) (3.0.2)\n",
            "Downloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n",
            "Building wheels for collected packages: pretrainedmodels\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60945 sha256=aec4ccc27e149f2af5fb7cf517452c5fa0d23a920e235321314675a94afa3663\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/5b/96/fd94bc35962d7c6b699e8814db545155ac91d2b95785e1b035\n",
            "Successfully built pretrainedmodels\n",
            "Installing collected packages: munch, pretrainedmodels\n",
            "Successfully installed munch-4.0.0 pretrainedmodels-0.7.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 3: Extract ResNet/CLIP Features\n",
        "import torch\n",
        "import numpy as np\n",
        "import glob\n",
        "from pretrainedmodels import resnet152\n",
        "from pretrainedmodels.utils import LoadTransformImage\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = resnet152(pretrained='imagenet')\n",
        "model.last_linear = torch.nn.Identity()\n",
        "model = model.to(device).eval()\n",
        "load_img = LoadTransformImage(model)\n",
        "\n",
        "def extract_features_from_frames(frame_dir, num_frames=40):\n",
        "    images = sorted(glob.glob(os.path.join(frame_dir, '*.jpg')))\n",
        "    sampled = np.linspace(0, len(images) - 1, num=num_frames).astype(int)\n",
        "    images = [images[i] for i in sampled]\n",
        "\n",
        "    feats = []\n",
        "    for img_path in images:\n",
        "        img = load_img(img_path).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            feat = model(img).cpu().squeeze()\n",
        "        feats.append(feat)\n",
        "\n",
        "    return torch.stack(feats).unsqueeze(0)  # [1, 40, 2048]\n",
        "\n",
        "# Example\n",
        "video_feat = extract_features_from_frames(\"/content/frames\").float().to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DB5nrsEQBWl7",
        "outputId": "8e2ecbbf-d0ba-4e5d-a3d3-e927c90ddd77"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet152-b121ed2d.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-b121ed2d.pth\n",
            "100%|██████████| 230M/230M [00:02<00:00, 94.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/transformer_model')\n",
        "\n",
        "from video_transformer import VideoTransformer\n"
      ],
      "metadata": {
        "id": "1VWdp1vAHmad"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load vocab beforehand\n",
        "model = VideoTransformer(\n",
        "    vocab_size=len(vocab),\n",
        "    d_model=512,\n",
        "    nhead=8,\n",
        "    num_layers=3,\n",
        "    dim_ff=2048,\n",
        "    max_len=45,\n",
        "    dropout=0.1,\n",
        "    input_feat_dim=2048\n",
        ").to(device)\n",
        "\n",
        "# Load weights from final checkpoint\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/transformer_checkpoints/transformer_final.pt'))\n",
        "model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        },
        "id": "WNt03cgAHpyN",
        "outputId": "e36bb01d-2892-420e-d179-f6aee445af60"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for VideoTransformer:\n\tMissing key(s) in state_dict: \"embedding.weight\", \"pos_enc.pe\", \"vid_fc.weight\", \"vid_fc.bias\", \"decoder.layers.0.self_attn.in_proj_weight\", \"decoder.layers.0.self_attn.in_proj_bias\", \"decoder.layers.0.self_attn.out_proj.weight\", \"decoder.layers.0.self_attn.out_proj.bias\", \"decoder.layers.0.multihead_attn.in_proj_weight\", \"decoder.layers.0.multihead_attn.in_proj_bias\", \"decoder.layers.0.multihead_attn.out_proj.weight\", \"decoder.layers.0.multihead_attn.out_proj.bias\", \"decoder.layers.0.linear1.weight\", \"decoder.layers.0.linear1.bias\", \"decoder.layers.0.linear2.weight\", \"decoder.layers.0.linear2.bias\", \"decoder.layers.0.norm1.weight\", \"decoder.layers.0.norm1.bias\", \"decoder.layers.0.norm2.weight\", \"decoder.layers.0.norm2.bias\", \"decoder.layers.0.norm3.weight\", \"decoder.layers.0.norm3.bias\", \"decoder.layers.1.self_attn.in_proj_weight\", \"decoder.layers.1.self_attn.in_proj_bias\", \"decoder.layers.1.self_attn.out_proj.weight\", \"decoder.layers.1.self_attn.out_proj.bias\", \"decoder.layers.1.multihead_attn.in_proj_weight\", \"decoder.layers.1.multihead_attn.in_proj_bias\", \"decoder.layers.1.multihead_attn.out_proj.weight\", \"decoder.layers.1.multihead_attn.out_proj.bias\", \"decoder.layers.1.linear1.weight\", \"decoder.layers.1.linear1.bias\", \"decoder.layers.1.linear2.weight\", \"decoder.layers.1.linear2.bias\", \"decoder.layers.1.norm1.weight\", \"decoder.layers.1.norm1.bias\", \"decoder.layers.1.norm2.weight\", \"decoder.layers.1.norm2.bias\", \"decoder.layers.1.norm3.weight\", \"decoder.layers.1.norm3.bias\", \"decoder.layers.2.self_attn.in_proj_weight\", \"decoder.layers.2.self_attn.in_proj_bias\", \"decoder.layers.2.self_attn.out_proj.weight\", \"decoder.layers.2.self_attn.out_proj.bias\", \"decoder.layers.2.multihead_attn.in_proj_weight\", \"decoder.layers.2.multihead_attn.in_proj_bias\", \"decoder.layers.2.multihead_attn.out_proj.weight\", \"decoder.layers.2.multihead_attn.out_proj.bias\", \"decoder.layers.2.linear1.weight\", \"decoder.layers.2.linear1.bias\", \"decoder.layers.2.linear2.weight\", \"decoder.layers.2.linear2.bias\", \"decoder.layers.2.norm1.weight\", \"decoder.layers.2.norm1.bias\", \"decoder.layers.2.norm2.weight\", \"decoder.layers.2.norm2.bias\", \"decoder.layers.2.norm3.weight\", \"decoder.layers.2.norm3.bias\", \"output.weight\", \"output.bias\". \n\tUnexpected key(s) in state_dict: \"conv1.weight\", \"bn1.weight\", \"bn1.bias\", \"bn1.running_mean\", \"bn1.running_var\", \"bn1.num_batches_tracked\", \"layer1.0.conv1.weight\", \"layer1.0.bn1.weight\", \"layer1.0.bn1.bias\", \"layer1.0.bn1.running_mean\", \"layer1.0.bn1.running_var\", \"layer1.0.bn1.num_batches_tracked\", \"layer1.0.conv2.weight\", \"layer1.0.bn2.weight\", \"layer1.0.bn2.bias\", \"layer1.0.bn2.running_mean\", \"layer1.0.bn2.running_var\", \"layer1.0.bn2.num_batches_tracked\", \"layer1.0.conv3.weight\", \"layer1.0.bn3.weight\", \"layer1.0.bn3.bias\", \"layer1.0.bn3.running_mean\", \"layer1.0.bn3.running_var\", \"layer1.0.bn3.num_batches_tracked\", \"layer1.0.downsample.0.weight\", \"layer1.0.downsample.1.weight\", \"layer1.0.downsample.1.bias\", \"layer1.0.downsample.1.running_mean\", \"layer1.0.downsample.1.running_var\", \"layer1.0.downsample.1.num_batches_tracked\", \"layer1.1.conv1.weight\", \"layer1.1.bn1.weight\", \"layer1.1.bn1.bias\", \"layer1.1.bn1.running_mean\", \"layer1.1.bn1.running_var\", \"layer1.1.bn1.num_batches_tracked\", \"layer1.1.conv2.weight\", \"layer1.1.bn2.weight\", \"layer1.1.bn2.bias\", \"layer1.1.bn2.running_mean\", \"layer1.1.bn2.running_var\", \"layer1.1.bn2.num_batches_tracked\", \"layer1.1.conv3.weight\", \"layer1.1.bn3.weight\", \"layer1.1.bn3.bias\", \"layer1.1.bn3.running_mean\", \"layer1.1.bn3.running_var\", \"layer1.1.bn3.num_batches_tracked\", \"layer1.2.conv1.weight\", \"layer1.2.bn1.weight\", \"layer1.2.bn1.bias\", \"layer1.2.bn1.running_mean\", \"layer1.2.bn1.running_var\", \"layer1.2.bn1.num_batches_tracked\", \"layer1.2.conv2.weight\", \"layer1.2.bn2.weight\", \"layer1.2.bn2.bias\", \"layer1.2.bn2.running_mean\", \"layer1.2.bn2.running_var\", \"layer1.2.bn2.num_batches_tracked\", \"layer1.2.conv3.weight\", \"layer1.2.bn3.weight\", \"layer1.2.bn3.bias\", \"layer1.2.bn3.running_mean\", \"layer1.2.bn3.running_var\", \"layer1.2.bn3.num_batches_tracked\", \"layer2.0.conv1.weight\", \"layer2.0.bn1.weight\", \"layer2.0.bn1.bias\", \"layer2.0.bn1.running_mean\", \"layer2.0.bn1.running_var\", \"layer2.0.bn1.num_batches_tracked\", \"layer2.0.conv2.weight\", \"layer2.0.bn2.weight\", \"layer2.0.bn2.bias\", \"layer2.0.bn2.running_mean\", \"layer2.0.bn2.running_var\", \"layer2.0.bn2.num_batches_tracked\", \"layer2.0.conv3.weight\", \"layer2.0.bn3.weight\", \"layer2.0.bn3.bias\", \"layer2.0.bn3.running_mean\", \"layer2.0.bn3.running_var\", \"layer2.0.bn3.num_batches_tracked\", \"layer2.0.downsample.0.weight\", \"layer2.0.downsample.1.weight\", \"layer2.0.downsample.1.bias\", \"layer2.0.downsample.1.running_mean\", \"layer2.0.downsample.1.running_var\", \"layer2.0.downsample.1.num_batches_tracked\", \"layer2.1.conv1.weight\", \"layer2.1.bn1.weight\", \"layer2.1.bn1.bias\", \"layer2.1.bn1.running_mean\", \"layer2.1.bn1.running_var\", \"layer2.1.bn1.num_batches_tracked\", \"layer2.1.conv2.weight\", \"layer2.1.bn2.weight\", \"layer2.1.bn2.bias\", \"layer2.1.bn2.running_mean\", \"layer2.1.bn2.running_var\", \"layer2.1.bn2.num_batches_tracked\", \"layer2.1.conv3.weight\", \"layer2.1.bn3.weight\", \"layer2.1.bn3.bias\", \"layer2.1.bn3.running_mean\", \"layer2.1.bn3.running_var\", \"layer2.1.bn3.num_batches_tracked\", \"layer2.2.conv1.weight\", \"layer2.2.bn1.weight\", \"layer2.2.bn1.bias\", \"layer2.2.bn1.running_mean\", \"layer2.2.bn1.running_var\", \"layer2.2.bn1.num_batches_tracked\", \"layer2.2.conv2.weight\", \"layer2.2.bn2.weight\", \"layer2.2.bn2.bias\", \"layer2.2.bn2.running_mean\", \"layer2.2.bn2.running_var\", \"layer2.2.bn2.num_batches_tracked\", \"layer2.2.conv3.weight\", \"layer2.2.bn3.weight\", \"layer2.2.bn3.bias\", \"layer2.2.bn3.running_mean\", \"layer2.2.bn3.running_var\", \"layer2.2.bn3.num_batches_tracked\", \"layer2.3.conv1.weight\", \"layer2.3.bn1.weight\", \"layer2.3.bn1.bias\", \"layer2.3.bn1.running_mean\", \"layer2.3.bn1.running_var\", \"layer2.3.bn1.num_batches_tracked\", \"layer2.3.conv2.weight\", \"layer2.3.bn2.weight\", \"layer2.3.bn2.bias\", \"layer2.3.bn2.running_mean\", \"layer2.3.bn2.running_var\", \"layer2.3.bn2.num_batches_tracked\", \"layer2.3.conv3.weight\", \"layer2.3.bn3.weight\", \"layer2.3.bn3.bias\", \"layer2.3.bn3.running_mean\", \"layer2.3.bn3.running_var\", \"layer2.3.bn3.num_batches_tracked\", \"layer2.4.conv1.weight\", \"layer2.4.bn1.weight\", \"layer2.4.bn1.bias\", \"layer2.4.bn1.running_mean\", \"layer2.4.bn1.running_var\", \"layer2.4.bn1.num_batches_tracked\", \"layer2.4.conv2.weight\", \"layer2.4.bn2.weight\", \"layer2.4.bn2.bias\", \"layer2.4.bn2.running_mean\", \"layer2.4.bn2.running_var\", \"layer2.4.bn2.num_batches_tracked\", \"layer2.4.conv3.weight\", \"layer2.4.bn3.weight\", \"layer2.4.bn3.bias\", \"layer2.4.bn3.running_mean\", \"layer2.4.bn3.running_var\", \"layer2.4.bn3.num_batches_tracked\", \"layer2.5.conv1.weight\", \"layer2.5.bn1.weight\", \"layer2.5.bn1.bias\", \"layer2.5.bn1.running_mean\", \"layer2.5.bn1.running_var\", \"layer2.5.bn1.num_batches_tracked\", \"layer2.5.conv2.weight\", \"layer2.5.bn2.weight\", \"layer2.5.bn2.bias\", \"layer2.5.bn2.running_mean\", \"layer2.5.bn2.running_var\", \"layer2.5.bn2.num_batches_tracked\", \"layer2.5.conv3.weight\", \"layer2.5.bn3.weight\", \"layer2.5.bn3.bias\", \"layer2.5.bn3.running_mean\", \"layer2.5.bn3.running_var\", \"layer2.5.bn3.num_batches_tracked\", \"layer2.6.conv1.weight\", \"layer2.6.bn1.weight\", \"layer2.6.bn1.bias\", \"layer2.6.bn1.running_mean\", \"layer2.6.bn1.running_var\", \"layer2.6.bn1.num_batches_tracked\", \"layer2.6.conv2.weight\", \"layer2.6.bn2.weight\", \"layer2.6.bn2.bias\", \"layer2.6.bn2.running_mean\", \"layer2.6.bn2.running_var\", \"layer2.6.bn2.num_batches_tracked\", \"layer2.6.conv3.weight\", \"layer2.6.bn3.weight\", \"layer2.6.bn3.bias\", \"layer2.6.bn3.running_mean\", \"layer2.6.bn3.running_var\", \"layer2.6.bn3.num_batches_tracked\", \"layer2.7.conv1.weight\", \"layer2.7.bn1.weight\", \"layer2.7.bn1.bias\", \"layer2.7.bn1.running_mean\", \"layer2.7.bn1.running_var\", \"layer2.7.bn1.num_batches_tracked\", \"layer2.7.conv2.weight\", \"layer2.7.bn2.weight\", \"layer2.7.bn2.bias\", \"layer2.7.bn2.running_mean\", \"layer2.7.bn2.running_var\", \"layer2.7.bn2.num_batches_tracked\", \"layer2.7.conv3.weight\", \"layer2.7.bn3.weight\", \"layer2.7.bn3.bias\", \"layer2.7.bn3.running_mean\", \"layer2.7.bn3.running_var\", \"layer2.7.bn3.num_batches_tracked\", \"layer3.0.conv1.weight\", \"layer3.0.bn1.weight\", \"layer3.0.bn1.bias\", \"layer3.0.bn1.running_mean\", \"layer3.0.bn1.running_var\", \"layer3.0.bn1.num_batches_tracked\", \"layer3.0.conv2.weight\", \"layer3.0.bn2.weight\", \"layer3.0.bn2.bias\", \"layer3.0.bn2.running_mean\", \"layer3.0.bn2.running_var\", \"layer3.0.bn2.num_batches_tracked\", \"layer3.0.conv3.weight\", \"layer3.0.bn3.weight\", \"layer3.0.bn3.bias\", \"layer3.0.bn3.running_mean\", \"layer3.0.bn3.running_var\", \"layer3.0.bn3.num_batches_tracked\", \"layer3.0.downsample.0.weight\", \"layer3.0.downsample.1.weight\", \"layer3.0.downsample.1.bias\", \"layer3.0.downsample.1.running_mean\", \"layer3.0.downsample.1.running_var\", \"layer3.0.downsample.1.num_batches_tracked\", \"layer3.1.conv1.weight\", \"layer3.1.bn1.weight\", \"layer3.1.bn1.bias\", \"layer3.1.bn1.running_mean\", \"layer3.1.bn1.running_var\", \"layer3.1.bn1.num_batches_tracked\", \"layer3.1.conv2.weight\", \"layer3.1.bn2.weight\", \"layer3.1.bn2.bias\", \"layer3.1.bn2.running_mean\", \"layer3.1.bn2.running_var\", \"layer3.1.bn2.num_batches_tracked\", \"layer3.1.conv3.weight\", \"layer3.1.bn3.weight\", \"layer3.1.bn3.bias\", \"layer3.1.bn3.running_mean\", \"layer3.1.bn3.running_var\", \"layer3.1.bn3.num_batches_tracked\", \"layer3.2.conv1.weight\", \"layer3.2.bn1.weight\", \"layer3.2.bn1.bias\", \"layer3.2.bn1.running_mean\", \"layer3.2.bn1.running_var\", \"layer3.2.bn1.num_batches_tracked\", \"layer3.2.conv2.weight\", \"layer3.2.bn2.weight\", \"layer3.2.bn2.bias\", \"layer3.2.bn2.running_mean\", \"layer3.2.bn2.running_var\", \"layer3.2.bn2.num_batches_tracked\", \"layer3.2.conv3.weight\", \"layer3.2.bn3.weight\", \"layer3.2.bn3.bias\", \"layer3.2.bn3.running_mean\", \"layer3.2.bn3.running_var\", \"layer3.2.bn3.num_batches_tracked\", \"layer3.3.conv1.weight\", \"layer3.3.bn1.weight\", \"layer3.3.bn1.bias\", \"layer3.3.bn1.running_mean\", \"layer3.3.bn1.running_var\", \"layer3.3.bn1.num_batches_tracked\", \"layer3.3.conv2.weight\", \"layer3.3.bn2.weight\", \"layer3.3.bn2.bias\", \"layer3.3.bn2.running_mean\", \"layer3.3.bn2.running_var\", \"layer3.3.bn2.num_batches_tracked\", \"layer3.3.conv3.weight\", \"layer3.3.bn3.weight\", \"layer3.3.bn3.bias\", \"layer3.3.bn3.running_mean\", \"layer3.3.bn3.running_var\", \"layer3.3.bn3.num_batches_tracked\", \"layer3.4.conv1.weight\", \"layer3.4.bn1.weight\", \"layer3.4.bn1.bias\", \"layer3.4.bn1.running_mean\", \"layer3.4.bn1.running_var\", \"layer3.4.bn1.num_batches_tracked\", \"layer3.4.conv2.weight\", \"layer3.4.bn2.weight\", \"layer3.4.bn2.bias\", \"layer3.4.bn2.running_mean\", \"layer3.4.bn2.running_var\", \"layer3.4.bn2.num_batches_tracked\", \"layer3.4.conv3.weight\", \"layer3.4.bn3.weight\", \"layer3.4.bn3.bias\", \"layer3.4.bn3.running_mean\", \"layer3.4.bn3.running_var\", \"layer3.4.bn3.num_batches_tracked\", \"layer3.5.conv1.weight\", \"layer3.5.bn1.weight\", \"layer3.5.bn1.bias\", \"layer3.5.bn1.running_mean\", \"layer3.5.bn1.running_var\", \"layer3.5.bn1.num_batches_tracked\", \"layer3.5.conv2.weight\", \"layer3.5.bn2.weight\", \"layer3.5.bn2.bias\", \"layer3.5.bn2.running_mean\", \"layer3.5.bn2.running_var\", \"layer3.5.bn2.num_batches_tracked\", \"layer3.5.conv3.weight\", \"layer3.5.bn3.weight\", \"layer3.5.bn3.bias\", \"layer3.5.bn3.running_mean\", \"layer3.5.bn3.running_var\", \"layer3.5.bn3.num_batches_tracked\", \"layer3.6.conv1.weight\", \"layer3.6.bn1.weight\", \"layer3.6.bn1.bias\", \"layer3.6.bn1.running_mean\", \"layer3.6.bn1.running_var\", \"layer3.6.bn1.num_batches_tracked\", \"layer3.6.conv2.weight\", \"layer3.6.bn2.weight\", \"layer3.6.bn2.bias\", \"layer3.6.bn2.running_mean\", \"layer3.6.bn2.running_var\", \"layer3.6.bn2.num_batches_tracked\", \"layer3.6.conv3.weight\", \"layer3.6.bn3.weight\", \"layer3.6.bn3.bias\", \"layer3.6.bn3.running_mean\", \"layer3.6.bn3.running_var\", \"layer3.6.bn3.num_batches_tracked\", \"layer3.7.conv1.weight\", \"layer3.7.bn1.weight\", \"layer3.7.bn1.bias\", \"layer3.7.bn1.running_mean\", \"layer3.7.bn1.running_var\", \"layer3.7.bn1.num_batches_tracked\", \"layer3.7.conv2.weight\", \"layer3.7.bn2.weight\", \"layer3.7.bn2.bias\", \"layer3.7.bn2.running_mean\", \"layer3.7.bn2.running_var\", \"layer3.7.bn2.num_batches_tracked\", \"layer3.7.conv3.weight\", \"layer3.7.bn3.weight\", \"layer3.7.bn3.bias\", \"layer3.7.bn3.running_mean\", \"layer3.7.bn3.running_var\", \"layer3.7.bn3.num_batches_tracked\", \"layer3.8.conv1.weight\", \"layer3.8.bn1.weight\", \"layer3.8.bn1.bias\", \"layer3.8.bn1.running_mean\", \"layer3.8.bn1.running_var\", \"layer3.8.bn1.num_batches_tracked\", \"layer3.8.conv2.weight\", \"layer3.8.bn2.weight\", \"layer3.8.bn2.bias\", \"layer3.8.bn2.running_mean\", \"layer3.8.bn2.running_var\", \"layer3.8.bn2.num_batches_tracked\", \"layer3.8.conv3.weight\", \"layer3.8.bn3.weight\", \"layer3.8.bn3.bias\", \"layer3.8.bn3.running_mean\", \"layer3.8.bn3.running_var\", \"layer3.8.bn3.num_batches_tracked\", \"layer3.9.conv1.weight\", \"layer3.9.bn1.weight\", \"layer3.9.bn1.bias\", \"layer3.9.bn1.running_mean\", \"layer3.9.bn1.running_var\", \"layer3.9.bn1.num_batches_tracked\", \"layer3.9.conv2.weight\", \"layer3.9.bn2.weight\", \"layer3.9.bn2.bias\", \"layer3.9.bn2.running_mean\", \"layer3.9.bn2.running_var\", \"layer3.9.bn2.num_batches_tracked\", \"layer3.9.conv3.weight\", \"layer3.9.bn3.weight\", \"layer3.9.bn3.bias\", \"layer3.9.bn3.running_mean\", \"layer3.9.bn3.running_var\", \"layer3.9.bn3.num_batches_tracked\", \"layer3.10.conv1.weight\", \"layer3.10.bn1.weight\", \"layer3.10.bn1.bias\", \"layer3.10.bn1.running_mean\", \"layer3.10.bn1.running_var\", \"layer3.10.bn1.num_batches_tracked\", \"layer3.10.conv2.weight\", \"layer3.10.bn2.weight\", \"layer3.10.bn2.bias\", \"layer3.10.bn2.running_mean\", \"layer3.10.bn2.running_var\", \"layer3.10.bn2.num_batches_tracked\", \"layer3.10.conv3.weight\", \"layer3.10.bn3.weight\", \"layer3.10.bn3.bias\", \"layer3.10.bn3.running_mean\", \"layer3.10.bn3.running_var\", \"layer3.10.bn3.num_batches_tracked\", \"layer3.11.conv1.weight\", \"layer3.11.bn1.weight\", \"layer3.11.bn1.bias\", \"layer3.11.bn1.running_mean\", \"layer3.11.bn1.running_var\", \"layer3.11.bn1.num_batches_tracked\", \"layer3.11.conv2.weight\", \"layer3.11.bn2.weight\", \"layer3.11.bn2.bias\", \"layer3.11.bn2.running_mean\", \"layer3.11.bn2.running_var\", \"layer3.11.bn2.num_batches_tracked\", \"layer3.11.conv3.weight\", \"layer3.11.bn3.weight\", \"layer3.11.bn3.bias\", \"layer3.11.bn3.running_mean\", \"layer3.11.bn3.running_var\", \"layer3.11.bn3.num_batches_tracked\", \"layer3.12.conv1.weight\", \"layer3.12.bn1.weight\", \"layer3.12.bn1.bias\", \"layer3.12.bn1.running_mean\", \"layer3.12.bn1.running_var\", \"layer3.12.bn1.num_batches_tracked\", \"layer3.12.conv2.weight\", \"layer3.12.bn2.weight\", \"layer3.12.bn2.bias\", \"layer3.12.bn2.running_mean\", \"layer3.12.bn2.running_var\", \"layer3.12.bn2.num_batches_tracked\", \"layer3.12.conv3.weight\", \"layer3.12.bn3.weight\", \"layer3.12.bn3.bias\", \"layer3.12.bn3.running_mean\", \"layer3.12.bn3.running_var\", \"layer3.12.bn3.num_batches_tracked\", \"layer3.13.conv1.weight\", \"layer3.13.bn1.weight\", \"layer3.13.bn1.bias\", \"layer3.13.bn1.running_mean\", \"layer3.13.bn1.running_var\", \"layer3.13.bn1.num_batches_tracked\", \"layer3.13.conv2.weight\", \"layer3.13.bn2.weight\", \"layer3.13.bn2.bias\", \"layer3.13.bn2.running_mean\", \"layer3.13.bn2.running_var\", \"layer3.13.bn2.num_batches_tracked\", \"layer3.13.conv3.weight\", \"layer3.13.bn3.weight\", \"layer3.13.bn3.bias\", \"layer3.13.bn3.running_mean\", \"layer3.13.bn3.running_var\", \"layer3.13.bn3.num_batches_tracked\", \"layer3.14.conv1.weight\", \"layer3.14.bn1.weight\", \"layer3.14.bn1.bias\", \"layer3.14.bn1.running_mean\", \"layer3.14.bn1.running_var\", \"layer3.14.bn1.num_batches_tracked\", \"layer3.14.conv2.weight\", \"layer3.14.bn2.weight\", \"layer3.14.bn2.bias\", \"layer3.14.bn2.running_mean\", \"layer3.14.bn2.running_var\", \"layer3.14.bn2.num_batches_tracked\", \"layer3.14.conv3.weight\", \"layer3.14.bn3.weight\", \"layer3.14.bn3.bias\", \"layer3.14.bn3.running_mean\", \"layer3.14.bn3.running_var\", \"layer3.14.bn3.num_batches_tracked\", \"layer3.15.conv1.weight\", \"layer3.15.bn1.weight\", \"layer3.15.bn1.bias\", \"layer3.15.bn1.running_mean\", \"layer3.15.bn1.running_var\", \"layer3.15.bn1.num_batches_tracked\", \"layer3.15.conv2.weight\", \"layer3.15.bn2.weight\", \"layer3.15.bn2.bias\", \"layer3.15.bn2.running_mean\", \"layer3.15.bn2.running_var\", \"layer3.15.bn2.num_batches_tracked\", \"layer3.15.conv3.weight\", \"layer3.15.bn3.weight\", \"layer3.15.bn3.bias\", \"layer3.15.bn3.running_mean\", \"layer3.15.bn3.running_var\", \"layer3.15.bn3.num_batches_tracked\", \"layer3.16.conv1.weight\", \"layer3.16.bn1.weight\", \"layer3.16.bn1.bias\", \"layer3.16.bn1.running_mean\", \"layer3.16.bn1.running_var\", \"layer3.16.bn1.num_batches_tracked\", \"layer3.16.conv2.weight\", \"layer3.16.bn2.weight\", \"layer3.16.bn2.bias\", \"layer3.16.bn2.running_mean\", \"layer3.16.bn2.running_var\", \"layer3.16.bn2.num_batches_tracked\", \"layer3.16.conv3.weight\", \"layer3.16.bn3.weight\", \"layer3.16.bn3.bias\", \"layer3.16.bn3.running_mean\", \"layer3.16.bn3.running_var\", \"layer3.16.bn3.num_batches_tracked\", \"layer3.17.conv1.weight\", \"layer3.17.bn1.weight\", \"layer3.17.bn1.bias\", \"layer3.17.bn1.running_mean\", \"layer3.17.bn1.running_var\", \"layer3.17.bn1.num_batches_tracked\", \"layer3.17.conv2.weight\", \"layer3.17.bn2.weight\", \"layer3.17.bn2.bias\", \"layer3.17.bn2.running_mean\", \"layer3.17.bn2.running_var\", \"layer3.17.bn2.num_batches_tracked\", \"layer3.17.conv3.weight\", \"layer3.17.bn3.weight\", \"layer3.17.bn3.bias\", \"layer3.17.bn3.running_mean\", \"layer3.17.bn3.running_var\", \"layer3.17.bn3.num_batches_tracked\", \"layer3.18.conv1.weight\", \"layer3.18.bn1.weight\", \"layer3.18.bn1.bias\", \"layer3.18.bn1.running_mean\", \"layer3.18.bn1.running_var\", \"layer3.18.bn1.num_batches_tracked\", \"layer3.18.conv2.weight\", \"layer3.18.bn2.weight\", \"layer3.18.bn2.bias\", \"layer3.18.bn2.running_mean\", \"layer3.18.bn2.running_var\", \"layer3.18.bn2.num_batches_tracked\", \"layer3.18.conv3.weight\", \"layer3.18.bn3.weight\", \"layer3.18.bn3.bias\", \"layer3.18.bn3.running_mean\", \"layer3.18.bn3.running_var\", \"layer3.18.bn3.num_batches_tracked\", \"layer3.19.conv1.weight\", \"layer3.19.bn1.weight\", \"layer3.19.bn1.bias\", \"layer3.19.bn1.running_mean\", \"layer3.19.bn1.running_var\", \"layer3.19.bn1.num_batches_tracked\", \"layer3.19.conv2.weight\", \"layer3.19.bn2.weight\", \"layer3.19.bn2.bias\", \"layer3.19.bn2.running_mean\", \"layer3.19.bn2.running_var\", \"layer3.19.bn2.num_batches_tracked\", \"layer3.19.conv3.weight\", \"layer3.19.bn3.weight\", \"layer3.19.bn3.bias\", \"layer3.19.bn3.running_mean\", \"layer3.19.bn3.running_var\", \"layer3.19.bn3.num_batches_tracked\", \"layer3.20.conv1.weight\", \"layer3.20.bn1.weight\", \"layer3.20.bn1.bias\", \"layer3.20.bn1.running_mean\", \"layer3.20.bn1.running_var\", \"layer3.20.bn1.num_batches_tracked\", \"layer3.20.conv2.weight\", \"layer3.20.bn2.weight\", \"layer3.20.bn2.bias\", \"layer3.20.bn2.running_mean\", \"layer3.20.bn2.running_var\", \"layer3.20.bn2.num_batches_tracked\", \"layer3.20.conv3.weight\", \"layer3.20.bn3.weight\", \"layer3.20.bn3.bias\", \"layer3.20.bn3.running_mean\", \"layer3.20.bn3.running_var\", \"layer3.20.bn3.num_batches_tracked\", \"layer3.21.conv1.weight\", \"layer3.21.bn1.weight\", \"layer3.21.bn1.bias\", \"layer3.21.bn1.running_mean\", \"layer3.21.bn1.running_var\", \"layer3.21.bn1.num_batches_tracked\", \"layer3.21.conv2.weight\", \"layer3.21.bn2.weight\", \"layer3.21.bn2.bias\", \"layer3.21.bn2.running_mean\", \"layer3.21.bn2.running_var\", \"layer3.21.bn2.num_batches_tracked\", \"layer3.21.conv3.weight\", \"layer3.21.bn3.weight\", \"layer3.21.bn3.bias\", \"layer3.21.bn3.running_mean\", \"layer3.21.bn3.running_var\", \"layer3.21.bn3.num_batches_tracked\", \"layer3.22.conv1.weight\", \"layer3.22.bn1.weight\", \"layer3.22.bn1.bias\", \"layer3.22.bn1.running_mean\", \"layer3.22.bn1.running_var\", \"layer3.22.bn1.num_batches_tracked\", \"layer3.22.conv2.weight\", \"layer3.22.bn2.weight\", \"layer3.22.bn2.bias\", \"layer3.22.bn2.running_mean\", \"layer3.22.bn2.running_var\", \"layer3.22.bn2.num_batches_tracked\", \"layer3.22.conv3.weight\", \"layer3.22.bn3.weight\", \"layer3.22.bn3.bias\", \"layer3.22.bn3.running_mean\", \"layer3.22.bn3.running_var\", \"layer3.22.bn3.num_batches_tracked\", \"layer3.23.conv1.weight\", \"layer3.23.bn1.weight\", \"layer3.23.bn1.bias\", \"layer3.23.bn1.running_mean\", \"layer3.23.bn1.running_var\", \"layer3.23.bn1.num_batches_tracked\", \"layer3.23.conv2.weight\", \"layer3.23.bn2.weight\", \"layer3.23.bn2.bias\", \"layer3.23.bn2.running_mean\", \"layer3.23.bn2.running_var\", \"layer3.23.bn2.num_batches_tracked\", \"layer3.23.conv3.weight\", \"layer3.23.bn3.weight\", \"layer3.23.bn3.bias\", \"layer3.23.bn3.running_mean\", \"layer3.23.bn3.running_var\", \"layer3.23.bn3.num_batches_tracked\", \"layer3.24.conv1.weight\", \"layer3.24.bn1.weight\", \"layer3.24.bn1.bias\", \"layer3.24.bn1.running_mean\", \"layer3.24.bn1.running_var\", \"layer3.24.bn1.num_batches_tracked\", \"layer3.24.conv2.weight\", \"layer3.24.bn2.weight\", \"layer3.24.bn2.bias\", \"layer3.24.bn2.running_mean\", \"layer3.24.bn2.running_var\", \"layer3.24.bn2.num_batches_tracked\", \"layer3.24.conv3.weight\", \"layer3.24.bn3.weight\", \"layer3.24.bn3.bias\", \"layer3.24.bn3.running_mean\", \"layer3.24.bn3.running_var\", \"layer3.24.bn3.num_batches_tracked\", \"layer3.25.conv1.weight\", \"layer3.25.bn1.weight\", \"layer3.25.bn1.bias\", \"layer3.25.bn1.running_mean\", \"layer3.25.bn1.running_var\", \"layer3.25.bn1.num_batches_tracked\", \"layer3.25.conv2.weight\", \"layer3.25.bn2.weight\", \"layer3.25.bn2.bias\", \"layer3.25.bn2.running_mean\", \"layer3.25.bn2.running_var\", \"layer3.25.bn2.num_batches_tracked\", \"layer3.25.conv3.weight\", \"layer3.25.bn3.weight\", \"layer3.25.bn3.bias\", \"layer3.25.bn3.running_mean\", \"layer3.25.bn3.running_var\", \"layer3.25.bn3.num_batches_tracked\", \"layer3.26.conv1.weight\", \"layer3.26.bn1.weight\", \"layer3.26.bn1.bias\", \"layer3.26.bn1.running_mean\", \"layer3.26.bn1.running_var\", \"layer3.26.bn1.num_batches_tracked\", \"layer3.26.conv2.weight\", \"layer3.26.bn2.weight\", \"layer3.26.bn2.bias\", \"layer3.26.bn2.running_mean\", \"layer3.26.bn2.running_var\", \"layer3.26.bn2.num_batches_tracked\", \"layer3.26.conv3.weight\", \"layer3.26.bn3.weight\", \"layer3.26.bn3.bias\", \"layer3.26.bn3.running_mean\", \"layer3.26.bn3.running_var\", \"layer3.26.bn3.num_batches_tracked\", \"layer3.27.conv1.weight\", \"layer3.27.bn1.weight\", \"layer3.27.bn1.bias\", \"layer3.27.bn1.running_mean\", \"layer3.27.bn1.running_var\", \"layer3.27.bn1.num_batches_tracked\", \"layer3.27.conv2.weight\", \"layer3.27.bn2.weight\", \"layer3.27.bn2.bias\", \"layer3.27.bn2.running_mean\", \"layer3.27.bn2.running_var\", \"layer3.27.bn2.num_batches_tracked\", \"layer3.27.conv3.weight\", \"layer3.27.bn3.weight\", \"layer3.27.bn3.bias\", \"layer3.27.bn3.running_mean\", \"layer3.27.bn3.running_var\", \"layer3.27.bn3.num_batches_tracked\", \"layer3.28.conv1.weight\", \"layer3.28.bn1.weight\", \"layer3.28.bn1.bias\", \"layer3.28.bn1.running_mean\", \"layer3.28.bn1.running_var\", \"layer3.28.bn1.num_batches_tracked\", \"layer3.28.conv2.weight\", \"layer3.28.bn2.weight\", \"layer3.28.bn2.bias\", \"layer3.28.bn2.running_mean\", \"layer3.28.bn2.running_var\", \"layer3.28.bn2.num_batches_tracked\", \"layer3.28.conv3.weight\", \"layer3.28.bn3.weight\", \"layer3.28.bn3.bias\", \"layer3.28.bn3.running_mean\", \"layer3.28.bn3.running_var\", \"layer3.28.bn3.num_batches_tracked\", \"layer3.29.conv1.weight\", \"layer3.29.bn1.weight\", \"layer3.29.bn1.bias\", \"layer3.29.bn1.running_mean\", \"layer3.29.bn1.running_var\", \"layer3.29.bn1.num_batches_tracked\", \"layer3.29.conv2.weight\", \"layer3.29.bn2.weight\", \"layer3.29.bn2.bias\", \"layer3.29.bn2.running_mean\", \"layer3.29.bn2.running_var\", \"layer3.29.bn2.num_batches_tracked\", \"layer3.29.conv3.weight\", \"layer3.29.bn3.weight\", \"layer3.29.bn3.bias\", \"layer3.29.bn3.running_mean\", \"layer3.29.bn3.running_var\", \"layer3.29.bn3.num_batches_tracked\", \"layer3.30.conv1.weight\", \"layer3.30.bn1.weight\", \"layer3.30.bn1.bias\", \"layer3.30.bn1.running_mean\", \"layer3.30.bn1.running_var\", \"layer3.30.bn1.num_batches_tracked\", \"layer3.30.conv2.weight\", \"layer3.30.bn2.weight\", \"layer3.30.bn2.bias\", \"layer3.30.bn2.running_mean\", \"layer3.30.bn2.running_var\", \"layer3.30.bn2.num_batches_tracked\", \"layer3.30.conv3.weight\", \"layer3.30.bn3.weight\", \"layer3.30.bn3.bias\", \"layer3.30.bn3.running_mean\", \"layer3.30.bn3.running_var\", \"layer3.30.bn3.num_batches_tracked\", \"layer3.31.conv1.weight\", \"layer3.31.bn1.weight\", \"layer3.31.bn1.bias\", \"layer3.31.bn1.running_mean\", \"layer3.31.bn1.running_var\", \"layer3.31.bn1.num_batches_tracked\", \"layer3.31.conv2.weight\", \"layer3.31.bn2.weight\", \"layer3.31.bn2.bias\", \"layer3.31.bn2.running_mean\", \"layer3.31.bn2.running_var\", \"layer3.31.bn2.num_batches_tracked\", \"layer3.31.conv3.weight\", \"layer3.31.bn3.weight\", \"layer3.31.bn3.bias\", \"layer3.31.bn3.running_mean\", \"layer3.31.bn3.running_var\", \"layer3.31.bn3.num_batches_tracked\", \"layer3.32.conv1.weight\", \"layer3.32.bn1.weight\", \"layer3.32.bn1.bias\", \"layer3.32.bn1.running_mean\", \"layer3.32.bn1.running_var\", \"layer3.32.bn1.num_batches_tracked\", \"layer3.32.conv2.weight\", \"layer3.32.bn2.weight\", \"layer3.32.bn2.bias\", \"layer3.32.bn2.running_mean\", \"layer3.32.bn2.running_var\", \"layer3.32.bn2.num_batches_tracked\", \"layer3.32.conv3.weight\", \"layer3.32.bn3.weight\", \"layer3.32.bn3.bias\", \"layer3.32.bn3.running_mean\", \"layer3.32.bn3.running_var\", \"layer3.32.bn3.num_batches_tracked\", \"layer3.33.conv1.weight\", \"layer3.33.bn1.weight\", \"layer3.33.bn1.bias\", \"layer3.33.bn1.running_mean\", \"layer3.33.bn1.running_var\", \"layer3.33.bn1.num_batches_tracked\", \"layer3.33.conv2.weight\", \"layer3.33.bn2.weight\", \"layer3.33.bn2.bias\", \"layer3.33.bn2.running_mean\", \"layer3.33.bn2.running_var\", \"layer3.33.bn2.num_batches_tracked\", \"layer3.33.conv3.weight\", \"layer3.33.bn3.weight\", \"layer3.33.bn3.bias\", \"layer3.33.bn3.running_mean\", \"layer3.33.bn3.running_var\", \"layer3.33.bn3.num_batches_tracked\", \"layer3.34.conv1.weight\", \"layer3.34.bn1.weight\", \"layer3.34.bn1.bias\", \"layer3.34.bn1.running_mean\", \"layer3.34.bn1.running_var\", \"layer3.34.bn1.num_batches_tracked\", \"layer3.34.conv2.weight\", \"layer3.34.bn2.weight\", \"layer3.34.bn2.bias\", \"layer3.34.bn2.running_mean\", \"layer3.34.bn2.running_var\", \"layer3.34.bn2.num_batches_tracked\", \"layer3.34.conv3.weight\", \"layer3.34.bn3.weight\", \"layer3.34.bn3.bias\", \"layer3.34.bn3.running_mean\", \"layer3.34.bn3.running_var\", \"layer3.34.bn3.num_batches_tracked\", \"layer3.35.conv1.weight\", \"layer3.35.bn1.weight\", \"layer3.35.bn1.bias\", \"layer3.35.bn1.running_mean\", \"layer3.35.bn1.running_var\", \"layer3.35.bn1.num_batches_tracked\", \"layer3.35.conv2.weight\", \"layer3.35.bn2.weight\", \"layer3.35.bn2.bias\", \"layer3.35.bn2.running_mean\", \"layer3.35.bn2.running_var\", \"layer3.35.bn2.num_batches_tracked\", \"layer3.35.conv3.weight\", \"layer3.35.bn3.weight\", \"layer3.35.bn3.bias\", \"layer3.35.bn3.running_mean\", \"layer3.35.bn3.running_var\", \"layer3.35.bn3.num_batches_tracked\", \"layer4.0.conv1.weight\", \"layer4.0.bn1.weight\", \"layer4.0.bn1.bias\", \"layer4.0.bn1.running_mean\", \"layer4.0.bn1.running_var\", \"layer4.0.bn1.num_batches_tracked\", \"layer4.0.conv2.weight\", \"layer4.0.bn2.weight\", \"layer4.0.bn2.bias\", \"layer4.0.bn2.running_mean\", \"layer4.0.bn2.running_var\", \"layer4.0.bn2.num_batches_tracked\", \"layer4.0.conv3.weight\", \"layer4.0.bn3.weight\", \"layer4.0.bn3.bias\", \"layer4.0.bn3.running_mean\", \"layer4.0.bn3.running_var\", \"layer4.0.bn3.num_batches_tracked\", \"layer4.0.downsample.0.weight\", \"layer4.0.downsample.1.weight\", \"layer4.0.downsample.1.bias\", \"layer4.0.downsample.1.running_mean\", \"layer4.0.downsample.1.running_var\", \"layer4.0.downsample.1.num_batches_tracked\", \"layer4.1.conv1.weight\", \"layer4.1.bn1.weight\", \"layer4.1.bn1.bias\", \"layer4.1.bn1.running_mean\", \"layer4.1.bn1.running_var\", \"layer4.1.bn1.num_batches_tracked\", \"layer4.1.conv2.weight\", \"layer4.1.bn2.weight\", \"layer4.1.bn2.bias\", \"layer4.1.bn2.running_mean\", \"layer4.1.bn2.running_var\", \"layer4.1.bn2.num_batches_tracked\", \"layer4.1.conv3.weight\", \"layer4.1.bn3.weight\", \"layer4.1.bn3.bias\", \"layer4.1.bn3.running_mean\", \"layer4.1.bn3.running_var\", \"layer4.1.bn3.num_batches_tracked\", \"layer4.2.conv1.weight\", \"layer4.2.bn1.weight\", \"layer4.2.bn1.bias\", \"layer4.2.bn1.running_mean\", \"layer4.2.bn1.running_var\", \"layer4.2.bn1.num_batches_tracked\", \"layer4.2.conv2.weight\", \"layer4.2.bn2.weight\", \"layer4.2.bn2.bias\", \"layer4.2.bn2.running_mean\", \"layer4.2.bn2.running_var\", \"layer4.2.bn2.num_batches_tracked\", \"layer4.2.conv3.weight\", \"layer4.2.bn3.weight\", \"layer4.2.bn3.bias\", \"layer4.2.bn3.running_mean\", \"layer4.2.bn3.running_var\", \"layer4.2.bn3.num_batches_tracked\". ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-1de3912331a3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Load weights from final checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/transformer_checkpoints/transformer_final.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2581\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   2582\u001b[0m                 \"Error(s) in loading state_dict for {}:\\n\\t{}\".format(\n\u001b[1;32m   2583\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\\t\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for VideoTransformer:\n\tMissing key(s) in state_dict: \"embedding.weight\", \"pos_enc.pe\", \"vid_fc.weight\", \"vid_fc.bias\", \"decoder.layers.0.self_attn.in_proj_weight\", \"decoder.layers.0.self_attn.in_proj_bias\", \"decoder.layers.0.self_attn.out_proj.weight\", \"decoder.layers.0.self_attn.out_proj.bias\", \"decoder.layers.0.multihead_attn.in_proj_weight\", \"decoder.layers.0.multihead_attn.in_proj_bias\", \"decoder.layers.0.multihead_attn.out_proj.weight\", \"decoder.layers.0.multihead_attn.out_proj.bias\", \"decoder.layers.0.linear1.weight\", \"decoder.layers.0.linear1.bias\", \"decoder.layers.0.linear2.weight\", \"decoder.layers.0.linear2.bias\", \"decoder.layers.0.norm1.weight\", \"decoder.layers.0.norm1.bias\", \"decoder.layers.0.norm2.weight\", \"decoder.layers.0.norm2.bias\", \"decoder.layers.0.norm3.weight\", \"decoder.layers.0.norm3.bias\", \"decoder.layers.1.self_attn.in_proj_weight\", \"decoder.layers.1.self_attn.in_proj_bias\", \"decoder.layers.1.self_attn.out_proj.weight\", \"decoder.layers.1.self_attn.out_proj.bias\", \"decoder.layers.1.multihead_attn.in_proj_weight\", \"decoder.layers.1.multihead_attn.in_proj_bias\", \"decoder.layers.1.multihead_attn.out_proj.weight\", \"decoder.layers.1.multihead_attn.out_proj.bias\", \"decoder.layers.1.linear1.weight\", \"decoder.layers.1.linear1.bias\", \"decoder.layers.1.linear2.weight\", \"decoder.layers.1.linear2.bias\", \"decoder.layers.1.norm1.weight\", \"decoder.layers.1.norm1.bias\", \"decoder.layers.1.norm2.weight\", \"decoder.layers.1.norm2.bias\", \"decoder.layers.1.norm3.weight\", \"decoder.layers.1.norm3...\n\tUnexpected key(s) in state_dict: \"conv1.weight\", \"bn1.weight\", \"bn1.bias\", \"bn1.running_mean\", \"bn1.running_var\", \"bn1.num_batches_tracked\", \"layer1.0.conv1.weight\", \"layer1.0.bn1.weight\", \"layer1.0.bn1.bias\", \"layer1.0.bn1.running_mean\", \"layer1.0.bn1.running_var\", \"layer1.0.bn1.num_batches_tracked\", \"layer1.0.conv2.weight\", \"layer1.0.bn2.weight\", \"layer1.0.bn2.bias\", \"layer1.0.bn2.running_mean\", \"layer1.0.bn2.running_var\", \"layer1.0.bn2.num_batches_tracked\", \"layer1.0.conv3.weight\", \"layer1.0.bn3.weight\", \"layer1.0.bn3.bias\", \"layer1.0.bn3.running_mean\", \"layer1.0.bn3.running_var\", \"layer1.0.bn3.num_batches_tracked\", \"layer1.0.downsample.0.weight\", \"layer1.0.downsample.1.weight\", \"layer1.0.downsample.1.bias\", \"layer1.0.downsample.1.running_mean\", \"layer1.0.downsample.1.running_var\", \"layer1.0.downsample.1.num_batches_tracked\", \"layer1.1.conv1.weight\", \"layer1.1.bn1.weight\", \"layer1.1.bn1.bias\", \"layer1.1.bn1.running_mean\", \"layer1.1.bn1.running_var\", \"layer1.1.bn1.num_batches_tracked\", \"layer1.1.conv2.weight\", \"layer1.1.bn2.weight\", \"layer1.1.bn2.bias\", \"layer1.1.bn2.running_mean\", \"layer1.1.bn2.running_var\", \"layer1.1.bn2.num_batches_tracked\", \"layer1.1.conv3.weight\", \"layer1.1.bn3.weight\", \"layer1.1.bn3.bias\", \"layer1.1.bn3.running_mean\", \"layer1.1.bn3.running_var\", \"layer1.1.bn3.num_batches_tracked\", \"layer1.2.conv1.weight\", \"layer1.2.bn1.weight\", \"layer1.2.bn1.bias\", \"layer1.2.bn1.running_mean\", \"layer1.2.bn1.running_var\", \"layer1.2.bn1.num_batches_tracked\", \"layer1..."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = VideoTransformer(\n",
        "    vocab_size=len(vocab),\n",
        "    d_model=512,\n",
        "    nhead=8,\n",
        "    num_layers=3,        # ✅ Match this!\n",
        "    dim_ff=2048,\n",
        "    max_len=45,\n",
        "    dropout=0.1,\n",
        "    input_feat_dim=2048\n",
        ").to(device)\n",
        "\n",
        "# Now load the matching weights\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/transformer_checkpoints/transformer_final.pt'))\n",
        "model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        },
        "id": "VHcOvFq8H2MR",
        "outputId": "7068145a-ee36-4b34-b175-b94de012ab8d"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for VideoTransformer:\n\tMissing key(s) in state_dict: \"embedding.weight\", \"pos_enc.pe\", \"vid_fc.weight\", \"vid_fc.bias\", \"decoder.layers.0.self_attn.in_proj_weight\", \"decoder.layers.0.self_attn.in_proj_bias\", \"decoder.layers.0.self_attn.out_proj.weight\", \"decoder.layers.0.self_attn.out_proj.bias\", \"decoder.layers.0.multihead_attn.in_proj_weight\", \"decoder.layers.0.multihead_attn.in_proj_bias\", \"decoder.layers.0.multihead_attn.out_proj.weight\", \"decoder.layers.0.multihead_attn.out_proj.bias\", \"decoder.layers.0.linear1.weight\", \"decoder.layers.0.linear1.bias\", \"decoder.layers.0.linear2.weight\", \"decoder.layers.0.linear2.bias\", \"decoder.layers.0.norm1.weight\", \"decoder.layers.0.norm1.bias\", \"decoder.layers.0.norm2.weight\", \"decoder.layers.0.norm2.bias\", \"decoder.layers.0.norm3.weight\", \"decoder.layers.0.norm3.bias\", \"decoder.layers.1.self_attn.in_proj_weight\", \"decoder.layers.1.self_attn.in_proj_bias\", \"decoder.layers.1.self_attn.out_proj.weight\", \"decoder.layers.1.self_attn.out_proj.bias\", \"decoder.layers.1.multihead_attn.in_proj_weight\", \"decoder.layers.1.multihead_attn.in_proj_bias\", \"decoder.layers.1.multihead_attn.out_proj.weight\", \"decoder.layers.1.multihead_attn.out_proj.bias\", \"decoder.layers.1.linear1.weight\", \"decoder.layers.1.linear1.bias\", \"decoder.layers.1.linear2.weight\", \"decoder.layers.1.linear2.bias\", \"decoder.layers.1.norm1.weight\", \"decoder.layers.1.norm1.bias\", \"decoder.layers.1.norm2.weight\", \"decoder.layers.1.norm2.bias\", \"decoder.layers.1.norm3.weight\", \"decoder.layers.1.norm3.bias\", \"decoder.layers.2.self_attn.in_proj_weight\", \"decoder.layers.2.self_attn.in_proj_bias\", \"decoder.layers.2.self_attn.out_proj.weight\", \"decoder.layers.2.self_attn.out_proj.bias\", \"decoder.layers.2.multihead_attn.in_proj_weight\", \"decoder.layers.2.multihead_attn.in_proj_bias\", \"decoder.layers.2.multihead_attn.out_proj.weight\", \"decoder.layers.2.multihead_attn.out_proj.bias\", \"decoder.layers.2.linear1.weight\", \"decoder.layers.2.linear1.bias\", \"decoder.layers.2.linear2.weight\", \"decoder.layers.2.linear2.bias\", \"decoder.layers.2.norm1.weight\", \"decoder.layers.2.norm1.bias\", \"decoder.layers.2.norm2.weight\", \"decoder.layers.2.norm2.bias\", \"decoder.layers.2.norm3.weight\", \"decoder.layers.2.norm3.bias\", \"output.weight\", \"output.bias\". \n\tUnexpected key(s) in state_dict: \"conv1.weight\", \"bn1.weight\", \"bn1.bias\", \"bn1.running_mean\", \"bn1.running_var\", \"bn1.num_batches_tracked\", \"layer1.0.conv1.weight\", \"layer1.0.bn1.weight\", \"layer1.0.bn1.bias\", \"layer1.0.bn1.running_mean\", \"layer1.0.bn1.running_var\", \"layer1.0.bn1.num_batches_tracked\", \"layer1.0.conv2.weight\", \"layer1.0.bn2.weight\", \"layer1.0.bn2.bias\", \"layer1.0.bn2.running_mean\", \"layer1.0.bn2.running_var\", \"layer1.0.bn2.num_batches_tracked\", \"layer1.0.conv3.weight\", \"layer1.0.bn3.weight\", \"layer1.0.bn3.bias\", \"layer1.0.bn3.running_mean\", \"layer1.0.bn3.running_var\", \"layer1.0.bn3.num_batches_tracked\", \"layer1.0.downsample.0.weight\", \"layer1.0.downsample.1.weight\", \"layer1.0.downsample.1.bias\", \"layer1.0.downsample.1.running_mean\", \"layer1.0.downsample.1.running_var\", \"layer1.0.downsample.1.num_batches_tracked\", \"layer1.1.conv1.weight\", \"layer1.1.bn1.weight\", \"layer1.1.bn1.bias\", \"layer1.1.bn1.running_mean\", \"layer1.1.bn1.running_var\", \"layer1.1.bn1.num_batches_tracked\", \"layer1.1.conv2.weight\", \"layer1.1.bn2.weight\", \"layer1.1.bn2.bias\", \"layer1.1.bn2.running_mean\", \"layer1.1.bn2.running_var\", \"layer1.1.bn2.num_batches_tracked\", \"layer1.1.conv3.weight\", \"layer1.1.bn3.weight\", \"layer1.1.bn3.bias\", \"layer1.1.bn3.running_mean\", \"layer1.1.bn3.running_var\", \"layer1.1.bn3.num_batches_tracked\", \"layer1.2.conv1.weight\", \"layer1.2.bn1.weight\", \"layer1.2.bn1.bias\", \"layer1.2.bn1.running_mean\", \"layer1.2.bn1.running_var\", \"layer1.2.bn1.num_batches_tracked\", \"layer1.2.conv2.weight\", \"layer1.2.bn2.weight\", \"layer1.2.bn2.bias\", \"layer1.2.bn2.running_mean\", \"layer1.2.bn2.running_var\", \"layer1.2.bn2.num_batches_tracked\", \"layer1.2.conv3.weight\", \"layer1.2.bn3.weight\", \"layer1.2.bn3.bias\", \"layer1.2.bn3.running_mean\", \"layer1.2.bn3.running_var\", \"layer1.2.bn3.num_batches_tracked\", \"layer2.0.conv1.weight\", \"layer2.0.bn1.weight\", \"layer2.0.bn1.bias\", \"layer2.0.bn1.running_mean\", \"layer2.0.bn1.running_var\", \"layer2.0.bn1.num_batches_tracked\", \"layer2.0.conv2.weight\", \"layer2.0.bn2.weight\", \"layer2.0.bn2.bias\", \"layer2.0.bn2.running_mean\", \"layer2.0.bn2.running_var\", \"layer2.0.bn2.num_batches_tracked\", \"layer2.0.conv3.weight\", \"layer2.0.bn3.weight\", \"layer2.0.bn3.bias\", \"layer2.0.bn3.running_mean\", \"layer2.0.bn3.running_var\", \"layer2.0.bn3.num_batches_tracked\", \"layer2.0.downsample.0.weight\", \"layer2.0.downsample.1.weight\", \"layer2.0.downsample.1.bias\", \"layer2.0.downsample.1.running_mean\", \"layer2.0.downsample.1.running_var\", \"layer2.0.downsample.1.num_batches_tracked\", \"layer2.1.conv1.weight\", \"layer2.1.bn1.weight\", \"layer2.1.bn1.bias\", \"layer2.1.bn1.running_mean\", \"layer2.1.bn1.running_var\", \"layer2.1.bn1.num_batches_tracked\", \"layer2.1.conv2.weight\", \"layer2.1.bn2.weight\", \"layer2.1.bn2.bias\", \"layer2.1.bn2.running_mean\", \"layer2.1.bn2.running_var\", \"layer2.1.bn2.num_batches_tracked\", \"layer2.1.conv3.weight\", \"layer2.1.bn3.weight\", \"layer2.1.bn3.bias\", \"layer2.1.bn3.running_mean\", \"layer2.1.bn3.running_var\", \"layer2.1.bn3.num_batches_tracked\", \"layer2.2.conv1.weight\", \"layer2.2.bn1.weight\", \"layer2.2.bn1.bias\", \"layer2.2.bn1.running_mean\", \"layer2.2.bn1.running_var\", \"layer2.2.bn1.num_batches_tracked\", \"layer2.2.conv2.weight\", \"layer2.2.bn2.weight\", \"layer2.2.bn2.bias\", \"layer2.2.bn2.running_mean\", \"layer2.2.bn2.running_var\", \"layer2.2.bn2.num_batches_tracked\", \"layer2.2.conv3.weight\", \"layer2.2.bn3.weight\", \"layer2.2.bn3.bias\", \"layer2.2.bn3.running_mean\", \"layer2.2.bn3.running_var\", \"layer2.2.bn3.num_batches_tracked\", \"layer2.3.conv1.weight\", \"layer2.3.bn1.weight\", \"layer2.3.bn1.bias\", \"layer2.3.bn1.running_mean\", \"layer2.3.bn1.running_var\", \"layer2.3.bn1.num_batches_tracked\", \"layer2.3.conv2.weight\", \"layer2.3.bn2.weight\", \"layer2.3.bn2.bias\", \"layer2.3.bn2.running_mean\", \"layer2.3.bn2.running_var\", \"layer2.3.bn2.num_batches_tracked\", \"layer2.3.conv3.weight\", \"layer2.3.bn3.weight\", \"layer2.3.bn3.bias\", \"layer2.3.bn3.running_mean\", \"layer2.3.bn3.running_var\", \"layer2.3.bn3.num_batches_tracked\", \"layer2.4.conv1.weight\", \"layer2.4.bn1.weight\", \"layer2.4.bn1.bias\", \"layer2.4.bn1.running_mean\", \"layer2.4.bn1.running_var\", \"layer2.4.bn1.num_batches_tracked\", \"layer2.4.conv2.weight\", \"layer2.4.bn2.weight\", \"layer2.4.bn2.bias\", \"layer2.4.bn2.running_mean\", \"layer2.4.bn2.running_var\", \"layer2.4.bn2.num_batches_tracked\", \"layer2.4.conv3.weight\", \"layer2.4.bn3.weight\", \"layer2.4.bn3.bias\", \"layer2.4.bn3.running_mean\", \"layer2.4.bn3.running_var\", \"layer2.4.bn3.num_batches_tracked\", \"layer2.5.conv1.weight\", \"layer2.5.bn1.weight\", \"layer2.5.bn1.bias\", \"layer2.5.bn1.running_mean\", \"layer2.5.bn1.running_var\", \"layer2.5.bn1.num_batches_tracked\", \"layer2.5.conv2.weight\", \"layer2.5.bn2.weight\", \"layer2.5.bn2.bias\", \"layer2.5.bn2.running_mean\", \"layer2.5.bn2.running_var\", \"layer2.5.bn2.num_batches_tracked\", \"layer2.5.conv3.weight\", \"layer2.5.bn3.weight\", \"layer2.5.bn3.bias\", \"layer2.5.bn3.running_mean\", \"layer2.5.bn3.running_var\", \"layer2.5.bn3.num_batches_tracked\", \"layer2.6.conv1.weight\", \"layer2.6.bn1.weight\", \"layer2.6.bn1.bias\", \"layer2.6.bn1.running_mean\", \"layer2.6.bn1.running_var\", \"layer2.6.bn1.num_batches_tracked\", \"layer2.6.conv2.weight\", \"layer2.6.bn2.weight\", \"layer2.6.bn2.bias\", \"layer2.6.bn2.running_mean\", \"layer2.6.bn2.running_var\", \"layer2.6.bn2.num_batches_tracked\", \"layer2.6.conv3.weight\", \"layer2.6.bn3.weight\", \"layer2.6.bn3.bias\", \"layer2.6.bn3.running_mean\", \"layer2.6.bn3.running_var\", \"layer2.6.bn3.num_batches_tracked\", \"layer2.7.conv1.weight\", \"layer2.7.bn1.weight\", \"layer2.7.bn1.bias\", \"layer2.7.bn1.running_mean\", \"layer2.7.bn1.running_var\", \"layer2.7.bn1.num_batches_tracked\", \"layer2.7.conv2.weight\", \"layer2.7.bn2.weight\", \"layer2.7.bn2.bias\", \"layer2.7.bn2.running_mean\", \"layer2.7.bn2.running_var\", \"layer2.7.bn2.num_batches_tracked\", \"layer2.7.conv3.weight\", \"layer2.7.bn3.weight\", \"layer2.7.bn3.bias\", \"layer2.7.bn3.running_mean\", \"layer2.7.bn3.running_var\", \"layer2.7.bn3.num_batches_tracked\", \"layer3.0.conv1.weight\", \"layer3.0.bn1.weight\", \"layer3.0.bn1.bias\", \"layer3.0.bn1.running_mean\", \"layer3.0.bn1.running_var\", \"layer3.0.bn1.num_batches_tracked\", \"layer3.0.conv2.weight\", \"layer3.0.bn2.weight\", \"layer3.0.bn2.bias\", \"layer3.0.bn2.running_mean\", \"layer3.0.bn2.running_var\", \"layer3.0.bn2.num_batches_tracked\", \"layer3.0.conv3.weight\", \"layer3.0.bn3.weight\", \"layer3.0.bn3.bias\", \"layer3.0.bn3.running_mean\", \"layer3.0.bn3.running_var\", \"layer3.0.bn3.num_batches_tracked\", \"layer3.0.downsample.0.weight\", \"layer3.0.downsample.1.weight\", \"layer3.0.downsample.1.bias\", \"layer3.0.downsample.1.running_mean\", \"layer3.0.downsample.1.running_var\", \"layer3.0.downsample.1.num_batches_tracked\", \"layer3.1.conv1.weight\", \"layer3.1.bn1.weight\", \"layer3.1.bn1.bias\", \"layer3.1.bn1.running_mean\", \"layer3.1.bn1.running_var\", \"layer3.1.bn1.num_batches_tracked\", \"layer3.1.conv2.weight\", \"layer3.1.bn2.weight\", \"layer3.1.bn2.bias\", \"layer3.1.bn2.running_mean\", \"layer3.1.bn2.running_var\", \"layer3.1.bn2.num_batches_tracked\", \"layer3.1.conv3.weight\", \"layer3.1.bn3.weight\", \"layer3.1.bn3.bias\", \"layer3.1.bn3.running_mean\", \"layer3.1.bn3.running_var\", \"layer3.1.bn3.num_batches_tracked\", \"layer3.2.conv1.weight\", \"layer3.2.bn1.weight\", \"layer3.2.bn1.bias\", \"layer3.2.bn1.running_mean\", \"layer3.2.bn1.running_var\", \"layer3.2.bn1.num_batches_tracked\", \"layer3.2.conv2.weight\", \"layer3.2.bn2.weight\", \"layer3.2.bn2.bias\", \"layer3.2.bn2.running_mean\", \"layer3.2.bn2.running_var\", \"layer3.2.bn2.num_batches_tracked\", \"layer3.2.conv3.weight\", \"layer3.2.bn3.weight\", \"layer3.2.bn3.bias\", \"layer3.2.bn3.running_mean\", \"layer3.2.bn3.running_var\", \"layer3.2.bn3.num_batches_tracked\", \"layer3.3.conv1.weight\", \"layer3.3.bn1.weight\", \"layer3.3.bn1.bias\", \"layer3.3.bn1.running_mean\", \"layer3.3.bn1.running_var\", \"layer3.3.bn1.num_batches_tracked\", \"layer3.3.conv2.weight\", \"layer3.3.bn2.weight\", \"layer3.3.bn2.bias\", \"layer3.3.bn2.running_mean\", \"layer3.3.bn2.running_var\", \"layer3.3.bn2.num_batches_tracked\", \"layer3.3.conv3.weight\", \"layer3.3.bn3.weight\", \"layer3.3.bn3.bias\", \"layer3.3.bn3.running_mean\", \"layer3.3.bn3.running_var\", \"layer3.3.bn3.num_batches_tracked\", \"layer3.4.conv1.weight\", \"layer3.4.bn1.weight\", \"layer3.4.bn1.bias\", \"layer3.4.bn1.running_mean\", \"layer3.4.bn1.running_var\", \"layer3.4.bn1.num_batches_tracked\", \"layer3.4.conv2.weight\", \"layer3.4.bn2.weight\", \"layer3.4.bn2.bias\", \"layer3.4.bn2.running_mean\", \"layer3.4.bn2.running_var\", \"layer3.4.bn2.num_batches_tracked\", \"layer3.4.conv3.weight\", \"layer3.4.bn3.weight\", \"layer3.4.bn3.bias\", \"layer3.4.bn3.running_mean\", \"layer3.4.bn3.running_var\", \"layer3.4.bn3.num_batches_tracked\", \"layer3.5.conv1.weight\", \"layer3.5.bn1.weight\", \"layer3.5.bn1.bias\", \"layer3.5.bn1.running_mean\", \"layer3.5.bn1.running_var\", \"layer3.5.bn1.num_batches_tracked\", \"layer3.5.conv2.weight\", \"layer3.5.bn2.weight\", \"layer3.5.bn2.bias\", \"layer3.5.bn2.running_mean\", \"layer3.5.bn2.running_var\", \"layer3.5.bn2.num_batches_tracked\", \"layer3.5.conv3.weight\", \"layer3.5.bn3.weight\", \"layer3.5.bn3.bias\", \"layer3.5.bn3.running_mean\", \"layer3.5.bn3.running_var\", \"layer3.5.bn3.num_batches_tracked\", \"layer3.6.conv1.weight\", \"layer3.6.bn1.weight\", \"layer3.6.bn1.bias\", \"layer3.6.bn1.running_mean\", \"layer3.6.bn1.running_var\", \"layer3.6.bn1.num_batches_tracked\", \"layer3.6.conv2.weight\", \"layer3.6.bn2.weight\", \"layer3.6.bn2.bias\", \"layer3.6.bn2.running_mean\", \"layer3.6.bn2.running_var\", \"layer3.6.bn2.num_batches_tracked\", \"layer3.6.conv3.weight\", \"layer3.6.bn3.weight\", \"layer3.6.bn3.bias\", \"layer3.6.bn3.running_mean\", \"layer3.6.bn3.running_var\", \"layer3.6.bn3.num_batches_tracked\", \"layer3.7.conv1.weight\", \"layer3.7.bn1.weight\", \"layer3.7.bn1.bias\", \"layer3.7.bn1.running_mean\", \"layer3.7.bn1.running_var\", \"layer3.7.bn1.num_batches_tracked\", \"layer3.7.conv2.weight\", \"layer3.7.bn2.weight\", \"layer3.7.bn2.bias\", \"layer3.7.bn2.running_mean\", \"layer3.7.bn2.running_var\", \"layer3.7.bn2.num_batches_tracked\", \"layer3.7.conv3.weight\", \"layer3.7.bn3.weight\", \"layer3.7.bn3.bias\", \"layer3.7.bn3.running_mean\", \"layer3.7.bn3.running_var\", \"layer3.7.bn3.num_batches_tracked\", \"layer3.8.conv1.weight\", \"layer3.8.bn1.weight\", \"layer3.8.bn1.bias\", \"layer3.8.bn1.running_mean\", \"layer3.8.bn1.running_var\", \"layer3.8.bn1.num_batches_tracked\", \"layer3.8.conv2.weight\", \"layer3.8.bn2.weight\", \"layer3.8.bn2.bias\", \"layer3.8.bn2.running_mean\", \"layer3.8.bn2.running_var\", \"layer3.8.bn2.num_batches_tracked\", \"layer3.8.conv3.weight\", \"layer3.8.bn3.weight\", \"layer3.8.bn3.bias\", \"layer3.8.bn3.running_mean\", \"layer3.8.bn3.running_var\", \"layer3.8.bn3.num_batches_tracked\", \"layer3.9.conv1.weight\", \"layer3.9.bn1.weight\", \"layer3.9.bn1.bias\", \"layer3.9.bn1.running_mean\", \"layer3.9.bn1.running_var\", \"layer3.9.bn1.num_batches_tracked\", \"layer3.9.conv2.weight\", \"layer3.9.bn2.weight\", \"layer3.9.bn2.bias\", \"layer3.9.bn2.running_mean\", \"layer3.9.bn2.running_var\", \"layer3.9.bn2.num_batches_tracked\", \"layer3.9.conv3.weight\", \"layer3.9.bn3.weight\", \"layer3.9.bn3.bias\", \"layer3.9.bn3.running_mean\", \"layer3.9.bn3.running_var\", \"layer3.9.bn3.num_batches_tracked\", \"layer3.10.conv1.weight\", \"layer3.10.bn1.weight\", \"layer3.10.bn1.bias\", \"layer3.10.bn1.running_mean\", \"layer3.10.bn1.running_var\", \"layer3.10.bn1.num_batches_tracked\", \"layer3.10.conv2.weight\", \"layer3.10.bn2.weight\", \"layer3.10.bn2.bias\", \"layer3.10.bn2.running_mean\", \"layer3.10.bn2.running_var\", \"layer3.10.bn2.num_batches_tracked\", \"layer3.10.conv3.weight\", \"layer3.10.bn3.weight\", \"layer3.10.bn3.bias\", \"layer3.10.bn3.running_mean\", \"layer3.10.bn3.running_var\", \"layer3.10.bn3.num_batches_tracked\", \"layer3.11.conv1.weight\", \"layer3.11.bn1.weight\", \"layer3.11.bn1.bias\", \"layer3.11.bn1.running_mean\", \"layer3.11.bn1.running_var\", \"layer3.11.bn1.num_batches_tracked\", \"layer3.11.conv2.weight\", \"layer3.11.bn2.weight\", \"layer3.11.bn2.bias\", \"layer3.11.bn2.running_mean\", \"layer3.11.bn2.running_var\", \"layer3.11.bn2.num_batches_tracked\", \"layer3.11.conv3.weight\", \"layer3.11.bn3.weight\", \"layer3.11.bn3.bias\", \"layer3.11.bn3.running_mean\", \"layer3.11.bn3.running_var\", \"layer3.11.bn3.num_batches_tracked\", \"layer3.12.conv1.weight\", \"layer3.12.bn1.weight\", \"layer3.12.bn1.bias\", \"layer3.12.bn1.running_mean\", \"layer3.12.bn1.running_var\", \"layer3.12.bn1.num_batches_tracked\", \"layer3.12.conv2.weight\", \"layer3.12.bn2.weight\", \"layer3.12.bn2.bias\", \"layer3.12.bn2.running_mean\", \"layer3.12.bn2.running_var\", \"layer3.12.bn2.num_batches_tracked\", \"layer3.12.conv3.weight\", \"layer3.12.bn3.weight\", \"layer3.12.bn3.bias\", \"layer3.12.bn3.running_mean\", \"layer3.12.bn3.running_var\", \"layer3.12.bn3.num_batches_tracked\", \"layer3.13.conv1.weight\", \"layer3.13.bn1.weight\", \"layer3.13.bn1.bias\", \"layer3.13.bn1.running_mean\", \"layer3.13.bn1.running_var\", \"layer3.13.bn1.num_batches_tracked\", \"layer3.13.conv2.weight\", \"layer3.13.bn2.weight\", \"layer3.13.bn2.bias\", \"layer3.13.bn2.running_mean\", \"layer3.13.bn2.running_var\", \"layer3.13.bn2.num_batches_tracked\", \"layer3.13.conv3.weight\", \"layer3.13.bn3.weight\", \"layer3.13.bn3.bias\", \"layer3.13.bn3.running_mean\", \"layer3.13.bn3.running_var\", \"layer3.13.bn3.num_batches_tracked\", \"layer3.14.conv1.weight\", \"layer3.14.bn1.weight\", \"layer3.14.bn1.bias\", \"layer3.14.bn1.running_mean\", \"layer3.14.bn1.running_var\", \"layer3.14.bn1.num_batches_tracked\", \"layer3.14.conv2.weight\", \"layer3.14.bn2.weight\", \"layer3.14.bn2.bias\", \"layer3.14.bn2.running_mean\", \"layer3.14.bn2.running_var\", \"layer3.14.bn2.num_batches_tracked\", \"layer3.14.conv3.weight\", \"layer3.14.bn3.weight\", \"layer3.14.bn3.bias\", \"layer3.14.bn3.running_mean\", \"layer3.14.bn3.running_var\", \"layer3.14.bn3.num_batches_tracked\", \"layer3.15.conv1.weight\", \"layer3.15.bn1.weight\", \"layer3.15.bn1.bias\", \"layer3.15.bn1.running_mean\", \"layer3.15.bn1.running_var\", \"layer3.15.bn1.num_batches_tracked\", \"layer3.15.conv2.weight\", \"layer3.15.bn2.weight\", \"layer3.15.bn2.bias\", \"layer3.15.bn2.running_mean\", \"layer3.15.bn2.running_var\", \"layer3.15.bn2.num_batches_tracked\", \"layer3.15.conv3.weight\", \"layer3.15.bn3.weight\", \"layer3.15.bn3.bias\", \"layer3.15.bn3.running_mean\", \"layer3.15.bn3.running_var\", \"layer3.15.bn3.num_batches_tracked\", \"layer3.16.conv1.weight\", \"layer3.16.bn1.weight\", \"layer3.16.bn1.bias\", \"layer3.16.bn1.running_mean\", \"layer3.16.bn1.running_var\", \"layer3.16.bn1.num_batches_tracked\", \"layer3.16.conv2.weight\", \"layer3.16.bn2.weight\", \"layer3.16.bn2.bias\", \"layer3.16.bn2.running_mean\", \"layer3.16.bn2.running_var\", \"layer3.16.bn2.num_batches_tracked\", \"layer3.16.conv3.weight\", \"layer3.16.bn3.weight\", \"layer3.16.bn3.bias\", \"layer3.16.bn3.running_mean\", \"layer3.16.bn3.running_var\", \"layer3.16.bn3.num_batches_tracked\", \"layer3.17.conv1.weight\", \"layer3.17.bn1.weight\", \"layer3.17.bn1.bias\", \"layer3.17.bn1.running_mean\", \"layer3.17.bn1.running_var\", \"layer3.17.bn1.num_batches_tracked\", \"layer3.17.conv2.weight\", \"layer3.17.bn2.weight\", \"layer3.17.bn2.bias\", \"layer3.17.bn2.running_mean\", \"layer3.17.bn2.running_var\", \"layer3.17.bn2.num_batches_tracked\", \"layer3.17.conv3.weight\", \"layer3.17.bn3.weight\", \"layer3.17.bn3.bias\", \"layer3.17.bn3.running_mean\", \"layer3.17.bn3.running_var\", \"layer3.17.bn3.num_batches_tracked\", \"layer3.18.conv1.weight\", \"layer3.18.bn1.weight\", \"layer3.18.bn1.bias\", \"layer3.18.bn1.running_mean\", \"layer3.18.bn1.running_var\", \"layer3.18.bn1.num_batches_tracked\", \"layer3.18.conv2.weight\", \"layer3.18.bn2.weight\", \"layer3.18.bn2.bias\", \"layer3.18.bn2.running_mean\", \"layer3.18.bn2.running_var\", \"layer3.18.bn2.num_batches_tracked\", \"layer3.18.conv3.weight\", \"layer3.18.bn3.weight\", \"layer3.18.bn3.bias\", \"layer3.18.bn3.running_mean\", \"layer3.18.bn3.running_var\", \"layer3.18.bn3.num_batches_tracked\", \"layer3.19.conv1.weight\", \"layer3.19.bn1.weight\", \"layer3.19.bn1.bias\", \"layer3.19.bn1.running_mean\", \"layer3.19.bn1.running_var\", \"layer3.19.bn1.num_batches_tracked\", \"layer3.19.conv2.weight\", \"layer3.19.bn2.weight\", \"layer3.19.bn2.bias\", \"layer3.19.bn2.running_mean\", \"layer3.19.bn2.running_var\", \"layer3.19.bn2.num_batches_tracked\", \"layer3.19.conv3.weight\", \"layer3.19.bn3.weight\", \"layer3.19.bn3.bias\", \"layer3.19.bn3.running_mean\", \"layer3.19.bn3.running_var\", \"layer3.19.bn3.num_batches_tracked\", \"layer3.20.conv1.weight\", \"layer3.20.bn1.weight\", \"layer3.20.bn1.bias\", \"layer3.20.bn1.running_mean\", \"layer3.20.bn1.running_var\", \"layer3.20.bn1.num_batches_tracked\", \"layer3.20.conv2.weight\", \"layer3.20.bn2.weight\", \"layer3.20.bn2.bias\", \"layer3.20.bn2.running_mean\", \"layer3.20.bn2.running_var\", \"layer3.20.bn2.num_batches_tracked\", \"layer3.20.conv3.weight\", \"layer3.20.bn3.weight\", \"layer3.20.bn3.bias\", \"layer3.20.bn3.running_mean\", \"layer3.20.bn3.running_var\", \"layer3.20.bn3.num_batches_tracked\", \"layer3.21.conv1.weight\", \"layer3.21.bn1.weight\", \"layer3.21.bn1.bias\", \"layer3.21.bn1.running_mean\", \"layer3.21.bn1.running_var\", \"layer3.21.bn1.num_batches_tracked\", \"layer3.21.conv2.weight\", \"layer3.21.bn2.weight\", \"layer3.21.bn2.bias\", \"layer3.21.bn2.running_mean\", \"layer3.21.bn2.running_var\", \"layer3.21.bn2.num_batches_tracked\", \"layer3.21.conv3.weight\", \"layer3.21.bn3.weight\", \"layer3.21.bn3.bias\", \"layer3.21.bn3.running_mean\", \"layer3.21.bn3.running_var\", \"layer3.21.bn3.num_batches_tracked\", \"layer3.22.conv1.weight\", \"layer3.22.bn1.weight\", \"layer3.22.bn1.bias\", \"layer3.22.bn1.running_mean\", \"layer3.22.bn1.running_var\", \"layer3.22.bn1.num_batches_tracked\", \"layer3.22.conv2.weight\", \"layer3.22.bn2.weight\", \"layer3.22.bn2.bias\", \"layer3.22.bn2.running_mean\", \"layer3.22.bn2.running_var\", \"layer3.22.bn2.num_batches_tracked\", \"layer3.22.conv3.weight\", \"layer3.22.bn3.weight\", \"layer3.22.bn3.bias\", \"layer3.22.bn3.running_mean\", \"layer3.22.bn3.running_var\", \"layer3.22.bn3.num_batches_tracked\", \"layer3.23.conv1.weight\", \"layer3.23.bn1.weight\", \"layer3.23.bn1.bias\", \"layer3.23.bn1.running_mean\", \"layer3.23.bn1.running_var\", \"layer3.23.bn1.num_batches_tracked\", \"layer3.23.conv2.weight\", \"layer3.23.bn2.weight\", \"layer3.23.bn2.bias\", \"layer3.23.bn2.running_mean\", \"layer3.23.bn2.running_var\", \"layer3.23.bn2.num_batches_tracked\", \"layer3.23.conv3.weight\", \"layer3.23.bn3.weight\", \"layer3.23.bn3.bias\", \"layer3.23.bn3.running_mean\", \"layer3.23.bn3.running_var\", \"layer3.23.bn3.num_batches_tracked\", \"layer3.24.conv1.weight\", \"layer3.24.bn1.weight\", \"layer3.24.bn1.bias\", \"layer3.24.bn1.running_mean\", \"layer3.24.bn1.running_var\", \"layer3.24.bn1.num_batches_tracked\", \"layer3.24.conv2.weight\", \"layer3.24.bn2.weight\", \"layer3.24.bn2.bias\", \"layer3.24.bn2.running_mean\", \"layer3.24.bn2.running_var\", \"layer3.24.bn2.num_batches_tracked\", \"layer3.24.conv3.weight\", \"layer3.24.bn3.weight\", \"layer3.24.bn3.bias\", \"layer3.24.bn3.running_mean\", \"layer3.24.bn3.running_var\", \"layer3.24.bn3.num_batches_tracked\", \"layer3.25.conv1.weight\", \"layer3.25.bn1.weight\", \"layer3.25.bn1.bias\", \"layer3.25.bn1.running_mean\", \"layer3.25.bn1.running_var\", \"layer3.25.bn1.num_batches_tracked\", \"layer3.25.conv2.weight\", \"layer3.25.bn2.weight\", \"layer3.25.bn2.bias\", \"layer3.25.bn2.running_mean\", \"layer3.25.bn2.running_var\", \"layer3.25.bn2.num_batches_tracked\", \"layer3.25.conv3.weight\", \"layer3.25.bn3.weight\", \"layer3.25.bn3.bias\", \"layer3.25.bn3.running_mean\", \"layer3.25.bn3.running_var\", \"layer3.25.bn3.num_batches_tracked\", \"layer3.26.conv1.weight\", \"layer3.26.bn1.weight\", \"layer3.26.bn1.bias\", \"layer3.26.bn1.running_mean\", \"layer3.26.bn1.running_var\", \"layer3.26.bn1.num_batches_tracked\", \"layer3.26.conv2.weight\", \"layer3.26.bn2.weight\", \"layer3.26.bn2.bias\", \"layer3.26.bn2.running_mean\", \"layer3.26.bn2.running_var\", \"layer3.26.bn2.num_batches_tracked\", \"layer3.26.conv3.weight\", \"layer3.26.bn3.weight\", \"layer3.26.bn3.bias\", \"layer3.26.bn3.running_mean\", \"layer3.26.bn3.running_var\", \"layer3.26.bn3.num_batches_tracked\", \"layer3.27.conv1.weight\", \"layer3.27.bn1.weight\", \"layer3.27.bn1.bias\", \"layer3.27.bn1.running_mean\", \"layer3.27.bn1.running_var\", \"layer3.27.bn1.num_batches_tracked\", \"layer3.27.conv2.weight\", \"layer3.27.bn2.weight\", \"layer3.27.bn2.bias\", \"layer3.27.bn2.running_mean\", \"layer3.27.bn2.running_var\", \"layer3.27.bn2.num_batches_tracked\", \"layer3.27.conv3.weight\", \"layer3.27.bn3.weight\", \"layer3.27.bn3.bias\", \"layer3.27.bn3.running_mean\", \"layer3.27.bn3.running_var\", \"layer3.27.bn3.num_batches_tracked\", \"layer3.28.conv1.weight\", \"layer3.28.bn1.weight\", \"layer3.28.bn1.bias\", \"layer3.28.bn1.running_mean\", \"layer3.28.bn1.running_var\", \"layer3.28.bn1.num_batches_tracked\", \"layer3.28.conv2.weight\", \"layer3.28.bn2.weight\", \"layer3.28.bn2.bias\", \"layer3.28.bn2.running_mean\", \"layer3.28.bn2.running_var\", \"layer3.28.bn2.num_batches_tracked\", \"layer3.28.conv3.weight\", \"layer3.28.bn3.weight\", \"layer3.28.bn3.bias\", \"layer3.28.bn3.running_mean\", \"layer3.28.bn3.running_var\", \"layer3.28.bn3.num_batches_tracked\", \"layer3.29.conv1.weight\", \"layer3.29.bn1.weight\", \"layer3.29.bn1.bias\", \"layer3.29.bn1.running_mean\", \"layer3.29.bn1.running_var\", \"layer3.29.bn1.num_batches_tracked\", \"layer3.29.conv2.weight\", \"layer3.29.bn2.weight\", \"layer3.29.bn2.bias\", \"layer3.29.bn2.running_mean\", \"layer3.29.bn2.running_var\", \"layer3.29.bn2.num_batches_tracked\", \"layer3.29.conv3.weight\", \"layer3.29.bn3.weight\", \"layer3.29.bn3.bias\", \"layer3.29.bn3.running_mean\", \"layer3.29.bn3.running_var\", \"layer3.29.bn3.num_batches_tracked\", \"layer3.30.conv1.weight\", \"layer3.30.bn1.weight\", \"layer3.30.bn1.bias\", \"layer3.30.bn1.running_mean\", \"layer3.30.bn1.running_var\", \"layer3.30.bn1.num_batches_tracked\", \"layer3.30.conv2.weight\", \"layer3.30.bn2.weight\", \"layer3.30.bn2.bias\", \"layer3.30.bn2.running_mean\", \"layer3.30.bn2.running_var\", \"layer3.30.bn2.num_batches_tracked\", \"layer3.30.conv3.weight\", \"layer3.30.bn3.weight\", \"layer3.30.bn3.bias\", \"layer3.30.bn3.running_mean\", \"layer3.30.bn3.running_var\", \"layer3.30.bn3.num_batches_tracked\", \"layer3.31.conv1.weight\", \"layer3.31.bn1.weight\", \"layer3.31.bn1.bias\", \"layer3.31.bn1.running_mean\", \"layer3.31.bn1.running_var\", \"layer3.31.bn1.num_batches_tracked\", \"layer3.31.conv2.weight\", \"layer3.31.bn2.weight\", \"layer3.31.bn2.bias\", \"layer3.31.bn2.running_mean\", \"layer3.31.bn2.running_var\", \"layer3.31.bn2.num_batches_tracked\", \"layer3.31.conv3.weight\", \"layer3.31.bn3.weight\", \"layer3.31.bn3.bias\", \"layer3.31.bn3.running_mean\", \"layer3.31.bn3.running_var\", \"layer3.31.bn3.num_batches_tracked\", \"layer3.32.conv1.weight\", \"layer3.32.bn1.weight\", \"layer3.32.bn1.bias\", \"layer3.32.bn1.running_mean\", \"layer3.32.bn1.running_var\", \"layer3.32.bn1.num_batches_tracked\", \"layer3.32.conv2.weight\", \"layer3.32.bn2.weight\", \"layer3.32.bn2.bias\", \"layer3.32.bn2.running_mean\", \"layer3.32.bn2.running_var\", \"layer3.32.bn2.num_batches_tracked\", \"layer3.32.conv3.weight\", \"layer3.32.bn3.weight\", \"layer3.32.bn3.bias\", \"layer3.32.bn3.running_mean\", \"layer3.32.bn3.running_var\", \"layer3.32.bn3.num_batches_tracked\", \"layer3.33.conv1.weight\", \"layer3.33.bn1.weight\", \"layer3.33.bn1.bias\", \"layer3.33.bn1.running_mean\", \"layer3.33.bn1.running_var\", \"layer3.33.bn1.num_batches_tracked\", \"layer3.33.conv2.weight\", \"layer3.33.bn2.weight\", \"layer3.33.bn2.bias\", \"layer3.33.bn2.running_mean\", \"layer3.33.bn2.running_var\", \"layer3.33.bn2.num_batches_tracked\", \"layer3.33.conv3.weight\", \"layer3.33.bn3.weight\", \"layer3.33.bn3.bias\", \"layer3.33.bn3.running_mean\", \"layer3.33.bn3.running_var\", \"layer3.33.bn3.num_batches_tracked\", \"layer3.34.conv1.weight\", \"layer3.34.bn1.weight\", \"layer3.34.bn1.bias\", \"layer3.34.bn1.running_mean\", \"layer3.34.bn1.running_var\", \"layer3.34.bn1.num_batches_tracked\", \"layer3.34.conv2.weight\", \"layer3.34.bn2.weight\", \"layer3.34.bn2.bias\", \"layer3.34.bn2.running_mean\", \"layer3.34.bn2.running_var\", \"layer3.34.bn2.num_batches_tracked\", \"layer3.34.conv3.weight\", \"layer3.34.bn3.weight\", \"layer3.34.bn3.bias\", \"layer3.34.bn3.running_mean\", \"layer3.34.bn3.running_var\", \"layer3.34.bn3.num_batches_tracked\", \"layer3.35.conv1.weight\", \"layer3.35.bn1.weight\", \"layer3.35.bn1.bias\", \"layer3.35.bn1.running_mean\", \"layer3.35.bn1.running_var\", \"layer3.35.bn1.num_batches_tracked\", \"layer3.35.conv2.weight\", \"layer3.35.bn2.weight\", \"layer3.35.bn2.bias\", \"layer3.35.bn2.running_mean\", \"layer3.35.bn2.running_var\", \"layer3.35.bn2.num_batches_tracked\", \"layer3.35.conv3.weight\", \"layer3.35.bn3.weight\", \"layer3.35.bn3.bias\", \"layer3.35.bn3.running_mean\", \"layer3.35.bn3.running_var\", \"layer3.35.bn3.num_batches_tracked\", \"layer4.0.conv1.weight\", \"layer4.0.bn1.weight\", \"layer4.0.bn1.bias\", \"layer4.0.bn1.running_mean\", \"layer4.0.bn1.running_var\", \"layer4.0.bn1.num_batches_tracked\", \"layer4.0.conv2.weight\", \"layer4.0.bn2.weight\", \"layer4.0.bn2.bias\", \"layer4.0.bn2.running_mean\", \"layer4.0.bn2.running_var\", \"layer4.0.bn2.num_batches_tracked\", \"layer4.0.conv3.weight\", \"layer4.0.bn3.weight\", \"layer4.0.bn3.bias\", \"layer4.0.bn3.running_mean\", \"layer4.0.bn3.running_var\", \"layer4.0.bn3.num_batches_tracked\", \"layer4.0.downsample.0.weight\", \"layer4.0.downsample.1.weight\", \"layer4.0.downsample.1.bias\", \"layer4.0.downsample.1.running_mean\", \"layer4.0.downsample.1.running_var\", \"layer4.0.downsample.1.num_batches_tracked\", \"layer4.1.conv1.weight\", \"layer4.1.bn1.weight\", \"layer4.1.bn1.bias\", \"layer4.1.bn1.running_mean\", \"layer4.1.bn1.running_var\", \"layer4.1.bn1.num_batches_tracked\", \"layer4.1.conv2.weight\", \"layer4.1.bn2.weight\", \"layer4.1.bn2.bias\", \"layer4.1.bn2.running_mean\", \"layer4.1.bn2.running_var\", \"layer4.1.bn2.num_batches_tracked\", \"layer4.1.conv3.weight\", \"layer4.1.bn3.weight\", \"layer4.1.bn3.bias\", \"layer4.1.bn3.running_mean\", \"layer4.1.bn3.running_var\", \"layer4.1.bn3.num_batches_tracked\", \"layer4.2.conv1.weight\", \"layer4.2.bn1.weight\", \"layer4.2.bn1.bias\", \"layer4.2.bn1.running_mean\", \"layer4.2.bn1.running_var\", \"layer4.2.bn1.num_batches_tracked\", \"layer4.2.conv2.weight\", \"layer4.2.bn2.weight\", \"layer4.2.bn2.bias\", \"layer4.2.bn2.running_mean\", \"layer4.2.bn2.running_var\", \"layer4.2.bn2.num_batches_tracked\", \"layer4.2.conv3.weight\", \"layer4.2.bn3.weight\", \"layer4.2.bn3.bias\", \"layer4.2.bn3.running_mean\", \"layer4.2.bn3.running_var\", \"layer4.2.bn3.num_batches_tracked\". ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-22eb694b7785>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Now load the matching weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/transformer_checkpoints/transformer_final.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2581\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   2582\u001b[0m                 \"Error(s) in loading state_dict for {}:\\n\\t{}\".format(\n\u001b[1;32m   2583\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\\t\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for VideoTransformer:\n\tMissing key(s) in state_dict: \"embedding.weight\", \"pos_enc.pe\", \"vid_fc.weight\", \"vid_fc.bias\", \"decoder.layers.0.self_attn.in_proj_weight\", \"decoder.layers.0.self_attn.in_proj_bias\", \"decoder.layers.0.self_attn.out_proj.weight\", \"decoder.layers.0.self_attn.out_proj.bias\", \"decoder.layers.0.multihead_attn.in_proj_weight\", \"decoder.layers.0.multihead_attn.in_proj_bias\", \"decoder.layers.0.multihead_attn.out_proj.weight\", \"decoder.layers.0.multihead_attn.out_proj.bias\", \"decoder.layers.0.linear1.weight\", \"decoder.layers.0.linear1.bias\", \"decoder.layers.0.linear2.weight\", \"decoder.layers.0.linear2.bias\", \"decoder.layers.0.norm1.weight\", \"decoder.layers.0.norm1.bias\", \"decoder.layers.0.norm2.weight\", \"decoder.layers.0.norm2.bias\", \"decoder.layers.0.norm3.weight\", \"decoder.layers.0.norm3.bias\", \"decoder.layers.1.self_attn.in_proj_weight\", \"decoder.layers.1.self_attn.in_proj_bias\", \"decoder.layers.1.self_attn.out_proj.weight\", \"decoder.layers.1.self_attn.out_proj.bias\", \"decoder.layers.1.multihead_attn.in_proj_weight\", \"decoder.layers.1.multihead_attn.in_proj_bias\", \"decoder.layers.1.multihead_attn.out_proj.weight\", \"decoder.layers.1.multihead_attn.out_proj.bias\", \"decoder.layers.1.linear1.weight\", \"decoder.layers.1.linear1.bias\", \"decoder.layers.1.linear2.weight\", \"decoder.layers.1.linear2.bias\", \"decoder.layers.1.norm1.weight\", \"decoder.layers.1.norm1.bias\", \"decoder.layers.1.norm2.weight\", \"decoder.layers.1.norm2.bias\", \"decoder.layers.1.norm3.weight\", \"decoder.layers.1.norm3...\n\tUnexpected key(s) in state_dict: \"conv1.weight\", \"bn1.weight\", \"bn1.bias\", \"bn1.running_mean\", \"bn1.running_var\", \"bn1.num_batches_tracked\", \"layer1.0.conv1.weight\", \"layer1.0.bn1.weight\", \"layer1.0.bn1.bias\", \"layer1.0.bn1.running_mean\", \"layer1.0.bn1.running_var\", \"layer1.0.bn1.num_batches_tracked\", \"layer1.0.conv2.weight\", \"layer1.0.bn2.weight\", \"layer1.0.bn2.bias\", \"layer1.0.bn2.running_mean\", \"layer1.0.bn2.running_var\", \"layer1.0.bn2.num_batches_tracked\", \"layer1.0.conv3.weight\", \"layer1.0.bn3.weight\", \"layer1.0.bn3.bias\", \"layer1.0.bn3.running_mean\", \"layer1.0.bn3.running_var\", \"layer1.0.bn3.num_batches_tracked\", \"layer1.0.downsample.0.weight\", \"layer1.0.downsample.1.weight\", \"layer1.0.downsample.1.bias\", \"layer1.0.downsample.1.running_mean\", \"layer1.0.downsample.1.running_var\", \"layer1.0.downsample.1.num_batches_tracked\", \"layer1.1.conv1.weight\", \"layer1.1.bn1.weight\", \"layer1.1.bn1.bias\", \"layer1.1.bn1.running_mean\", \"layer1.1.bn1.running_var\", \"layer1.1.bn1.num_batches_tracked\", \"layer1.1.conv2.weight\", \"layer1.1.bn2.weight\", \"layer1.1.bn2.bias\", \"layer1.1.bn2.running_mean\", \"layer1.1.bn2.running_var\", \"layer1.1.bn2.num_batches_tracked\", \"layer1.1.conv3.weight\", \"layer1.1.bn3.weight\", \"layer1.1.bn3.bias\", \"layer1.1.bn3.running_mean\", \"layer1.1.bn3.running_var\", \"layer1.1.bn3.num_batches_tracked\", \"layer1.2.conv1.weight\", \"layer1.2.bn1.weight\", \"layer1.2.bn1.bias\", \"layer1.2.bn1.running_mean\", \"layer1.2.bn1.running_var\", \"layer1.2.bn1.num_batches_tracked\", \"layer1..."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 5: Greedy Decode / Beam Search to generate caption\n",
        "def greedy_decode(model, feat, vocab, max_len=45):\n",
        "    idx2word = {v: k for k, v in vocab.items()}\n",
        "    generated = [vocab['<SOS>']]\n",
        "    for _ in range(max_len):\n",
        "        trg = torch.tensor(generated).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            out = model(feat, trg)\n",
        "        next_token = out[0, -1].argmax().item()\n",
        "        if next_token == vocab['<EOS>']:\n",
        "            break\n",
        "        generated.append(next_token)\n",
        "\n",
        "    return ' '.join([idx2word[idx] for idx in generated[1:]])\n",
        "\n",
        "# Generate Caption\n",
        "caption = greedy_decode(model, video_feat, vocab)\n",
        "print(\"🎬 Predicted Caption:\", caption)\n"
      ],
      "metadata": {
        "id": "XBLpwR2cBmFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-hXG9c1FI3re"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L2wGuSbHI3xJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d_GW2tA6I3zo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ESTPQozrI32o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b6ZAuSbNI35a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Main tumhare training phase me auto-save logic de du\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "def train_transformer(model, train_dataset, val_dataset, vocab, device,\n",
        "                      num_epochs=20, batch_size=8, learning_rate=1e-4,\n",
        "                      checkpoint_dir='/content/drive/MyDrive/transformer_checkpoints',\n",
        "                      save_every=5):  # ← Save after every N epochs\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=vocab['<PAD>'])\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        print(f\"\\n🚀 Epoch {epoch}/{num_epochs}\")\n",
        "\n",
        "        for video_feats, captions in tqdm(train_loader, desc=\"Training\"):\n",
        "            video_feats, captions = video_feats.to(device), captions.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            tgt_input = captions[:, :-1]\n",
        "            tgt_output = captions[:, 1:]\n",
        "\n",
        "            logits = model(video_feats, tgt_input)\n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), tgt_output.reshape(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f\"✅ Training Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # === Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for video_feats, captions in val_loader:\n",
        "                video_feats, captions = video_feats.to(device), captions.to(device)\n",
        "                tgt_input = captions[:, :-1]\n",
        "                tgt_output = captions[:, 1:]\n",
        "\n",
        "                logits = model(video_feats, tgt_input)\n",
        "                loss = criterion(logits.view(-1, logits.size(-1)), tgt_output.reshape(-1))\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        print(f\"🧪 Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        # === Auto-save checkpoint\n",
        "        if epoch % save_every == 0 or epoch == num_epochs:\n",
        "            ckpt_path = os.path.join(checkpoint_dir, f\"transformer_decoder_epoch_{epoch}.pt\")\n",
        "            torch.save(model.state_dict(), ckpt_path)\n",
        "            print(f\"💾 Saved checkpoint: {ckpt_path}\")\n"
      ],
      "metadata": {
        "id": "Y810ebSiI38w"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train\n",
        "train_transformer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    val_dataset=val_dataset,\n",
        "    vocab=vocab,\n",
        "    device=device,\n",
        "    num_epochs=30,\n",
        "    batch_size=8,\n",
        "    learning_rate=1e-4,\n",
        "    checkpoint_dir=\"/content/drive/MyDrive/transformer_checkpoints\",\n",
        "    save_every=5  # Save at epochs 5, 10, 15, ...\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4G3b0HpSI8U7",
        "outputId": "90dbbed2-87ff-495f-b7f7-98f36bd1fdd3"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🚀 Epoch 1/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 157/157 [00:08<00:00, 17.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Training Loss: 5.4108\n",
            "🧪 Validation Loss: 4.5337\n",
            "\n",
            "🚀 Epoch 2/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 157/157 [00:07<00:00, 19.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Training Loss: 4.3264\n",
            "🧪 Validation Loss: 4.3262\n",
            "\n",
            "🚀 Epoch 3/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 157/157 [00:08<00:00, 19.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Training Loss: 4.0294\n",
            "🧪 Validation Loss: 3.7730\n",
            "\n",
            "🚀 Epoch 4/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 157/157 [00:08<00:00, 19.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Training Loss: 3.8594\n",
            "🧪 Validation Loss: 3.8122\n",
            "\n",
            "🚀 Epoch 5/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 157/157 [00:07<00:00, 20.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Training Loss: 3.7236\n",
            "🧪 Validation Loss: 3.7753\n",
            "💾 Saved checkpoint: /content/drive/MyDrive/transformer_checkpoints/transformer_decoder_epoch_5.pt\n",
            "\n",
            "🚀 Epoch 6/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 157/157 [00:08<00:00, 18.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Training Loss: 3.6082\n",
            "🧪 Validation Loss: 3.3932\n",
            "\n",
            "🚀 Epoch 7/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 157/157 [00:08<00:00, 18.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Training Loss: 3.5553\n",
            "🧪 Validation Loss: 3.8743\n",
            "\n",
            "🚀 Epoch 8/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 157/157 [00:07<00:00, 20.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Training Loss: 3.5894\n",
            "🧪 Validation Loss: 3.3342\n",
            "\n",
            "🚀 Epoch 9/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 157/157 [00:08<00:00, 19.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Training Loss: 3.4466\n",
            "🧪 Validation Loss: 3.4264\n",
            "\n",
            "🚀 Epoch 10/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 157/157 [00:08<00:00, 19.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Training Loss: 3.3852\n",
            "🧪 Validation Loss: 3.5293\n",
            "💾 Saved checkpoint: /content/drive/MyDrive/transformer_checkpoints/transformer_decoder_epoch_10.pt\n",
            "\n",
            "🚀 Epoch 11/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 157/157 [00:08<00:00, 19.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Training Loss: 3.2952\n",
            "🧪 Validation Loss: 3.6353\n",
            "\n",
            "🚀 Epoch 12/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 157/157 [00:08<00:00, 18.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Training Loss: 3.2118\n",
            "🧪 Validation Loss: 2.9639\n",
            "\n",
            "🚀 Epoch 13/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 157/157 [00:08<00:00, 19.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Training Loss: 3.1912\n",
            "🧪 Validation Loss: 3.4093\n",
            "\n",
            "🚀 Epoch 14/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 157/157 [00:07<00:00, 20.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Training Loss: 3.1633\n",
            "🧪 Validation Loss: 3.2087\n",
            "\n",
            "🚀 Epoch 15/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 157/157 [00:08<00:00, 19.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Training Loss: 3.1003\n",
            "🧪 Validation Loss: 3.1731\n",
            "💾 Saved checkpoint: /content/drive/MyDrive/transformer_checkpoints/transformer_decoder_epoch_15.pt\n",
            "\n",
            "🚀 Epoch 16/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 157/157 [00:08<00:00, 17.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Training Loss: 3.1000\n",
            "🧪 Validation Loss: 3.1552\n",
            "\n",
            "🚀 Epoch 17/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 157/157 [00:07<00:00, 20.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Training Loss: 3.0357\n",
            "🧪 Validation Loss: 3.2603\n",
            "\n",
            "🚀 Epoch 18/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 157/157 [00:08<00:00, 19.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Training Loss: 2.9433\n",
            "🧪 Validation Loss: 3.0677\n",
            "\n",
            "🚀 Epoch 19/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 157/157 [00:08<00:00, 19.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Training Loss: 2.8947\n",
            "🧪 Validation Loss: 3.2817\n",
            "\n",
            "🚀 Epoch 20/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 157/157 [00:07<00:00, 20.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Training Loss: 2.9638\n",
            "🧪 Validation Loss: 2.9872\n",
            "💾 Saved checkpoint: /content/drive/MyDrive/transformer_checkpoints/transformer_decoder_epoch_20.pt\n",
            "\n",
            "🚀 Epoch 21/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 157/157 [00:08<00:00, 18.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Training Loss: 2.8218\n",
            "🧪 Validation Loss: 3.2754\n",
            "\n",
            "🚀 Epoch 22/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 157/157 [00:08<00:00, 18.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Training Loss: 2.8400\n",
            "🧪 Validation Loss: 3.2090\n",
            "\n",
            "🚀 Epoch 23/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 157/157 [00:07<00:00, 20.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Training Loss: 2.8855\n",
            "🧪 Validation Loss: 3.1425\n",
            "\n",
            "🚀 Epoch 24/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 157/157 [00:08<00:00, 19.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Training Loss: 2.8026\n",
            "🧪 Validation Loss: 3.3355\n",
            "\n",
            "🚀 Epoch 25/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 157/157 [00:07<00:00, 19.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Training Loss: 2.7615\n",
            "🧪 Validation Loss: 3.0278\n",
            "💾 Saved checkpoint: /content/drive/MyDrive/transformer_checkpoints/transformer_decoder_epoch_25.pt\n",
            "\n",
            "🚀 Epoch 26/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 157/157 [00:08<00:00, 19.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Training Loss: 2.7005\n",
            "🧪 Validation Loss: 3.2970\n",
            "\n",
            "🚀 Epoch 27/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 157/157 [00:08<00:00, 18.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Training Loss: 2.7575\n",
            "🧪 Validation Loss: 2.9026\n",
            "\n",
            "🚀 Epoch 28/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 157/157 [00:07<00:00, 19.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Training Loss: 2.7813\n",
            "🧪 Validation Loss: 3.1231\n",
            "\n",
            "🚀 Epoch 29/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 157/157 [00:07<00:00, 19.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Training Loss: 2.6372\n",
            "🧪 Validation Loss: 2.7363\n",
            "\n",
            "🚀 Epoch 30/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 157/157 [00:08<00:00, 19.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Training Loss: 2.6946\n",
            "🧪 Validation Loss: 2.6889\n",
            "💾 Saved checkpoint: /content/drive/MyDrive/transformer_checkpoints/transformer_decoder_epoch_30.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e60Ra06LKCsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = '/content/04.mp4'                # Local video\n",
        "frame_dir = '/content/frames_extracted'       # Frames folder\n",
        "feature_path = '/content/04_feat.npy'         # Local feature file\n",
        "vocab_path = '/content/drive/MyDrive/msvd_split/vocab.json'\n",
        "model_path = '/content/drive/MyDrive/transformer_checkpoints/transformer_decoder_epoch_30.pt'\n"
      ],
      "metadata": {
        "id": "aYd7yhP_KcRm"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import subprocess\n",
        "\n",
        "if os.path.exists(frame_dir):\n",
        "    shutil.rmtree(frame_dir)\n",
        "os.makedirs(frame_dir, exist_ok=True)\n",
        "\n",
        "# Extract at 2 fps\n",
        "!ffmpeg -i \"$video_path\" -vf \"fps=2\" \"$frame_dir/frame_%03d.jpg\" -hide_banner -loglevel error\n"
      ],
      "metadata": {
        "id": "iAatn-5HL0kC"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import torch\n",
        "import glob\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "frame_paths = sorted(glob.glob(os.path.join(frame_dir, '*.jpg')))\n",
        "frames_tensor = torch.stack([transform(Image.open(fp)) for fp in frame_paths]).to(device)\n",
        "print(\"🎞️ Loaded frames:\", frames_tensor.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_8knsACL3sF",
        "outputId": "38cb4c90-0ca1-4a02-e75b-0401bfaf5e31"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎞️ Loaded frames: torch.Size([60, 3, 224, 224])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "resnet = models.resnet152(pretrained=True)\n",
        "resnet.fc = nn.Identity()\n",
        "resnet = resnet.to(device)\n",
        "resnet.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    feats = resnet(frames_tensor)  # [T, 2048]\n",
        "\n",
        "# Save locally\n",
        "np.save(feature_path, feats.cpu().numpy())\n",
        "print(f\"✅ Saved features to {feature_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AK1pyE9L6pq",
        "outputId": "e83f5fdb-dac8-4c10-bb49-58a421ddeb60"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet152-394f9c45.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-394f9c45.pth\n",
            "100%|██████████| 230M/230M [00:01<00:00, 172MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved features to /content/04_feat.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feat_arr = np.load(feature_path)\n",
        "video_feat = torch.tensor(feat_arr).unsqueeze(0).to(torch.float32).to(device)\n",
        "print(\"📦 Video Feature Shape:\", video_feat.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtnS13B9L-VO",
        "outputId": "3245277e-8362-48d2-af54-6c85c2841b9c"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📦 Video Feature Shape: torch.Size([1, 60, 2048])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(vocab_path, 'r') as f:\n",
        "    vocab = json.load(f)\n",
        "rev_vocab = {v: k for k, v in vocab.items()}\n"
      ],
      "metadata": {
        "id": "LX23vxSpMBb0"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.pe = pe.unsqueeze(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1)].to(x.device)\n",
        "        return self.dropout(x)\n",
        "\n",
        "class VideoTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=512, nhead=8, num_layers=3, dim_ff=2048, max_len=45, dropout=0.1, input_feat_dim=2048):\n",
        "        super().__init__()\n",
        "        self.input_fc = nn.Linear(input_feat_dim, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model, nhead, dim_ff, dropout)\n",
        "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers)\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "        self.max_len = max_len\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, vid_feats, tgt_input):\n",
        "        vid_feats = self.input_fc(vid_feats)\n",
        "        memory = self.pos_encoder(vid_feats)\n",
        "        tgt_embed = self.embedding(tgt_input) * (self.d_model ** 0.5)\n",
        "        tgt_embed = self.pos_encoder(tgt_embed)\n",
        "        output = self.decoder(tgt_embed.transpose(0, 1), memory.transpose(0, 1))\n",
        "        return self.fc_out(output.transpose(0, 1))\n"
      ],
      "metadata": {
        "id": "ZJbxoXiKMEl4"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = VideoTransformer(\n",
        "    vocab_size=len(vocab),\n",
        "    d_model=512,\n",
        "    nhead=8,\n",
        "    num_layers=3,\n",
        "    max_len=45,\n",
        "    input_feat_dim=2048\n",
        ").to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "2IdlxKPmMH0F",
        "outputId": "a1f29324-fa6f-4e7e-bac4-b7822c40beda"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for VideoTransformer:\n\tMissing key(s) in state_dict: \"input_fc.weight\", \"input_fc.bias\", \"fc_out.weight\", \"fc_out.bias\". \n\tUnexpected key(s) in state_dict: \"pos_enc.pe\", \"vid_fc.weight\", \"vid_fc.bias\", \"output.weight\", \"output.bias\". ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-f1106e13f44d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m ).to(device)\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2581\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   2582\u001b[0m                 \"Error(s) in loading state_dict for {}:\\n\\t{}\".format(\n\u001b[1;32m   2583\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\\t\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for VideoTransformer:\n\tMissing key(s) in state_dict: \"input_fc.weight\", \"input_fc.bias\", \"fc_out.weight\", \"fc_out.bias\". \n\tUnexpected key(s) in state_dict: \"pos_enc.pe\", \"vid_fc.weight\", \"vid_fc.bias\", \"output.weight\", \"output.bias\". "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), '/content/drive/MyDrive/transformer_checkpoints/video_transformer_final.pt')\n"
      ],
      "metadata": {
        "id": "1mIwKIbAMYg2"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state_dict = torch.load(model_path)\n",
        "model.load_state_dict(state_dict, strict=False)  # ⚠️ Not recommended for deployment\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xO2nPK1AMaR0",
        "outputId": "082638a7-d322-4b3a-ddae-f25ddd8d3abb"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_IncompatibleKeys(missing_keys=['input_fc.weight', 'input_fc.bias', 'fc_out.weight', 'fc_out.bias'], unexpected_keys=['pos_enc.pe', 'vid_fc.weight', 'vid_fc.bias', 'output.weight', 'output.bias'])"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save after training\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/transformer_checkpoints/video_transformer_final.pt')\n"
      ],
      "metadata": {
        "id": "oKemRnIFMeEf"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bWZIKePRMmhM"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VnWGOijmP7Z-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p6V-oc_KP7c5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S6tOdo6RP7f5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NEXT MODEL\n",
        "\n"
      ],
      "metadata": {
        "id": "zs8TsENDP7jl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}