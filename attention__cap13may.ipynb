{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUBPOivpABXs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed295004-15c6-4cf7-e8d7-0cfdceefd078"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baFdE_w2cvX5",
        "outputId": "fb2a2566-31ce-4928-dd6f-747a955cb881"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the required libraries\n",
        "!pip install numpy torch torchvision nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uj5eY5BrAq1w",
        "outputId": "f356e285-e067-4849-9128-091ac12671f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UnXWRQgtAvOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpRH3Lfbh0yJ",
        "outputId": "4ee56eef-db73-4b59-dbb7-3b6fe0da77fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Video file #.avi not found in /content/drive/MyDrive/YouTubeClips.\n",
            "Data split and saved successfully.\n"
          ]
        }
      ],
      "source": [
        "#video and json split\n",
        "\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "def split_json_data(input_json_path, videos_dir, output_dir, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
        "    # Load JSON data\n",
        "    with open(input_json_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Shuffle the video data to ensure randomness\n",
        "    video_entries = data['videos']\n",
        "    random.shuffle(video_entries)\n",
        "\n",
        "    # Calculate the split sizes\n",
        "    total_videos = len(video_entries)\n",
        "    train_size = int(total_videos * train_ratio)\n",
        "    val_size = int(total_videos * val_ratio)\n",
        "    test_size = total_videos - train_size - val_size\n",
        "\n",
        "    # Split the data\n",
        "    train_videos = video_entries[:train_size]\n",
        "    val_videos = video_entries[train_size:train_size + val_size]\n",
        "    test_videos = video_entries[train_size + val_size:]\n",
        "\n",
        "    # Organize sentences based on video_ids for each split\n",
        "    video_sentences = {video['video_id']: [] for video in video_entries}\n",
        "    for sentence in data['sentences']:\n",
        "        video_sentences[sentence['video_id']].append(sentence)\n",
        "\n",
        "    def create_split_data(split_videos, split_name):\n",
        "        split_data = {\n",
        "            \"videos\": split_videos,\n",
        "            \"sentences\": []\n",
        "        }\n",
        "        split_videos_dir = os.path.join(output_dir, split_name, \"videos\")\n",
        "        os.makedirs(split_videos_dir, exist_ok=True)\n",
        "\n",
        "        # Add captions and copy video files\n",
        "        for video in split_videos:\n",
        "            video_id = video['video_id']\n",
        "            split_data['sentences'].extend(video_sentences[video_id])\n",
        "\n",
        "            # Copy video file to the split directory\n",
        "            video_filename = f\"{video_id}.avi\"\n",
        "            src_video_path = os.path.join(videos_dir, video_filename)\n",
        "            dst_video_path = os.path.join(split_videos_dir, video_filename)\n",
        "            if os.path.exists(src_video_path):\n",
        "                shutil.copy(src_video_path, dst_video_path)\n",
        "            else:\n",
        "                print(f\"Warning: Video file {video_filename} not found in {videos_dir}.\")\n",
        "\n",
        "        # Save the JSON file for the split\n",
        "        split_json_path = os.path.join(output_dir, split_name, f\"{split_name}_captions.json\")\n",
        "        with open(split_json_path, 'w') as f:\n",
        "            json.dump(split_data, f, indent=4)\n",
        "\n",
        "    # Create each split\n",
        "    create_split_data(train_videos, \"train\")\n",
        "    create_split_data(val_videos, \"val\")\n",
        "    create_split_data(test_videos, \"test\")\n",
        "\n",
        "    print(\"Data split and saved successfully.\")\n",
        "\n",
        "# Define paths based on your directory structure\n",
        "input_json_path = '/content/drive/MyDrive/msvd_captions.json'  # Path to the original JSON file\n",
        "videos_dir = '/content/drive/MyDrive/YouTubeClips'  # Directory where video files are stored\n",
        "output_dir = '/content/drive/MyDrive/msvd_split'  # Directory where you want to save the splits\n",
        "\n",
        "# Run the split function\n",
        "split_json_data(input_json_path, videos_dir, output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4qWoJn_CAynM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kY022BoYyTbH"
      },
      "outputs": [],
      "source": [
        "#verification for split\n",
        "\n",
        "import os\n",
        "import json\n",
        "\n",
        "def verify_split_integrity(split_name, base_dir):\n",
        "    \"\"\"\n",
        "    Verifies that each video in the split has a corresponding entry in the JSON file\n",
        "    and each entry in the JSON file has a corresponding video file.\n",
        "\n",
        "    Args:\n",
        "        split_name (str): Name of the split (e.g., 'train', 'val', 'test').\n",
        "        base_dir (str): Base directory where splits are stored (e.g., '/content/drive/MyDrive/msvd_split/').\n",
        "    \"\"\"\n",
        "    video_dir = os.path.join(base_dir, split_name, 'videos')\n",
        "    json_path = os.path.join(base_dir, split_name, f\"{split_name}_captions.json\")\n",
        "\n",
        "    # Load JSON data\n",
        "    with open(json_path, 'r') as f:\n",
        "        json_data = json.load(f)\n",
        "\n",
        "    # Get the list of video files and remove file extensions\n",
        "    video_files = {os.path.splitext(file)[0] for file in os.listdir(video_dir) if file.endswith('.avi')}\n",
        "\n",
        "    # Get the list of video_ids from JSON \"videos\" section\n",
        "    json_video_ids = {video['video_id'] for video in json_data['videos']}\n",
        "\n",
        "    # Check that each video file has a corresponding entry in JSON \"videos\" section\n",
        "    missing_in_json_videos = video_files - json_video_ids\n",
        "    if missing_in_json_videos:\n",
        "        print(f\"[{split_name}] Videos present in folder but missing in JSON 'videos' section: {missing_in_json_videos}\")\n",
        "    else:\n",
        "        print(f\"[{split_name}] All videos in folder have matching entries in JSON 'videos' section.\")\n",
        "\n",
        "    # Check that each JSON entry in \"videos\" section has a corresponding video file in the folder\n",
        "    missing_in_videos_folder = json_video_ids - video_files\n",
        "    if missing_in_videos_folder:\n",
        "        print(f\"[{split_name}] Entries in JSON 'videos' section but missing video files: {missing_in_videos_folder}\")\n",
        "    else:\n",
        "        print(f\"[{split_name}] All entries in JSON 'videos' section have matching video files in folder.\")\n",
        "\n",
        "    # Get the list of video_ids from JSON \"sentences\" section\n",
        "    json_caption_video_ids = {sentence['video_id'] for sentence in json_data['sentences']}\n",
        "\n",
        "    # Check that each video file has a corresponding caption in JSON \"sentences\" section\n",
        "    missing_captions_for_videos = video_files - json_caption_video_ids\n",
        "    if missing_captions_for_videos:\n",
        "        print(f\"[{split_name}] Videos present in folder but missing captions in JSON 'sentences' section: {missing_captions_for_videos}\")\n",
        "    else:\n",
        "        print(f\"[{split_name}] All videos in folder have matching captions in JSON 'sentences' section.\")\n",
        "\n",
        "    # Check that each JSON entry in \"sentences\" section has a corresponding video file in the folder\n",
        "    missing_videos_for_captions = json_caption_video_ids - video_files\n",
        "    if missing_videos_for_captions:\n",
        "        print(f\"[{split_name}] Captions in JSON 'sentences' section but missing video files: {missing_videos_for_captions}\")\n",
        "    else:\n",
        "        print(f\"[{split_name}] All captions in JSON 'sentences' section have matching video files in folder.\")\n",
        "\n",
        "# Run verification for each split\n",
        "base_dir = '/content/drive/MyDrive/msvd_split'  # Change to your base directory if different\n",
        "for split in ['train', 'val', 'test']:\n",
        "    verify_split_integrity(split, base_dir)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z25tS76EA1Fr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivF7r3xRzIJv",
        "outputId": "e161753e-77b8-465b-c90d-4c010448f881"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning train JSON file...\n",
            "Cleaned JSON file saved to /content/drive/MyDrive/msvd_split/train/train_captions.json\n",
            "train JSON file cleaned.\n",
            "\n",
            "Cleaning val JSON file...\n",
            "Cleaned JSON file saved to /content/drive/MyDrive/msvd_split/val/val_captions.json\n",
            "val JSON file cleaned.\n",
            "\n",
            "Cleaning test JSON file...\n",
            "Cleaned JSON file saved to /content/drive/MyDrive/msvd_split/test/test_captions.json\n",
            "test JSON file cleaned.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#removing caption of which video is not present\n",
        "\n",
        "import os\n",
        "import json\n",
        "\n",
        "def clean_json_for_existing_videos(json_path, video_dir):\n",
        "    # Load JSON data\n",
        "    with open(json_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Get the set of video_ids from the actual video files in the folder\n",
        "    video_files = {os.path.splitext(file)[0] for file in os.listdir(video_dir) if file.endswith('.avi')}\n",
        "\n",
        "    # Filter out entries in 'videos' and 'sentences' that don't have a corresponding video file\n",
        "    data['videos'] = [video for video in data['videos'] if video['video_id'] in video_files]\n",
        "    data['sentences'] = [sentence for sentence in data['sentences'] if sentence['video_id'] in video_files]\n",
        "\n",
        "    # Save the cleaned JSON data back to the file\n",
        "    with open(json_path, 'w') as f:\n",
        "        json.dump(data, f, indent=4)\n",
        "\n",
        "    print(f\"Cleaned JSON file saved to {json_path}\")\n",
        "\n",
        "# Define paths to each split's JSON file and video folder\n",
        "base_dir = '/content/drive/MyDrive/msvd_split'\n",
        "splits = {\n",
        "    'train': {'json_path': os.path.join(base_dir, 'train', 'train_captions.json'),\n",
        "              'video_dir': os.path.join(base_dir, 'train', 'videos')},\n",
        "    'val': {'json_path': os.path.join(base_dir, 'val', 'val_captions.json'),\n",
        "            'video_dir': os.path.join(base_dir, 'val', 'videos')},\n",
        "    'test': {'json_path': os.path.join(base_dir, 'test', 'test_captions.json'),\n",
        "             'video_dir': os.path.join(base_dir, 'test', 'videos')}\n",
        "}\n",
        "\n",
        "# Clean each JSON file based on existing video files\n",
        "for split, paths in splits.items():\n",
        "    print(f\"Cleaning {split} JSON file...\")\n",
        "    clean_json_for_existing_videos(paths['json_path'], paths['video_dir'])\n",
        "    print(f\"{split} JSON file cleaned.\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m5fYn6G2A3kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pejwd2C5zO2S",
        "outputId": "2fcb43e3-c003-406f-97ea-3b210ba5b85f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary built with 10658 words. Saved to /content/drive/MyDrive/msvd_split/vocab.json\n"
          ]
        }
      ],
      "source": [
        "#vocabulary building\n",
        "\n",
        "import json\n",
        "from collections import Counter\n",
        "\n",
        "def build_vocabulary(json_paths, min_freq=1, save_path='/content/drive/MyDrive/msvd_split/vocab.json'):\n",
        "    special_tokens = ['<PAD>', '<SOS>', '<EOS>', '<UNK>']\n",
        "    word_counter = Counter()\n",
        "\n",
        "    # Process each JSON file to gather word frequencies\n",
        "    for json_path in json_paths:\n",
        "        with open(json_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "            for sentence in data['sentences']:\n",
        "                words = sentence['caption'].lower().split()\n",
        "                word_counter.update(words)\n",
        "\n",
        "    # Create vocabulary by adding special tokens and frequent words\n",
        "    vocab = {token: idx for idx, token in enumerate(special_tokens)}\n",
        "    for word, freq in word_counter.items():\n",
        "        if freq >= min_freq:\n",
        "            vocab[word] = len(vocab)\n",
        "\n",
        "    # Save vocabulary as a JSON file\n",
        "    with open(save_path, 'w') as f:\n",
        "        json.dump(vocab, f, indent=4)\n",
        "\n",
        "    print(f\"Vocabulary built with {len(vocab)} words. Saved to {save_path}\")\n",
        "\n",
        "# Define paths to JSON files\n",
        "json_paths = [\n",
        "    '/content/drive/MyDrive/msvd_split/train/train_captions.json',\n",
        "    '/content/drive/MyDrive/msvd_split/val/val_captions.json',\n",
        "    '/content/drive/MyDrive/msvd_split/test/test_captions.json'\n",
        "]\n",
        "\n",
        "# Build vocabulary\n",
        "build_vocabulary(json_paths)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZYzoNdC3A6L4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJ0Jql7PdlJH",
        "outputId": "eeedf12f-d534-4d72-e4d7-04ad1e3d97ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.6.0+cu124)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (2.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchvision\n",
        "!pip install opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pretrainedmodels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLs5W1_mBAJ-",
        "outputId": "4fcabb87-10b4-41a8-8565-11290e35c5e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pretrainedmodels in /usr/local/lib/python3.11/dist-packages (0.7.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from pretrainedmodels) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from pretrainedmodels) (0.21.0+cu124)\n",
            "Requirement already satisfied: munch in /usr/local/lib/python3.11/dist-packages (from pretrainedmodels) (4.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from pretrainedmodels) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->pretrainedmodels) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->pretrainedmodels) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->pretrainedmodels) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->pretrainedmodels) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cjvFcYizbj1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aaOhiAxDbj40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2crQ3kyRzY0z",
        "outputId": "d8fd925e-5ee9-45b9-86cf-b66ea8e7e3ee"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet152-b121ed2d.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-b121ed2d.pth\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 230M/230M [00:01<00:00, 204MB/s]\n",
            "Processing train videos:   0%|          | 0/1535 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "#feature extraction\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import subprocess\n",
        "import glob\n",
        "import numpy as np\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import pretrainedmodels\n",
        "from pretrainedmodels import utils\n",
        "\n",
        "# Constants for input dimensions\n",
        "C, H, W = 3, 224, 224\n",
        "\n",
        "# Function to extract frames from a video\n",
        "def extract_frames(video_path, dst):\n",
        "    if os.path.exists(dst):\n",
        "        shutil.rmtree(dst)\n",
        "    os.makedirs(dst)\n",
        "    video_to_frames_command = [\n",
        "        \"ffmpeg\",\n",
        "        '-y',\n",
        "        '-i', video_path,\n",
        "        '-vf', \"scale=400:300\",\n",
        "        '-qscale:v', \"2\",\n",
        "        f\"{dst}/%06d.jpg\"\n",
        "    ]\n",
        "    subprocess.call(video_to_frames_command, stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)\n",
        "\n",
        "# Function to extract features from frames\n",
        "def extract_feats(params, model, load_image_fn, split):\n",
        "    model.eval()\n",
        "    dir_fc = os.path.join(params['output_dir'], split, 'features')  # Store features in respective split folder\n",
        "    os.makedirs(dir_fc, exist_ok=True)\n",
        "\n",
        "    # Load video list from JSON file for the split\n",
        "    json_path = params[f'{split}_json']\n",
        "    with open(json_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "        video_ids = {video['video_id'] for video in data['videos']}\n",
        "\n",
        "    # Process each video in the specified directory\n",
        "    video_dir = os.path.join(params['video_path'], split, 'videos')  # Use respective split folder\n",
        "    video_list = glob.glob(os.path.join(video_dir, '*.avi'))\n",
        "    for video in tqdm(video_list, desc=f\"Processing {split} videos\"):\n",
        "        video_id = os.path.splitext(os.path.basename(video))[0]\n",
        "        if video_id not in video_ids:\n",
        "            continue\n",
        "\n",
        "        # Extract frames\n",
        "        dst = os.path.join(params['tmp_dir'], video_id)\n",
        "        extract_frames(video, dst)\n",
        "\n",
        "        # Load frames and extract features\n",
        "        image_list = sorted(glob.glob(os.path.join(dst, '*.jpg')))\n",
        "        samples = np.round(np.linspace(0, len(image_list) - 1, params['n_frame_steps'])).astype(int)\n",
        "        image_list = [image_list[sample] for sample in samples]\n",
        "        images = torch.zeros((len(image_list), C, H, W))\n",
        "\n",
        "        for i, img_path in enumerate(image_list):\n",
        "            img = load_image_fn(img_path)\n",
        "            images[i] = img\n",
        "\n",
        "        # Move images to GPU for feature extraction\n",
        "        images = images.cuda()\n",
        "        with torch.no_grad():\n",
        "            fc_feats = model(images).cpu().squeeze()\n",
        "\n",
        "        # Save features\n",
        "        outfile = os.path.join(dir_fc, f\"{video_id}.npy\")\n",
        "        np.save(outfile, fc_feats.numpy())\n",
        "\n",
        "        # Clean up\n",
        "        shutil.rmtree(dst)\n",
        "\n",
        "    print(f\"Feature extraction for {split} set is complete.\")\n",
        "\n",
        "# Main code setup with hardcoded parameters\n",
        "params = {\n",
        "    'output_dir': '/content/drive/MyDrive/msvd_split',\n",
        "    'video_path': '/content/drive/MyDrive/msvd_split',\n",
        "    'n_frame_steps': 40,\n",
        "    'tmp_dir': '/content/tmp_frames',\n",
        "    'train_json': '/content/drive/MyDrive/msvd_split/train/train_captions.json',\n",
        "    'val_json': '/content/drive/MyDrive/msvd_split/val/val_captions.json',\n",
        "    'test_json': '/content/drive/MyDrive/msvd_split/test/test_captions.json',\n",
        "    'model': 'resnet152'  # Set your model choice here (resnet152, inception_v3, or inception_v4)\n",
        "}\n",
        "\n",
        "# Set up model and image loader\n",
        "if params['model'] == 'inception_v3':\n",
        "    C, H, W = 3, 299, 299\n",
        "    model = pretrainedmodels.inceptionv3(pretrained='imagenet')\n",
        "    load_image_fn = utils.LoadTransformImage(model)\n",
        "elif params['model'] == 'resnet152':\n",
        "    C, H, W = 3, 224, 224\n",
        "    model = pretrainedmodels.resnet152(pretrained='imagenet')\n",
        "    load_image_fn = utils.LoadTransformImage(model)\n",
        "elif params['model'] == 'inception_v4':\n",
        "    C, H, W = 3, 299, 299\n",
        "    model = pretrainedmodels.inceptionv4(num_classes=1000, pretrained='imagenet')\n",
        "    load_image_fn = utils.LoadTransformImage(model)\n",
        "else:\n",
        "    raise ValueError(f\"Model {params['model']} is not supported\")\n",
        "\n",
        "model.last_linear = utils.Identity()  # Remove final classification layer\n",
        "model = model.cuda()  # Use GPU\n",
        "\n",
        "# Extract features for each split\n",
        "for split in ['train', 'val', 'test']:\n",
        "    extract_feats(params, model, load_image_fn, split)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "myGqtMqZBBKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NL3b52zs02dc",
        "outputId": "a1df6587-9b40-4fdb-fe14-6667078d9d7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[train] All video files have corresponding features.\n",
            "[train] All JSON entries have corresponding features.\n",
            "[train] No extra feature files found.\n",
            "[val] All video files have corresponding features.\n",
            "[val] All JSON entries have corresponding features.\n",
            "[val] No extra feature files found.\n",
            "[test] All video files have corresponding features.\n",
            "[test] All JSON entries have corresponding features.\n",
            "[test] No extra feature files found.\n"
          ]
        }
      ],
      "source": [
        "#verifying feature extracted correctly or not\n",
        "\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Function to verify features extraction for each split by cross-checking both video files and JSON entries\n",
        "def verify_features_extraction(split, params):\n",
        "    # Paths for the split\n",
        "    video_dir = os.path.join(params['output_dir'], split, 'videos')\n",
        "    features_dir = os.path.join(params['output_dir'], split, 'features')\n",
        "    json_path = os.path.join(params['output_dir'], split, f\"{split}_captions.json\")\n",
        "\n",
        "    # Load JSON file for the split\n",
        "    with open(json_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "        video_ids_in_json = {video['video_id'] for video in data['videos']}\n",
        "\n",
        "    # Check from Video Files\n",
        "    missing_features_from_videos = []\n",
        "    extra_features_in_folder_from_videos = []\n",
        "    video_files = {os.path.splitext(video)[0] for video in os.listdir(video_dir) if video.endswith('.avi')}\n",
        "    for video_id in video_files:\n",
        "        feature_path = os.path.join(features_dir, f\"{video_id}.npy\")\n",
        "        if not os.path.exists(feature_path):\n",
        "            missing_features_from_videos.append(video_id)\n",
        "\n",
        "    # Check from JSON Entries\n",
        "    missing_features_from_json = []\n",
        "    for video_id in video_ids_in_json:\n",
        "        feature_path = os.path.join(features_dir, f\"{video_id}.npy\")\n",
        "        if not os.path.exists(feature_path):\n",
        "            missing_features_from_json.append(video_id)\n",
        "\n",
        "    # Check for extra feature files\n",
        "    extra_features = []\n",
        "    for feature_file in os.listdir(features_dir):\n",
        "        video_id = os.path.splitext(feature_file)[0]\n",
        "        if video_id not in video_files and video_id not in video_ids_in_json:\n",
        "            extra_features.append(video_id)\n",
        "\n",
        "    # Print results\n",
        "    if missing_features_from_videos:\n",
        "        print(f\"[{split}] Missing features for videos in the video folder: {len(missing_features_from_videos)}\")\n",
        "        print(\"Missing video IDs from video files:\", missing_features_from_videos)\n",
        "    else:\n",
        "        print(f\"[{split}] All video files have corresponding features.\")\n",
        "\n",
        "    if missing_features_from_json:\n",
        "        print(f\"[{split}] Missing features for videos in the JSON file: {len(missing_features_from_json)}\")\n",
        "        print(\"Missing video IDs from JSON:\", missing_features_from_json)\n",
        "    else:\n",
        "        print(f\"[{split}] All JSON entries have corresponding features.\")\n",
        "\n",
        "    if extra_features:\n",
        "        print(f\"[{split}] Extra feature files found that do not match any video or JSON entry: {len(extra_features)}\")\n",
        "        print(\"Extra feature file video IDs:\", extra_features)\n",
        "    else:\n",
        "        print(f\"[{split}] No extra feature files found.\")\n",
        "\n",
        "# Define your parameters\n",
        "params = {\n",
        "    'output_dir': '/content/drive/MyDrive/msvd_split',\n",
        "}\n",
        "\n",
        "# Verify for each split\n",
        "for split in ['train', 'val', 'test']:\n",
        "    verify_features_extraction(split, params)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uvPCocdnBDtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pNwb0p0GBF-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kA-HbIAwBGBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xXUjG2pvBGEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GOiQAiB3BGG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ScOtcvrNnsrc"
      },
      "outputs": [],
      "source": [
        "#video and json split\n",
        "\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "def split_json_data(input_json_path, videos_dir, output_dir, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
        "    # Load JSON data\n",
        "    with open(input_json_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Shuffle the video data to ensure randomness\n",
        "    video_entries = data['videos']\n",
        "    random.shuffle(video_entries)\n",
        "\n",
        "    # Calculate the split sizes\n",
        "    total_videos = len(video_entries)\n",
        "    train_size = int(total_videos * train_ratio)\n",
        "    val_size = int(total_videos * val_ratio)\n",
        "    test_size = total_videos - train_size - val_size\n",
        "\n",
        "    # Split the data\n",
        "    train_videos = video_entries[:train_size]\n",
        "    val_videos = video_entries[train_size:train_size + val_size]\n",
        "    test_videos = video_entries[train_size + val_size:]\n",
        "\n",
        "    # Organize sentences based on video_ids for each split\n",
        "    video_sentences = {video['video_id']: [] for video in video_entries}\n",
        "    for sentence in data['sentences']:\n",
        "        video_sentences[sentence['video_id']].append(sentence)\n",
        "\n",
        "    def create_split_data(split_videos, split_name):\n",
        "        split_data = {\n",
        "            \"videos\": split_videos,\n",
        "            \"sentences\": []\n",
        "        }\n",
        "        split_videos_dir = os.path.join(output_dir, split_name, \"videos\")\n",
        "        os.makedirs(split_videos_dir, exist_ok=True)\n",
        "\n",
        "        # Add captions and copy video files\n",
        "        for video in split_videos:\n",
        "            video_id = video['video_id']\n",
        "            split_data['sentences'].extend(video_sentences[video_id])\n",
        "\n",
        "            # Copy video file to the split directory\n",
        "            video_filename = f\"{video_id}.avi\"\n",
        "            src_video_path = os.path.join(videos_dir, video_filename)\n",
        "            dst_video_path = os.path.join(split_videos_dir, video_filename)\n",
        "            if os.path.exists(src_video_path):\n",
        "                shutil.copy(src_video_path, dst_video_path)\n",
        "            else:\n",
        "                print(f\"Warning: Video file {video_filename} not found in {videos_dir}.\")\n",
        "\n",
        "        # Save the JSON file for the split\n",
        "        split_json_path = os.path.join(output_dir, split_name, f\"{split_name}_captions.json\")\n",
        "        with open(split_json_path, 'w') as f:\n",
        "            json.dump(split_data, f, indent=4)\n",
        "\n",
        "    # Create each split\n",
        "    create_split_data(train_videos, \"train\")\n",
        "    create_split_data(val_videos, \"val\")\n",
        "    create_split_data(test_videos, \"test\")\n",
        "\n",
        "    print(\"Data split and saved successfully.\")\n",
        "\n",
        "# Define paths based on your directory structure\n",
        "input_json_path = '/content/drive/MyDrive/msvd_captions.json'  # Path to the original JSON file\n",
        "videos_dir = '/content/drive/MyDrive/YouTubeClips'  # Directory where video files are stored\n",
        "output_dir = '/content/drive/MyDrive/msvd_split'  # Directory where you want to save the splits\n",
        "\n",
        "# Run the split function\n",
        "split_json_data(input_json_path, videos_dir, output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qY19wtX6BInH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eni0vvQkeeIU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fd7b3ba-d85e-47d3-ac83-e1f3f35255c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Vocabulary built with 10658 tokens.\n",
            "ðŸ“ Saved vocab to: /content/drive/MyDrive/msvd_split/vocab.json\n",
            "ðŸ“ Saved reverse vocab to: /content/drive/MyDrive/msvd_split/vocab_rev.json\n",
            "ðŸ“Š Top 10 frequent words:\n",
            "   a               : 70529\n",
            "   is              : 34539\n",
            "   the             : 22887\n",
            "   man             : 17914\n",
            "   woman           : 7939\n",
            "   on              : 7417\n",
            "   in              : 7370\n",
            "   playing         : 6272\n",
            "   are             : 5718\n",
            "   of              : 5240\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from collections import Counter\n",
        "\n",
        "def build_vocabulary(\n",
        "    json_paths,\n",
        "    min_freq=1,\n",
        "    save_path='/content/drive/MyDrive/msvd_split/vocab.json',\n",
        "    reverse_path='/content/drive/MyDrive/msvd_split/vocab_rev.json',\n",
        "    top_k_preview=10\n",
        "):\n",
        "    special_tokens = ['<PAD>', '<SOS>', '<EOS>', '<UNK>']\n",
        "    word_counter = Counter()\n",
        "\n",
        "    # Step 1: Count all words\n",
        "    for json_path in json_paths:\n",
        "        with open(json_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "            for sentence in data['sentences']:\n",
        "                words = sentence['caption'].lower().split()\n",
        "                word_counter.update(words)\n",
        "\n",
        "    # Step 2: Initialize vocab with special tokens\n",
        "    vocab = {token: idx for idx, token in enumerate(special_tokens)}\n",
        "\n",
        "    # Step 3: Add frequent words sorted by frequency (descending)\n",
        "    for word, freq in word_counter.most_common():\n",
        "        if freq >= min_freq:\n",
        "            if word not in vocab:  # Avoid conflict with special tokens\n",
        "                vocab[word] = len(vocab)\n",
        "\n",
        "    # Step 4: Create reverse vocab\n",
        "    vocab_rev = {idx: word for word, idx in vocab.items()}\n",
        "\n",
        "    # Step 5: Save both files\n",
        "    with open(save_path, 'w') as f:\n",
        "        json.dump(vocab, f, indent=4)\n",
        "    with open(reverse_path, 'w') as f:\n",
        "        json.dump(vocab_rev, f, indent=4)\n",
        "\n",
        "    # Step 6: Show summary\n",
        "    print(f\"âœ… Vocabulary built with {len(vocab)} tokens.\")\n",
        "    print(f\"ðŸ“ Saved vocab to: {save_path}\")\n",
        "    print(f\"ðŸ“ Saved reverse vocab to: {reverse_path}\")\n",
        "    print(f\"ðŸ“Š Top {top_k_preview} frequent words:\")\n",
        "    for word, freq in word_counter.most_common(top_k_preview):\n",
        "        print(f\"   {word:<15} : {freq}\")\n",
        "\n",
        "# === Paths to your cleaned split JSON files\n",
        "json_paths = [\n",
        "    '/content/drive/MyDrive/msvd_split/train/train_captions.json',\n",
        "    '/content/drive/MyDrive/msvd_split/val/val_captions.json',\n",
        "    '/content/drive/MyDrive/msvd_split/test/test_captions.json'\n",
        "]\n",
        "\n",
        "# === Run vocabulary builder\n",
        "build_vocabulary(json_paths, min_freq=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zJRcdH-QBLe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U79BgOCT6NLP",
        "outputId": "60283ff6-6d83-4451-dae4-6bdbf3b0493b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing train videos: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1789/1789 [35:07<00:00,  1.18s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature extraction for train set is complete.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing val videos: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 542/542 [07:32<00:00,  1.20it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature extraction for val set is complete.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing test videos: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 546/546 [07:24<00:00,  1.23it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature extraction for test set is complete.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#feature extraction\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import subprocess\n",
        "import glob\n",
        "import numpy as np\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import pretrainedmodels\n",
        "from pretrainedmodels import utils\n",
        "\n",
        "# Constants for input dimensions\n",
        "C, H, W = 3, 224, 224\n",
        "\n",
        "# Function to extract frames from a video\n",
        "def extract_frames(video_path, dst):\n",
        "    if os.path.exists(dst):\n",
        "        shutil.rmtree(dst)\n",
        "    os.makedirs(dst)\n",
        "    video_to_frames_command = [\n",
        "        \"ffmpeg\",\n",
        "        '-y',\n",
        "        '-i', video_path,\n",
        "        '-vf', \"scale=400:300\",\n",
        "        '-qscale:v', \"2\",\n",
        "        f\"{dst}/%06d.jpg\"\n",
        "    ]\n",
        "    subprocess.call(video_to_frames_command, stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)\n",
        "\n",
        "# Function to extract features from frames\n",
        "def extract_feats(params, model, load_image_fn, split):\n",
        "    model.eval()\n",
        "    dir_fc = os.path.join(params['output_dir'], split, 'features')  # Store features in respective split folder\n",
        "    os.makedirs(dir_fc, exist_ok=True)\n",
        "\n",
        "    # Load video list from JSON file for the split\n",
        "    json_path = params[f'{split}_json']\n",
        "    with open(json_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "        video_ids = {video['video_id'] for video in data['videos']}\n",
        "\n",
        "    # Process each video in the specified directory\n",
        "    video_dir = os.path.join(params['video_path'], split, 'videos')  # Use respective split folder\n",
        "    video_list = glob.glob(os.path.join(video_dir, '*.avi'))\n",
        "    for video in tqdm(video_list, desc=f\"Processing {split} videos\"):\n",
        "        video_id = os.path.splitext(os.path.basename(video))[0]\n",
        "        if video_id not in video_ids:\n",
        "            continue\n",
        "\n",
        "        # Extract frames\n",
        "        dst = os.path.join(params['tmp_dir'], video_id)\n",
        "        extract_frames(video, dst)\n",
        "\n",
        "        # Load frames and extract features\n",
        "        # image_list = sorted(glob.glob(os.path.join(dst, '*.jpg')))\n",
        "        # samples = np.round(np.linspace(0, len(image_list) - 1, params['n_frame_steps'])).astype(int)\n",
        "        # image_list = [image_list[sample] for sample in samples]\n",
        "        image_list = sorted(glob.glob(os.path.join(dst, '*.jpg')))\n",
        "        if len(image_list) == 0:\n",
        "            print(f\"[WARNING] No frames extracted for video {video_id}. Skipping...\")\n",
        "            shutil.rmtree(dst)\n",
        "            continue  # Skip to next video\n",
        "\n",
        "        # Safe sampling\n",
        "        n_frames = len(image_list)\n",
        "        sample_count = min(params['n_frame_steps'], n_frames)\n",
        "        samples = np.round(np.linspace(0, n_frames - 1, sample_count)).astype(int)\n",
        "        image_list = [image_list[sample] for sample in samples]\n",
        "\n",
        "        images = torch.zeros((len(image_list), C, H, W))\n",
        "\n",
        "        for i, img_path in enumerate(image_list):\n",
        "            img = load_image_fn(img_path)\n",
        "            images[i] = img\n",
        "\n",
        "        # Move images to GPU for feature extraction\n",
        "        images = images.cuda()\n",
        "        with torch.no_grad():\n",
        "            fc_feats = model(images).cpu().squeeze()\n",
        "\n",
        "        # Save features\n",
        "        outfile = os.path.join(dir_fc, f\"{video_id}.npy\")\n",
        "        np.save(outfile, fc_feats.numpy())\n",
        "\n",
        "        # Clean up\n",
        "        shutil.rmtree(dst)\n",
        "\n",
        "    print(f\"Feature extraction for {split} set is complete.\")\n",
        "\n",
        "# Main code setup with hardcoded parameters\n",
        "params = {\n",
        "    'output_dir': '/content/drive/MyDrive/VideoCaptioning/msvd_split',\n",
        "    'video_path': '/content/drive/MyDrive/VideoCaptioning/msvd_split',\n",
        "    'n_frame_steps': 40,\n",
        "    'tmp_dir': '/content/tmp_frames',\n",
        "    'train_json': '/content/drive/MyDrive/VideoCaptioning/msvd_split/train/train_captions.json',\n",
        "    'val_json': '/content/drive/MyDrive/VideoCaptioning/msvd_split/val/val_captions.json',\n",
        "    'test_json': '/content/drive/MyDrive/VideoCaptioning/msvd_split/test/test_captions.json',\n",
        "    'model': 'resnet152'  # Set your model choice here (resnet152, inception_v3, or inception_v4)\n",
        "}\n",
        "\n",
        "# Set up model and image loader\n",
        "if params['model'] == 'inception_v3':\n",
        "    C, H, W = 3, 299, 299\n",
        "    model = pretrainedmodels.inceptionv3(pretrained='imagenet')\n",
        "    load_image_fn = utils.LoadTransformImage(model)\n",
        "elif params['model'] == 'resnet152':\n",
        "    C, H, W = 3, 224, 224\n",
        "    model = pretrainedmodels.resnet152(pretrained='imagenet')\n",
        "    load_image_fn = utils.LoadTransformImage(model)\n",
        "elif params['model'] == 'inception_v4':\n",
        "    C, H, W = 3, 299, 299\n",
        "    model = pretrainedmodels.inceptionv4(num_classes=1000, pretrained='imagenet')\n",
        "    load_image_fn = utils.LoadTransformImage(model)\n",
        "else:\n",
        "    raise ValueError(f\"Model {params['model']} is not supported\")\n",
        "\n",
        "model.last_linear = utils.Identity()  # Remove final classification layer\n",
        "model = model.cuda()  # Use GPU\n",
        "\n",
        "# Extract features for each split\n",
        "for split in ['train', 'val', 'test']:\n",
        "    extract_feats(params, model, load_image_fn, split)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gZmRBEJzBOy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpISv9fCep4m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f0bd82f-7659-4ba4-e4d9-d5d0bf5ecf32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Cleaned JSON for [train]\n",
            "ðŸ§® Removed 0 invalid video entries\n",
            "ðŸ§¾ Removed 0 invalid captions\n",
            "âœ… [train] Verified and fixed. All valid videos now have features and JSON alignment.\n",
            "\n",
            "âœ… Cleaned JSON for [val]\n",
            "ðŸ§® Removed 0 invalid video entries\n",
            "ðŸ§¾ Removed 0 invalid captions\n",
            "âœ… [val] Verified and fixed. All valid videos now have features and JSON alignment.\n",
            "\n",
            "âœ… Cleaned JSON for [test]\n",
            "ðŸ§® Removed 0 invalid video entries\n",
            "ðŸ§¾ Removed 0 invalid captions\n",
            "âœ… [test] Verified and fixed. All valid videos now have features and JSON alignment.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#cleaning feature ,which is not match\n",
        "import os\n",
        "import json\n",
        "\n",
        "def verify_and_fix_features(split, params):\n",
        "    video_dir = os.path.join(params['output_dir'], split, 'videos')\n",
        "    features_dir = os.path.join(params['output_dir'], split, 'features')\n",
        "    json_path = os.path.join(params['output_dir'], split, f\"{split}_captions.json\")\n",
        "\n",
        "    # Load JSON\n",
        "    with open(json_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "        json_video_ids = {video['video_id'] for video in data['videos']}\n",
        "\n",
        "    # All video and feature files\n",
        "    video_files = {os.path.splitext(f)[0] for f in os.listdir(video_dir) if f.endswith('.avi')}\n",
        "    feature_files = {os.path.splitext(f)[0] for f in os.listdir(features_dir) if f.endswith('.npy')}\n",
        "\n",
        "    # Identify corrupted or extra items\n",
        "    missing_feats_from_videos = video_files - feature_files\n",
        "    missing_feats_from_json = json_video_ids - feature_files\n",
        "    extra_feats = feature_files - video_files - json_video_ids\n",
        "\n",
        "    # === ACTION 1: Delete corrupted video files (with no features)\n",
        "    for vid in missing_feats_from_videos:\n",
        "        path = os.path.join(video_dir, f\"{vid}.avi\")\n",
        "        if os.path.exists(path):\n",
        "            os.remove(path)\n",
        "            print(f\"ðŸ—‘ï¸ Deleted incomplete video (no features): {path}\")\n",
        "\n",
        "    # === ACTION 2: Remove JSON entries with missing features\n",
        "    original_video_count = len(data['videos'])\n",
        "    original_caption_count = len(data['sentences'])\n",
        "    data['videos'] = [v for v in data['videos'] if v['video_id'] in feature_files]\n",
        "    data['sentences'] = [s for s in data['sentences'] if s['video_id'] in feature_files]\n",
        "\n",
        "    with open(json_path, 'w') as f:\n",
        "        json.dump(data, f, indent=4)\n",
        "\n",
        "    print(f\"âœ… Cleaned JSON for [{split}]\")\n",
        "    print(f\"ðŸ§® Removed {original_video_count - len(data['videos'])} invalid video entries\")\n",
        "    print(f\"ðŸ§¾ Removed {original_caption_count - len(data['sentences'])} invalid captions\")\n",
        "\n",
        "    # === ACTION 3: Delete stray feature files\n",
        "    for vid in extra_feats:\n",
        "        path = os.path.join(features_dir, f\"{vid}.npy\")\n",
        "        if os.path.exists(path):\n",
        "            os.remove(path)\n",
        "            print(f\"ðŸ§¼ Deleted stray feature file: {path}\")\n",
        "\n",
        "    print(f\"âœ… [{split}] Verified and fixed. All valid videos now have features and JSON alignment.\\n\")\n",
        "\n",
        "# Run for all splits\n",
        "params = {\n",
        "    'output_dir': '/content/drive/MyDrive/msvd_split',\n",
        "}\n",
        "\n",
        "for split in ['train', 'val', 'test']:\n",
        "    verify_and_fix_features(split, params)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CzNUWmB-BR-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32MO5vnShuPP"
      },
      "outputs": [],
      "source": [
        "#data loader\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class VideoCaptionDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset class for loading video features and their associated captions.\n",
        "\n",
        "    Args:\n",
        "        feature_dir (str): Directory containing .npy feature files.\n",
        "        json_path (str): Path to JSON file with \"videos\" and \"sentences\".\n",
        "        vocab (dict): Vocabulary mapping words to indices.\n",
        "        max_caption_length (int): Max length of tokenized captions (including <SOS> and <EOS>).\n",
        "        verbose (bool): If True, prints sample-level debug info.\n",
        "    \"\"\"\n",
        "    def __init__(self, feature_dir, json_path, vocab, max_caption_length=15, verbose=False):\n",
        "        self.feature_dir = feature_dir\n",
        "        self.vocab = vocab\n",
        "        self.max_caption_length = max_caption_length\n",
        "        self.verbose = verbose\n",
        "\n",
        "        # Load JSON\n",
        "        with open(json_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        # Map video_id to all its captions\n",
        "        self.video_captions = {}\n",
        "        for item in data['sentences']:\n",
        "            vid = item['video_id']\n",
        "            if vid in self.video_captions:\n",
        "                self.video_captions[vid].append(item['caption'])\n",
        "            else:\n",
        "                self.video_captions[vid] = [item['caption']]\n",
        "\n",
        "        # Keep video_ids that have both captions and feature files\n",
        "        all_video_ids = [v['video_id'] for v in data['videos']]\n",
        "        self.video_ids = [\n",
        "            vid for vid in all_video_ids\n",
        "            if vid in self.video_captions and os.path.exists(os.path.join(self.feature_dir, f\"{vid}.npy\"))\n",
        "        ]\n",
        "\n",
        "        if self.verbose:\n",
        "            print(\"âœ… Initialized VideoCaptionDataset\")\n",
        "            print(f\"ðŸ§¾ Total valid samples: {len(self.video_ids)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_id = self.video_ids[idx]\n",
        "        feature_path = os.path.join(self.feature_dir, f\"{video_id}.npy\")\n",
        "\n",
        "        try:\n",
        "            video_features = np.load(feature_path)\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"âŒ Failed to load features for {video_id}: {e}\")\n",
        "\n",
        "        # Choose a random caption and tokenize\n",
        "        caption = np.random.choice(self.video_captions[video_id])\n",
        "        tokens = [self.vocab['<SOS>']] + [\n",
        "            self.vocab.get(word, self.vocab['<UNK>']) for word in caption.lower().split()\n",
        "        ] + [self.vocab['<EOS>']]\n",
        "\n",
        "        # Truncate and pad\n",
        "        tokens = tokens[:self.max_caption_length]\n",
        "        tokens += [self.vocab['<PAD>']] * (self.max_caption_length - len(tokens))\n",
        "\n",
        "        caption_tensor = torch.tensor(tokens, dtype=torch.long)\n",
        "        video_tensor = torch.tensor(video_features, dtype=torch.float32)\n",
        "\n",
        "        # Debug preview\n",
        "        if self.verbose and idx == 0:\n",
        "            reverse_vocab = {v: k for k, v in self.vocab.items()}\n",
        "            print(f\"\\nðŸ“¦ Sample [{video_id}]\")\n",
        "            print(\"ðŸ“ Caption:\", caption)\n",
        "            print(\"ðŸ”¢ Tokens :\", tokens)\n",
        "            print(\"ðŸ”  Decoded:\", [reverse_vocab.get(t, '<UNK>') for t in tokens])\n",
        "            print(\"ðŸŽžï¸ Video Features Shape:\", video_tensor.shape)\n",
        "            print(\"ðŸ§  Caption Tensor:\", caption_tensor)\n",
        "\n",
        "        return video_tensor, caption_tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hu6i7tHrBU9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2wsXk8Nh3s0",
        "outputId": "88917191-300b-4bcd-b714-a22d7a3531bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Initialized VideoCaptionDataset\n",
            "ðŸ§¾ Total valid samples: 1251\n",
            "Dataset size: 1251\n",
            "\n",
            "ðŸ“¦ Sample [p9g06ktIkJg_4_11]\n",
            "ðŸ“ Caption: several lemurs are huddling together\n",
            "ðŸ”¢ Tokens : [1, 259, 2084, 12, 4031, 214, 2, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "ðŸ”  Decoded: ['<SOS>', 'several', 'lemurs', 'are', 'huddling', 'together', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "ðŸŽžï¸ Video Features Shape: torch.Size([40, 2048])\n",
            "ðŸ§  Caption Tensor: tensor([   1,  259, 2084,   12, 4031,  214,    2,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0])\n",
            "Sample Video Features Shape: torch.Size([40, 2048])\n",
            "Sample Caption Tensor: tensor([   1,  259, 2084,   12, 4031,  214,    2,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0])\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Define directories based on your structure\n",
        "    feature_dir = '/content/drive/MyDrive/msvd_split/train/features'  # Path to train features directory\n",
        "    json_path = '/content/drive/MyDrive/msvd_split/train/train_captions.json'  # Path to train captions JSON\n",
        "    vocab_path = '/content/drive/MyDrive/msvd_split/vocab.json'  # Path to vocabulary JSON\n",
        "\n",
        "    # Load vocabulary\n",
        "    with open(vocab_path, 'r') as f:\n",
        "        vocab = json.load(f)\n",
        "\n",
        "    # Initialize dataset\n",
        "    dataset = VideoCaptionDataset(feature_dir, json_path, vocab, verbose=True)\n",
        "    print(f\"Dataset size: {len(dataset)}\")\n",
        "\n",
        "    # Access a sample item to verify\n",
        "    video_features, caption_tensor = dataset[0]\n",
        "    print(\"Sample Video Features Shape:\", video_features.shape)\n",
        "    print(\"Sample Caption Tensor:\", caption_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rb8RUiD4BX0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTj9UfbViMq5",
        "outputId": "cebc0f63-65f1-4d8b-d776-24ca2156a2a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Initialized VideoCaptionDataset\n",
            "ðŸ§¾ Total valid samples: 1251\n",
            "Dataset Size: 1251\n",
            "\n",
            "ðŸ“¦ Sample [p9g06ktIkJg_4_11]\n",
            "ðŸ“ Caption: several furry animals are huddled together\n",
            "ðŸ”¢ Tokens : [1, 259, 1970, 298, 12, 2866, 214, 2, 0, 0, 0, 0, 0, 0, 0]\n",
            "ðŸ”  Decoded: ['<SOS>', 'several', 'furry', 'animals', 'are', 'huddled', 'together', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "ðŸŽžï¸ Video Features Shape: torch.Size([40, 2048])\n",
            "ðŸ§  Caption Tensor: tensor([   1,  259, 1970,  298,   12, 2866,  214,    2,    0,    0,    0,    0,\n",
            "           0,    0,    0])\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    feature_dir = '/content/drive/MyDrive/msvd_split/train/features'\n",
        "    json_path = '/content/drive/MyDrive/msvd_split/train/train_captions.json'\n",
        "    vocab_path = '/content/drive/MyDrive/msvd_split/vocab.json'\n",
        "\n",
        "    with open(vocab_path, 'r') as f:\n",
        "        vocab = json.load(f)\n",
        "\n",
        "    dataset = VideoCaptionDataset(\n",
        "        feature_dir=feature_dir,\n",
        "        json_path=json_path,\n",
        "        vocab=vocab,\n",
        "        max_caption_length=15,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    print(f\"Dataset Size: {len(dataset)}\")\n",
        "\n",
        "    # Test sample access\n",
        "    video_feat, cap_tensor = dataset[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X2WeEcglBaVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbQq1y71iNHW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BahdanauAttention(nn.Module):\n",
        "    def __init__(self, hidden_size, encoder_output_size):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.W2 = nn.Linear(encoder_output_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        hidden = hidden.unsqueeze(1)  # [B, 1, H]\n",
        "        score = self.V(torch.tanh(self.W1(hidden) + self.W2(encoder_outputs)))  # [B, T, 1]\n",
        "        attention_weights = F.softmax(score, dim=1)  # [B, T, 1]\n",
        "        context = torch.sum(attention_weights * encoder_outputs, dim=1)  # [B, H]\n",
        "        return context, attention_weights\n",
        "\n",
        "class S2VTModel(nn.Module):\n",
        "    def __init__(self, vocab_size, max_len=45, dim_hidden=1024, dim_word=512, dim_vid=2048,\n",
        "                 sos_id=1, eos_id=0, n_layers=1, rnn_cell='lstm', rnn_dropout_p=0.3):\n",
        "        super(S2VTModel, self).__init__()\n",
        "\n",
        "        self.rnn_cell_type = rnn_cell.lower()\n",
        "        self.rnn_cell = nn.LSTM if self.rnn_cell_type == 'lstm' else nn.GRU\n",
        "\n",
        "        self.rnn1 = self.rnn_cell(dim_vid, dim_hidden, n_layers, batch_first=True, dropout=rnn_dropout_p)\n",
        "        self.rnn2 = self.rnn_cell(dim_hidden + dim_word, dim_hidden, n_layers, batch_first=True, dropout=rnn_dropout_p)\n",
        "\n",
        "        self.attention = BahdanauAttention(dim_hidden, dim_hidden)\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, dim_word)\n",
        "        self.out = nn.Linear(dim_hidden, vocab_size)\n",
        "\n",
        "        self.dim_vid = dim_vid\n",
        "        self.dim_hidden = dim_hidden\n",
        "        self.dim_word = dim_word\n",
        "        self.max_length = max_len\n",
        "        self.vocab_size = vocab_size\n",
        "        self.sos_id = sos_id\n",
        "        self.eos_id = eos_id\n",
        "\n",
        "    def forward(self, vid_feats, target_variable=None, mode='train', beam_width=3):\n",
        "        batch_size, n_frames, _ = vid_feats.shape\n",
        "        device = vid_feats.device\n",
        "\n",
        "        encoder_outputs, state1 = self.rnn1(vid_feats)  # [B, T, H]\n",
        "\n",
        "        if mode == 'train':\n",
        "            seq_probs = []\n",
        "            for t in range(self.max_length - 1):\n",
        "                current_word = self.embedding(target_variable[:, t])\n",
        "                context, _ = self.attention(state1[0][-1] if self.rnn_cell_type == 'lstm' else state1[-1], encoder_outputs)\n",
        "                input2 = torch.cat((context, current_word), dim=1).unsqueeze(1)\n",
        "                output2, state2 = self.rnn2(input2, state1)\n",
        "                logits = self.out(output2.squeeze(1))\n",
        "                logits = F.log_softmax(logits, dim=1)\n",
        "                seq_probs.append(logits.unsqueeze(1))\n",
        "                state1 = state2\n",
        "            return torch.cat(seq_probs, dim=1), None\n",
        "\n",
        "        else:  # Beam Search Inference\n",
        "            beams = [(torch.tensor([self.sos_id], device=device), 0.0, state1)]\n",
        "            completed = []\n",
        "\n",
        "            for _ in range(self.max_length - 1):\n",
        "                new_beams = []\n",
        "                for seq, score, state in beams:\n",
        "                    last_word = seq[-1].unsqueeze(0)\n",
        "                    if last_word.item() == self.eos_id:\n",
        "                        completed.append((seq, score))\n",
        "                        continue\n",
        "\n",
        "                    emb = self.embedding(last_word).unsqueeze(0)  # [1, 1, D]\n",
        "                    context, _ = self.attention(state[0][-1] if self.rnn_cell_type == 'lstm' else state[-1], encoder_outputs)\n",
        "                    input2 = torch.cat((context, emb.squeeze(1)), dim=-1).unsqueeze(1)\n",
        "                    output2, new_state = self.rnn2(input2, state)\n",
        "                    logits = self.out(output2.squeeze(1))\n",
        "                    log_probs = F.log_softmax(logits, dim=1)\n",
        "                    topk_log_probs, topk_indices = torch.topk(log_probs, beam_width)\n",
        "\n",
        "                    for k in range(beam_width):\n",
        "                        new_seq = torch.cat([seq, topk_indices[0, k].unsqueeze(0)])\n",
        "                        new_score = score + topk_log_probs[0, k].item()\n",
        "                        new_beams.append((new_seq, new_score, new_state))\n",
        "\n",
        "                beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_width]\n",
        "\n",
        "            best_seq = max(completed or beams, key=lambda x: x[1])[0]\n",
        "            return None, best_seq.unsqueeze(0)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z-w12IK2Bcx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmtDaRjbkgd3",
        "outputId": "9d37a40a-1cbc-4010-d15c-e95b9e910b67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Model saved as S2VTModel_Attention.py in your Drive.\n"
          ]
        }
      ],
      "source": [
        "# Define the full model code as a string\n",
        "model_code = '''import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BahdanauAttention(nn.Module):\n",
        "    def __init__(self, hidden_size, encoder_output_size):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.W2 = nn.Linear(encoder_output_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        hidden = hidden.unsqueeze(1)  # [B, 1, H]\n",
        "        score = self.V(torch.tanh(self.W1(hidden) + self.W2(encoder_outputs)))  # [B, T, 1]\n",
        "        attention_weights = F.softmax(score, dim=1)  # [B, T, 1]\n",
        "        context = torch.sum(attention_weights * encoder_outputs, dim=1)  # [B, H]\n",
        "        return context, attention_weights\n",
        "\n",
        "class S2VTModel(nn.Module):\n",
        "    def __init__(self, vocab_size, max_len=45, dim_hidden=1024, dim_word=512, dim_vid=2048,\n",
        "                 sos_id=1, eos_id=0, n_layers=1, rnn_cell='lstm', rnn_dropout_p=0.3):\n",
        "        super(S2VTModel, self).__init__()\n",
        "\n",
        "        self.rnn_cell_type = rnn_cell.lower()\n",
        "        self.rnn_cell = nn.LSTM if self.rnn_cell_type == 'lstm' else nn.GRU\n",
        "\n",
        "        self.rnn1 = self.rnn_cell(dim_vid, dim_hidden, n_layers, batch_first=True, dropout=rnn_dropout_p)\n",
        "        self.rnn2 = self.rnn_cell(dim_hidden + dim_word, dim_hidden, n_layers, batch_first=True, dropout=rnn_dropout_p)\n",
        "\n",
        "        self.attention = BahdanauAttention(dim_hidden, dim_hidden)\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, dim_word)\n",
        "        self.out = nn.Linear(dim_hidden, vocab_size)\n",
        "\n",
        "        self.dim_vid = dim_vid\n",
        "        self.dim_hidden = dim_hidden\n",
        "        self.dim_word = dim_word\n",
        "        self.max_length = max_len\n",
        "        self.vocab_size = vocab_size\n",
        "        self.sos_id = sos_id\n",
        "        self.eos_id = eos_id\n",
        "\n",
        "    def forward(self, vid_feats, target_variable=None, mode='train', beam_width=3):\n",
        "        batch_size, n_frames, _ = vid_feats.shape\n",
        "        device = vid_feats.device\n",
        "\n",
        "        encoder_outputs, state1 = self.rnn1(vid_feats)  # [B, T, H]\n",
        "\n",
        "        if mode == 'train':\n",
        "            seq_probs = []\n",
        "            for t in range(self.max_length - 1):\n",
        "                current_word = self.embedding(target_variable[:, t])\n",
        "                context, _ = self.attention(state1[0][-1] if self.rnn_cell_type == 'lstm' else state1[-1], encoder_outputs)\n",
        "                input2 = torch.cat((context, current_word), dim=1).unsqueeze(1)\n",
        "                output2, state2 = self.rnn2(input2, state1)\n",
        "                logits = self.out(output2.squeeze(1))\n",
        "                logits = F.log_softmax(logits, dim=1)\n",
        "                seq_probs.append(logits.unsqueeze(1))\n",
        "                state1 = state2\n",
        "            return torch.cat(seq_probs, dim=1), None\n",
        "\n",
        "        else:  # Beam Search Inference\n",
        "            beams = [(torch.tensor([self.sos_id], device=device), 0.0, state1)]\n",
        "            completed = []\n",
        "\n",
        "            for _ in range(self.max_length - 1):\n",
        "                new_beams = []\n",
        "                for seq, score, state in beams:\n",
        "                    last_word = seq[-1].unsqueeze(0)\n",
        "                    if last_word.item() == self.eos_id:\n",
        "                        completed.append((seq, score))\n",
        "                        continue\n",
        "\n",
        "                    emb = self.embedding(last_word).unsqueeze(0)  # [1, 1, D]\n",
        "                    context, _ = self.attention(state[0][-1] if self.rnn_cell_type == 'lstm' else state[-1], encoder_outputs)\n",
        "                    input2 = torch.cat((context, emb.squeeze(1)), dim=-1).unsqueeze(1)\n",
        "                    output2, new_state = self.rnn2(input2, state)\n",
        "                    logits = self.out(output2.squeeze(1))\n",
        "                    log_probs = F.log_softmax(logits, dim=1)\n",
        "                    topk_log_probs, topk_indices = torch.topk(log_probs, beam_width)\n",
        "\n",
        "                    for k in range(beam_width):\n",
        "                        new_seq = torch.cat([seq, topk_indices[0, k].unsqueeze(0)])\n",
        "                        new_score = score + topk_log_probs[0, k].item()\n",
        "                        new_beams.append((new_seq, new_score, new_state))\n",
        "\n",
        "                beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_width]\n",
        "\n",
        "            best_seq = max(completed or beams, key=lambda x: x[1])[0]\n",
        "            return None, best_seq.unsqueeze(0)\n",
        "\n",
        "'''\n",
        "\n",
        "# Save to Google Drive\n",
        "with open('/content/drive/MyDrive/S2VTModel_Attention.py', 'w') as f:\n",
        "    f.write(model_code)\n",
        "\n",
        "print(\"âœ… Model saved as S2VTModel_Attention.py in your Drive.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1fXLd2lvBfX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yx00QdBZk3ds"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train_model(model, train_dataset, val_dataset, vocab, device,\n",
        "                num_epochs=10, batch_size=8, learning_rate=1e-4, checkpoint_dir=None):\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    model = model.to(device)\n",
        "    criterion = nn.NLLLoss(ignore_index=vocab['<PAD>'])\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "\n",
        "        print(f\"\\nðŸš€ Epoch [{epoch}/{num_epochs}]\")\n",
        "\n",
        "        for video_feats, captions in tqdm(train_loader, desc='Training'):\n",
        "            video_feats, captions = video_feats.to(device), captions.to(device)\n",
        "\n",
        "            # Prepare input and target\n",
        "            inputs = captions[:, :-1]\n",
        "            targets = captions[:, 1:]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs, _ = model(video_feats, target_variable=inputs, mode='train')  # [B, T, V]\n",
        "\n",
        "            outputs = outputs.view(-1, outputs.size(-1))       # [B*T, V]\n",
        "            targets = targets.reshape(-1)                      # [B*T]\n",
        "\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "        print(f\"âœ… Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # === Validation ===\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for video_feats, captions in tqdm(val_loader, desc='Validation'):\n",
        "                video_feats, captions = video_feats.to(device), captions.to(device)\n",
        "                inputs = captions[:, :-1]\n",
        "                targets = captions[:, 1:]\n",
        "\n",
        "                outputs, _ = model(video_feats, target_variable=inputs, mode='train')\n",
        "                outputs = outputs.view(-1, outputs.size(-1))\n",
        "                targets = targets.reshape(-1)\n",
        "\n",
        "                loss = criterion(outputs, targets)\n",
        "                total_val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "        print(f\"ðŸ§ª Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Save checkpoint\n",
        "        if checkpoint_dir:\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'train_loss': avg_train_loss,\n",
        "                'val_loss': avg_val_loss,\n",
        "            }, f\"{checkpoint_dir}/checkpoint_epoch_{epoch}.pt\")\n",
        "            print(f\"ðŸ’¾ Saved checkpoint to {checkpoint_dir}/checkpoint_epoch_{epoch}.pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5WS28HyhBij0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rGmYJpumQor"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import json\n",
        "\n",
        "# === Paths ===\n",
        "feature_dir_train = '/content/drive/MyDrive/msvd_split/train/features'\n",
        "feature_dir_val = '/content/drive/MyDrive/msvd_split/val/features'\n",
        "json_train = '/content/drive/MyDrive/msvd_split/train/train_captions.json'\n",
        "json_val = '/content/drive/MyDrive/msvd_split/val/val_captions.json'\n",
        "vocab_path = '/content/drive/MyDrive/msvd_split/vocab.json'\n",
        "\n",
        "# === Load vocab ===\n",
        "with open(vocab_path, 'r') as f:\n",
        "    vocab = json.load(f)\n",
        "\n",
        "# === Initialize datasets ===\n",
        "train_dataset = VideoCaptionDataset(\n",
        "    feature_dir=feature_dir_train,\n",
        "    json_path=json_train,\n",
        "    vocab=vocab,\n",
        "    max_caption_length=45\n",
        ")\n",
        "\n",
        "val_dataset = VideoCaptionDataset(\n",
        "    feature_dir=feature_dir_val,\n",
        "    json_path=json_val,\n",
        "    vocab=vocab,\n",
        "    max_caption_length=45\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-pAtiHOBBlMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1sX8NmMltFH",
        "outputId": "df9eed02-1050-40c4-c214-8d8a30060dc2"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸš€ Epoch [1/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [11:44<00:00,  4.48s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Training Loss: 5.4056\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:44<00:00,  3.68s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ§ª Validation Loss: 4.7492\n",
            "ðŸ’¾ Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_1.pt\n",
            "\n",
            "ðŸš€ Epoch [2/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:47<00:00,  3.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Training Loss: 4.5354\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:00<00:00, 15.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ§ª Validation Loss: 4.5767\n",
            "ðŸ’¾ Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_2.pt\n",
            "\n",
            "ðŸš€ Epoch [3/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:47<00:00,  3.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Training Loss: 4.3235\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:00<00:00, 15.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ§ª Validation Loss: 4.4360\n",
            "ðŸ’¾ Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_3.pt\n",
            "\n",
            "ðŸš€ Epoch [4/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:47<00:00,  3.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Training Loss: 4.1335\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:00<00:00, 15.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ§ª Validation Loss: 4.3701\n",
            "ðŸ’¾ Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_4.pt\n",
            "\n",
            "ðŸš€ Epoch [5/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:46<00:00,  3.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Training Loss: 4.0684\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:00<00:00, 15.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ§ª Validation Loss: 4.1582\n",
            "ðŸ’¾ Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_5.pt\n",
            "\n",
            "ðŸš€ Epoch [6/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:46<00:00,  3.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Training Loss: 3.9724\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:00<00:00, 14.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ§ª Validation Loss: 3.8608\n",
            "ðŸ’¾ Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_6.pt\n",
            "\n",
            "ðŸš€ Epoch [7/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:37<00:00,  4.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Training Loss: 3.8226\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:00<00:00, 14.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ§ª Validation Loss: 3.9018\n",
            "ðŸ’¾ Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_7.pt\n",
            "\n",
            "ðŸš€ Epoch [8/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:37<00:00,  4.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Training Loss: 3.8469\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:00<00:00, 13.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ§ª Validation Loss: 3.6028\n",
            "ðŸ’¾ Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_8.pt\n",
            "\n",
            "ðŸš€ Epoch [9/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:37<00:00,  4.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Training Loss: 3.6757\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:00<00:00, 15.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ§ª Validation Loss: 3.8771\n",
            "ðŸ’¾ Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_9.pt\n",
            "\n",
            "ðŸš€ Epoch [10/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:37<00:00,  4.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Training Loss: 3.6954\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:00<00:00, 15.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ§ª Validation Loss: 4.2475\n",
            "ðŸ’¾ Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_10.pt\n",
            "\n",
            "ðŸš€ Epoch [11/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:37<00:00,  4.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Training Loss: 3.6614\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:00<00:00, 15.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ§ª Validation Loss: 3.9698\n",
            "ðŸ’¾ Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_11.pt\n",
            "\n",
            "ðŸš€ Epoch [12/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:37<00:00,  4.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Training Loss: 3.5523\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:00<00:00, 15.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ§ª Validation Loss: 3.8174\n",
            "ðŸ’¾ Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_12.pt\n",
            "\n",
            "ðŸš€ Epoch [13/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:37<00:00,  4.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Training Loss: 3.5984\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:00<00:00, 15.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ§ª Validation Loss: 3.7847\n",
            "ðŸ’¾ Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_13.pt\n",
            "\n",
            "ðŸš€ Epoch [14/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:37<00:00,  4.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Training Loss: 3.5181\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:00<00:00, 15.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ§ª Validation Loss: 3.4858\n",
            "ðŸ’¾ Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_14.pt\n",
            "\n",
            "ðŸš€ Epoch [15/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:37<00:00,  4.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Training Loss: 3.5702\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:00<00:00, 15.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ§ª Validation Loss: 3.7691\n",
            "ðŸ’¾ Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_15.pt\n",
            "\n",
            "ðŸš€ Epoch [16/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:38<00:00,  4.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Training Loss: 3.5573\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:00<00:00, 15.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ§ª Validation Loss: 3.5152\n",
            "ðŸ’¾ Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_16.pt\n",
            "\n",
            "ðŸš€ Epoch [17/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:38<00:00,  4.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Training Loss: 3.5329\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:00<00:00, 14.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ§ª Validation Loss: 3.3757\n",
            "ðŸ’¾ Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_17.pt\n",
            "\n",
            "ðŸš€ Epoch [18/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:37<00:00,  4.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Training Loss: 3.5281\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:00<00:00, 13.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ§ª Validation Loss: 3.5658\n",
            "ðŸ’¾ Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_18.pt\n",
            "\n",
            "ðŸš€ Epoch [19/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:37<00:00,  4.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Training Loss: 3.4557\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:00<00:00, 15.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ§ª Validation Loss: 3.7165\n",
            "ðŸ’¾ Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_19.pt\n",
            "\n",
            "ðŸš€ Epoch [20/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:37<00:00,  4.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Training Loss: 3.4613\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:00<00:00, 15.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ§ª Validation Loss: 3.5408\n",
            "ðŸ’¾ Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_20.pt\n",
            "\n",
            "ðŸš€ Epoch [21/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:37<00:00,  4.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Training Loss: 3.4997\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:00<00:00, 15.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ§ª Validation Loss: 3.4975\n",
            "ðŸ’¾ Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_21.pt\n",
            "\n",
            "ðŸš€ Epoch [22/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:37<00:00,  4.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Training Loss: 3.4768\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:00<00:00, 15.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ§ª Validation Loss: 3.1841\n",
            "ðŸ’¾ Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_22.pt\n",
            "\n",
            "ðŸš€ Epoch [23/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:37<00:00,  4.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Training Loss: 3.4418\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:00<00:00, 15.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ§ª Validation Loss: 3.6346\n",
            "ðŸ’¾ Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_23.pt\n",
            "\n",
            "ðŸš€ Epoch [24/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:36<00:00,  4.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Training Loss: 3.3959\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:00<00:00, 15.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ§ª Validation Loss: 3.5725\n",
            "ðŸ’¾ Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_24.pt\n",
            "\n",
            "ðŸš€ Epoch [25/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:36<00:00,  4.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Training Loss: 3.4514\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:00<00:00, 15.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ§ª Validation Loss: 3.7555\n",
            "ðŸ’¾ Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_25.pt\n",
            "\n",
            "ðŸš€ Epoch [26/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:36<00:00,  4.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Training Loss: 3.4606\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:00<00:00, 15.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ§ª Validation Loss: 3.6945\n",
            "ðŸ’¾ Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_26.pt\n",
            "\n",
            "ðŸš€ Epoch [27/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:36<00:00,  4.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Training Loss: 3.4542\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:00<00:00, 15.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ§ª Validation Loss: 3.4339\n",
            "ðŸ’¾ Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_27.pt\n",
            "\n",
            "ðŸš€ Epoch [28/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:37<00:00,  4.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Training Loss: 3.4202\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:00<00:00, 13.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ§ª Validation Loss: 3.8997\n",
            "ðŸ’¾ Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_28.pt\n",
            "\n",
            "ðŸš€ Epoch [29/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:37<00:00,  4.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Training Loss: 3.5267\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:00<00:00, 15.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ§ª Validation Loss: 3.3861\n",
            "ðŸ’¾ Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_29.pt\n",
            "\n",
            "ðŸš€ Epoch [30/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:43<00:00,  3.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Training Loss: 3.3833\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:00<00:00, 15.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ§ª Validation Loss: 3.7119\n",
            "ðŸ’¾ Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_30.pt\n"
          ]
        }
      ],
      "source": [
        "from drive.MyDrive.S2VTModel_Attention import S2VTModel\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = S2VTModel(\n",
        "    vocab_size=len(vocab),\n",
        "    max_len=45,\n",
        "    dim_hidden=1024,\n",
        "    dim_word=512,\n",
        "    dim_vid=2048,\n",
        "    sos_id=vocab['<SOS>'],\n",
        "    eos_id=vocab['<EOS>'],\n",
        "    n_layers=1,\n",
        "    rnn_cell='lstm',\n",
        "    rnn_dropout_p=0.3\n",
        ")\n",
        "\n",
        "train_model(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    val_dataset=val_dataset,\n",
        "    vocab=vocab,\n",
        "    device=device,\n",
        "    num_epochs=30,\n",
        "    batch_size=8,\n",
        "    learning_rate=1e-4,\n",
        "    checkpoint_dir=\"/content/drive/MyDrive/msvd_split/checkpoints\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TqhddxycBn9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "nQMs5sWZ1v8A",
        "outputId": "4f281ca7-5659-491d-993c-22d77efb5742"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸš€ Epoch [1/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  13%|â–ˆâ–Ž        | 20/157 [00:11<01:15,  1.81it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-bb3dbb23173b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Then pass this to training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m train_model(\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-7f5e5a0da347>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataset, val_dataset, vocab, device, num_epochs, batch_size, learning_rate, checkpoint_dir)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_variable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [B, T, V]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;31m# [B*T, V]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/S2VTModel_Attention.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, vid_feats, target_variable, mode, beam_width)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mcurrent_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_variable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                 \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_cell_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'lstm'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mstate1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m                 \u001b[0minput2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0moutput2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/S2VTModel_Attention.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden, encoder_outputs)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [B, 1, H]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [B, T, 1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mattention_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [B, T, 1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_weights\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [B, H]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "checkpoint_path = \"/content/drive/MyDrive/msvd_split/checkpoints\"\n",
        "os.makedirs(checkpoint_path, exist_ok=True)\n",
        "\n",
        "# Then pass this to training\n",
        "train_model(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    val_dataset=val_dataset,\n",
        "    vocab=vocab,\n",
        "    device=device,\n",
        "    num_epochs=30,\n",
        "    batch_size=8,\n",
        "    learning_rate=1e-4,\n",
        "    checkpoint_dir=checkpoint_path\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-amwD-66BtOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mpr4DSIosEK0",
        "outputId": "8bb7fe71-d600-4bb5-ddf1-f8d6fd82995a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating on test set...\n",
            "\n",
            "Example 1\n",
            "Predicted    : a woman is cooking\n",
            "Ground Truth : the lady added ingredients to the water in the bowl and whisked it\n",
            "\n",
            "Example 2\n",
            "Predicted    : a man is riding a horse\n",
            "Ground Truth : a woman hits volleyballs at the beach\n",
            "\n",
            "Example 3\n",
            "Predicted    : a woman is dancing\n",
            "Ground Truth : two couple are talking with each other\n",
            "\n",
            "Example 4\n",
            "Predicted    : a man is riding a horse\n",
            "Ground Truth : a man was hosing down a jogger and the water turned black\n",
            "\n",
            "Example 5\n",
            "Predicted    : a man is riding a horse\n",
            "Ground Truth : a guy is kicking a ball\n",
            "\n",
            "Final BLEU-4 Score: 0.0709\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from drive.MyDrive.S2VTModel_Attention import S2VTModel\n",
        "\n",
        "# === Paths ===\n",
        "vocab_path = '/content/drive/MyDrive/msvd_split/vocab.json'\n",
        "test_json_path = '/content/drive/MyDrive/msvd_split/test/test_captions.json'\n",
        "test_feature_dir = '/content/drive/MyDrive/msvd_split/test/features'\n",
        "checkpoint_path = '/content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_30.pt'\n",
        "\n",
        "# === Load vocab ===\n",
        "with open(vocab_path, 'r') as f:\n",
        "    vocab = json.load(f)\n",
        "vocab_rev = {v: k for k, v in vocab.items()}\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# === Load model (must match training config) ===\n",
        "model = S2VTModel(\n",
        "    vocab_size=vocab_size,\n",
        "    max_len=45,\n",
        "    dim_hidden=1024,\n",
        "    dim_word=512,\n",
        "    dim_vid=2048,\n",
        "    sos_id=vocab['<SOS>'],\n",
        "    eos_id=vocab['<EOS>'],\n",
        "    n_layers=1,\n",
        "    rnn_cell='lstm',\n",
        "    rnn_dropout_p=0.3\n",
        ")\n",
        "\n",
        "# === Load checkpoint ===\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# === Load test data ===\n",
        "test_dataset = VideoCaptionDataset(test_feature_dir, test_json_path, vocab, max_caption_length=45)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# === Inference + BLEU computation ===\n",
        "print(\"Evaluating on test set...\")\n",
        "\n",
        "all_references = []\n",
        "all_hypotheses = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for idx, (video_feats, captions) in enumerate(test_loader):\n",
        "        video_feats, captions = video_feats.to(device), captions.to(device)\n",
        "\n",
        "        # Model inference\n",
        "        _, predicted_ids = model(video_feats, mode='inference')  # [1, T]\n",
        "\n",
        "        # Decode generated caption\n",
        "        pred_tokens = []\n",
        "        for tok in predicted_ids[0]:\n",
        "            word = vocab_rev.get(int(tok), '<UNK>')\n",
        "            if word == '<EOS>':\n",
        "                break\n",
        "            if word not in ['<SOS>', '<PAD>']:\n",
        "                pred_tokens.append(word)\n",
        "        all_hypotheses.append(pred_tokens)\n",
        "\n",
        "        # Decode ground truth caption\n",
        "        ref_tokens = []\n",
        "        for tok in captions[0]:\n",
        "            word = vocab_rev.get(int(tok), '<UNK>')\n",
        "            if word == '<EOS>':\n",
        "                break\n",
        "            if word not in ['<SOS>', '<PAD>']:\n",
        "                ref_tokens.append(word)\n",
        "        all_references.append([ref_tokens])  # list of references per hypothesis\n",
        "\n",
        "        if idx < 5:\n",
        "            print(f\"\\nExample {idx + 1}\")\n",
        "            print(f\"Predicted    : {' '.join(pred_tokens)}\")\n",
        "            print(f\"Ground Truth : {' '.join(ref_tokens)}\")\n",
        "\n",
        "# === Compute BLEU score ===\n",
        "bleu_score = corpus_bleu(all_references, all_hypotheses)\n",
        "print(f\"\\nFinal BLEU-4 Score: {bleu_score:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ue4t55HiBw0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ducIHQcfBw9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qW4sqh6zBxAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zd-G7wJsBxDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "szFaxyLOBxGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ntvohxNdBxJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kn3ByhCr8_pB",
        "outputId": "473c0e3f-d4af-42b7-97a8-23377bf72c0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/sliders.py:61: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  if event.key is 'enter':\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import subprocess\n",
        "import tempfile\n",
        "import glob\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "from moviepy.editor import VideoFileClip\n",
        "\n",
        "video_path = '/content/drive/MyDrive/YouTubeClips/-8y1Q0rA3n8_108_115.avi'\n",
        "\n",
        "def extract_frames_ffmpeg(video_path, output_dir, frame_rate=1):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    cmd = [\n",
        "        'ffmpeg', '-i', video_path, '-vf', f\"fps={frame_rate},scale=224:224\",\n",
        "        os.path.join(output_dir, '%06d.jpg'), '-hide_banner', '-loglevel', 'error'\n",
        "    ]\n",
        "    subprocess.run(cmd)\n",
        "\n",
        "def load_and_sample_frames(frame_dir, n_frames=40):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    frames = sorted(glob.glob(os.path.join(frame_dir, '*.jpg')))\n",
        "    total = len(frames)\n",
        "    if total == 0:\n",
        "        raise ValueError(\"No frames extracted\")\n",
        "\n",
        "    indices = np.linspace(0, total - 1, min(n_frames, total)).astype(int)\n",
        "    sampled = [frames[i] for i in indices]\n",
        "\n",
        "    images = [transform(Image.open(f).convert('RGB')) for f in sampled]\n",
        "    return torch.stack(images)  # [T, 3, H, W]\n",
        "\n",
        "def extract_video_tensor(video_path, n_frames=40):\n",
        "    with tempfile.TemporaryDirectory() as tmpdir:\n",
        "        extract_frames_ffmpeg(video_path, tmpdir)\n",
        "        tensor = load_and_sample_frames(tmpdir, n_frames)\n",
        "    return tensor\n",
        "\n",
        "def generate_caption_from_video(video_path, model, feature_extractor, vocab_rev, device, max_len=45):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    video_tensor = extract_video_tensor(video_path).to(device)  # [T, 3, 224, 224]\n",
        "    with torch.no_grad():\n",
        "        features = feature_extractor(video_tensor)  # e.g. ResNet: [T, 2048]\n",
        "        features = features.unsqueeze(0)  # Add batch dim: [1, T, 2048]\n",
        "        _, predicted_ids = model(features, mode='inference')\n",
        "\n",
        "    tokens = []\n",
        "    for idx in predicted_ids[0]:\n",
        "        word = vocab_rev.get(int(idx), '<UNK>')\n",
        "        if word == '<EOS>':\n",
        "            break\n",
        "        if word not in ['<SOS>', '<PAD>']:\n",
        "            tokens.append(word)\n",
        "\n",
        "    return ' '.join(tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QSwbZ8oYBx5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        },
        "id": "jkvMExP-9vSQ",
        "outputId": "bbf4c099-52dc-44ff-ecb2-b2236571393d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "\n",
            "Downloading: \"https://download.pytorch.org/models/resnet152-b121ed2d.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-b121ed2d.pth\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 230M/230M [00:04<00:00, 53.1MB/s]\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "No frames extracted",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-2f546e2765e9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mvideo_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/msvd_split/test/videos/-_hbPLsZvvo_19_25.avi'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m caption = generate_caption_from_video(\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0mvideo_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-7eabce44e5e4>\u001b[0m in \u001b[0;36mgenerate_caption_from_video\u001b[0;34m(video_path, model, feature_extractor, vocab_rev, device, max_len)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mvideo_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_video_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [T, 3, 224, 224]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_tensor\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# e.g. ResNet: [T, 2048]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-7eabce44e5e4>\u001b[0m in \u001b[0;36mextract_video_tensor\u001b[0;34m(video_path, n_frames)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtempfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTemporaryDirectory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtmpdir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mextract_frames_ffmpeg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmpdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_sample_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmpdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_frames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-7eabce44e5e4>\u001b[0m in \u001b[0;36mload_and_sample_frames\u001b[0;34m(frame_dir, n_frames)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No frames extracted\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: No frames extracted"
          ]
        }
      ],
      "source": [
        "import pretrainedmodels\n",
        "from pretrainedmodels import utils\n",
        "from drive.MyDrive.S2VTModel_Attention import S2VTModel\n",
        "import torch\n",
        "import json\n",
        "\n",
        "\n",
        "# Load ResNet152 and remove final classification layer\n",
        "resnet = pretrainedmodels.resnet152(pretrained='imagenet')\n",
        "resnet.last_linear = utils.Identity()  # Remove classification head\n",
        "feature_extractor = resnet.cuda().eval()\n",
        "load_image_fn = utils.LoadTransformImage(resnet)\n",
        "\n",
        "\n",
        "\n",
        "with open('/content/drive/MyDrive/msvd_split/vocab.json', 'r') as f:\n",
        "    vocab = json.load(f)\n",
        "\n",
        "vocab_rev = {v: k for k, v in vocab.items()}  # This is what you must pass\n",
        "\n",
        "\n",
        "\n",
        "checkpoint = torch.load('/content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_30.pt', map_location='cuda')\n",
        "model = S2VTModel(\n",
        "    vocab_size=len(vocab),\n",
        "    max_len=45,\n",
        "    dim_hidden=1024,\n",
        "    dim_word=512,\n",
        "    dim_vid=2048,\n",
        "    sos_id=vocab['<SOS>'],\n",
        "    eos_id=vocab['<EOS>'],\n",
        "    n_layers=1,\n",
        "    rnn_cell='lstm',\n",
        "    rnn_dropout_p=0.3\n",
        ")\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "video_path = '/content/drive/MyDrive/msvd_split/test/videos/-_hbPLsZvvo_19_25.avi'\n",
        "\n",
        "caption = generate_caption_from_video(\n",
        "    video_path=video_path,\n",
        "    model=model,\n",
        "    feature_extractor=feature_extractor,\n",
        "    vocab_rev=vocab_rev,\n",
        "    device=torch.device('cuda')\n",
        ")\n",
        "\n",
        "print(\"ðŸŽ¬ Generated Caption:\", caption)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hO4yzZYpblnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mKtj7QzJblvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "import subprocess\n",
        "import glob\n",
        "import numpy as np\n",
        "import torch\n",
        "import pretrainedmodels\n",
        "from pretrainedmodels import utils\n",
        "\n",
        "# === Setup ===\n",
        "video_path = '/content/01_inpainted.mp4'\n",
        "frame_dir = 'frames_tmp'\n",
        "feature_save_path = 'example_video.npy'\n",
        "\n",
        "# === Extract frames ===\n",
        "if os.path.exists(frame_dir):\n",
        "    subprocess.call(['rm', '-rf', frame_dir])\n",
        "os.makedirs(frame_dir, exist_ok=True)\n",
        "\n",
        "cmd = [\n",
        "    'ffmpeg', '-y', '-i', video_path,\n",
        "    '-vf', 'scale=400:300',\n",
        "    '-qscale:v', '2',\n",
        "    f'{frame_dir}/%06d.jpg'\n",
        "]\n",
        "subprocess.call(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "\n",
        "# === Load pretrained model (same as training) ===\n",
        "model = pretrainedmodels.resnet152(pretrained='imagenet')\n",
        "model.last_linear = utils.Identity()\n",
        "model.eval().cuda()\n",
        "\n",
        "load_image_fn = utils.LoadTransformImage(model)\n",
        "\n",
        "# === Load 40 frames ===\n",
        "image_list = sorted(glob.glob(f'{frame_dir}/*.jpg'))\n",
        "samples = np.linspace(0, len(image_list) - 1, 40).astype(int)\n",
        "image_list = [image_list[i] for i in samples]\n",
        "C, H, W = 3, 224, 224\n",
        "\n",
        "images = torch.zeros((len(image_list), C, H, W))\n",
        "for i, img_path in enumerate(image_list):\n",
        "    img = load_image_fn(img_path)\n",
        "    images[i] = img\n",
        "\n",
        "images = images.cuda()\n",
        "\n",
        "# === Extract features ===\n",
        "with torch.no_grad():\n",
        "    feats = model(images).cpu().numpy()\n",
        "np.save(feature_save_path, feats)\n",
        "print(f\"âœ… Feature extracted and saved as {feature_save_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fesw31i-bly_",
        "outputId": "69d02411-a4f8-4bea-a211-855bc8822a65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Feature extracted and saved as example_video.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install pretrainedmodels for ResNet152\n",
        "!pip install pretrainedmodels\n",
        "\n",
        "# Upload a video (e.g., example_video.avi)\n",
        "from google.colab import files\n",
        "uploaded = files.upload()  # upload your .avi file\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596
        },
        "id": "dNqmO1budQHS",
        "outputId": "95cf5303-bb8a-4fd4-865d-b627bab05743"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pretrainedmodels in /usr/local/lib/python3.11/dist-packages (0.7.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from pretrainedmodels) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from pretrainedmodels) (0.21.0+cu124)\n",
            "Requirement already satisfied: munch in /usr/local/lib/python3.11/dist-packages (from pretrainedmodels) (4.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from pretrainedmodels) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->pretrainedmodels) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->pretrainedmodels) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->pretrainedmodels) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->pretrainedmodels) (3.0.2)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4fab05b9-ee9e-4b3e-b771-9b093c85cf80\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4fab05b9-ee9e-4b3e-b771-9b093c85cf80\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 01_inpainted.mp4 to 01_inpainted (1).mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob, subprocess\n",
        "import numpy as np\n",
        "import torch\n",
        "import pretrainedmodels\n",
        "from pretrainedmodels import utils\n",
        "\n",
        "# === Paths\n",
        "video_path = 'example_video.avi'  # or replace with uploaded file name\n",
        "frame_dir = 'frames_tmp'\n",
        "feature_save_path = 'example_video.npy'\n",
        "\n",
        "# === Extract frames from video\n",
        "if os.path.exists(frame_dir):\n",
        "    subprocess.call(['rm', '-rf', frame_dir])\n",
        "os.makedirs(frame_dir, exist_ok=True)\n",
        "\n",
        "cmd = [\n",
        "    'ffmpeg', '-y', '-i', video_path,\n",
        "    '-vf', 'scale=400:300',\n",
        "    '-qscale:v', '2',\n",
        "    f'{frame_dir}/%06d.jpg'\n",
        "]\n",
        "subprocess.call(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "\n",
        "# === Load pretrained ResNet152\n",
        "model_cnn = pretrainedmodels.resnet152(pretrained='imagenet')\n",
        "model_cnn.last_linear = utils.Identity()\n",
        "model_cnn.eval().cuda()\n",
        "load_image_fn = utils.LoadTransformImage(model_cnn)\n",
        "\n",
        "# === Load 40 frames\n",
        "image_list = sorted(glob.glob(f'{frame_dir}/*.jpg'))\n",
        "samples = np.linspace(0, len(image_list) - 1, 40).astype(int)\n",
        "image_list = [image_list[i] for i in samples]\n",
        "images = torch.zeros((len(image_list), 3, 224, 224))\n",
        "\n",
        "for i, img_path in enumerate(image_list):\n",
        "    images[i] = load_image_fn(img_path)\n",
        "\n",
        "# === Extract features\n",
        "with torch.no_grad():\n",
        "    feats = model_cnn(images.cuda()).cpu().numpy()\n",
        "np.save(feature_save_path, feats)\n",
        "print(\"âœ… Features saved as:\", feature_save_path)\n"
      ],
      "metadata": {
        "id": "CmPLm-itdk9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aH78Ya89NIyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pretrainedmodels\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_ptiFI4NI_2",
        "outputId": "cf9b3016-d354-41f0-a76f-0af7d0b44d93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pretrainedmodels in /usr/local/lib/python3.11/dist-packages (0.7.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from pretrainedmodels) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from pretrainedmodels) (0.21.0+cu124)\n",
            "Requirement already satisfied: munch in /usr/local/lib/python3.11/dist-packages (from pretrainedmodels) (4.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from pretrainedmodels) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->pretrainedmodels) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->pretrainedmodels) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->pretrainedmodels) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->pretrainedmodels) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0w7AVDgqm962"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_CP_BNMwnAPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h5W5uJ7WnAR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import subprocess\n",
        "\n",
        "video_path = '/content/t1.mp4'\n",
        "frame_dir = '/content/frames_t1mp4'\n",
        "\n",
        "if os.path.exists(frame_dir):\n",
        "    shutil.rmtree(frame_dir)\n",
        "os.makedirs(frame_dir, exist_ok=True)\n",
        "\n",
        "!ffmpeg -i {video_path} -vf \"scale=400:300\" -qscale:v 2 {frame_dir}/%06d.jpg\n"
      ],
      "metadata": {
        "id": "S9PB72UKnAVe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c56490f6-2f9c-4bde-d902-cc797c7dde29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
            "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 70.100 / 56. 70.100\n",
            "  libavcodec     58.134.100 / 58.134.100\n",
            "  libavformat    58. 76.100 / 58. 76.100\n",
            "  libavdevice    58. 13.100 / 58. 13.100\n",
            "  libavfilter     7.110.100 /  7.110.100\n",
            "  libswscale      5.  9.100 /  5.  9.100\n",
            "  libswresample   3.  9.100 /  3.  9.100\n",
            "  libpostproc    55.  9.100 / 55.  9.100\n",
            "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '/content/t1.mp4':\n",
            "  Metadata:\n",
            "    major_brand     : mp42\n",
            "    minor_version   : 0\n",
            "    compatible_brands: mp42isom\n",
            "  Duration: 00:00:04.16, start: 0.000000, bitrate: 1739 kb/s\n",
            "  Stream #0:0(und): Video: h264 (High) (avc1 / 0x31637661), yuv420p(tv, bt709), 848x480, 1504 kb/s, 20.07 fps, 20 tbr, 30k tbn, 60k tbc (default)\n",
            "    Metadata:\n",
            "      vendor_id       : [0][0][0][0]\n",
            "  Stream #0:1(und): Audio: aac (LC) (mp4a / 0x6134706D), 48000 Hz, stereo, fltp, 256 kb/s (default)\n",
            "    Metadata:\n",
            "      vendor_id       : [0][0][0][0]\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (h264 (native) -> mjpeg (native))\n",
            "Press [q] to stop, [?] for help\n",
            "\u001b[1;34m[swscaler @ 0x58ca3cc6cc80] \u001b[0m\u001b[0;33mdeprecated pixel format used, make sure you did set range correctly\n",
            "\u001b[0mOutput #0, image2, to '/content/frames_t1mp4/%06d.jpg':\n",
            "  Metadata:\n",
            "    major_brand     : mp42\n",
            "    minor_version   : 0\n",
            "    compatible_brands: mp42isom\n",
            "    encoder         : Lavf58.76.100\n",
            "  Stream #0:0(und): Video: mjpeg, yuvj420p(pc, bt709, progressive), 400x300, q=2-31, 200 kb/s, 20 fps, 20 tbn (default)\n",
            "    Metadata:\n",
            "      vendor_id       : [0][0][0][0]\n",
            "      encoder         : Lavc58.134.100 mjpeg\n",
            "    Side data:\n",
            "      cpb: bitrate max/min/avg: 0/0/200000 buffer size: 0 vbv_delay: N/A\n",
            "frame=    1 fps=0.0 q=0.0 size=N/A time=00:00:00.00 bitrate=N/A speed=   0x    \rframe=   82 fps=0.0 q=2.0 Lsize=N/A time=00:00:04.10 bitrate=N/A speed=13.6x    \n",
            "video:822kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: unknown\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pretrainedmodels\n",
        "from pretrainedmodels import utils as pmutils\n",
        "\n",
        "# Load pretrained ResNet-152 and remove classification layer\n",
        "resnet = pretrainedmodels.resnet152(pretrained='imagenet')\n",
        "resnet.last_linear = pmutils.Identity()\n",
        "resnet = resnet.eval().cuda()\n",
        "\n",
        "# Load image preprocessing function\n",
        "load_image_fn = pmutils.LoadTransformImage(resnet)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQ6DPP6AQx9E",
        "outputId": "f01ad82b-eb6b-4072-ad77-ee8258d659c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pretrainedmodels\n",
        "from pretrainedmodels import utils as pmutils\n",
        "\n",
        "resnet = pretrainedmodels.resnet152(pretrained='imagenet')\n",
        "resnet.last_linear = pmutils.Identity()\n",
        "resnet = resnet.eval().cuda()\n",
        "load_image_fn = pmutils.LoadTransformImage(resnet)\n"
      ],
      "metadata": {
        "id": "8bRQpBixQ5qs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "frame_dir = '/content/frames_t1mp4'\n",
        "frame_list = sorted(glob.glob(os.path.join(frame_dir, '*.jpg')))\n",
        "selected_indices = np.linspace(0, len(frame_list) - 1, 40, dtype=int)\n",
        "selected_frames = [frame_list[i] for i in selected_indices]\n",
        "\n",
        "# Feature extraction\n",
        "C, H, W = 3, 224, 224\n",
        "features = torch.zeros((len(selected_frames), C, H, W)).cuda()\n",
        "for i, frame_path in enumerate(selected_frames):\n",
        "    img = load_image_fn(frame_path)\n",
        "    features[i] = img\n",
        "\n",
        "with torch.no_grad():\n",
        "    video_feats = resnet(features).unsqueeze(0).cpu()  # [1, 40, 2048]\n"
      ],
      "metadata": {
        "id": "-XVx_g-JQ6im"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hhaT4yDYQyEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "frame_dir = '/content/frames_t1mp4'  # ðŸ‘ˆ ADD THIS LINE\n",
        "\n",
        "# Select 40 frames\n",
        "frame_list = sorted(glob.glob(os.path.join(frame_dir, '*.jpg')))\n",
        "selected_indices = np.linspace(0, len(frame_list) - 1, 40, dtype=int)\n",
        "selected_frames = [frame_list[i] for i in selected_indices]\n",
        "\n",
        "# Extract features\n",
        "C, H, W = 3, 224, 224\n",
        "features = torch.zeros((len(selected_frames), C, H, W)).cuda()\n",
        "for i, frame_path in enumerate(selected_frames):\n",
        "    img = load_image_fn(frame_path)\n",
        "    features[i] = img\n",
        "\n",
        "with torch.no_grad():\n",
        "    video_feats = resnet(features).unsqueeze(0).cpu()  # [1, 40, 2048]\n"
      ],
      "metadata": {
        "id": "BYwqr2UaVES3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from drive.MyDrive.S2VTModel_Attention import S2VTModel\n",
        "\n",
        "# Load vocab\n",
        "vocab_path = '/content/drive/MyDrive/msvd_split/vocab.json'\n",
        "with open(vocab_path, 'r') as f:\n",
        "    vocab = json.load(f)\n",
        "vocab_rev = {v: k for k, v in vocab.items()}\n",
        "\n",
        "# Load model\n",
        "model = S2VTModel(\n",
        "    vocab_size=len(vocab),\n",
        "    max_len=45,\n",
        "    dim_hidden=1024,\n",
        "    dim_word=512,\n",
        "    dim_vid=2048,\n",
        "    sos_id=vocab['<SOS>'],\n",
        "    eos_id=vocab['<EOS>'],\n",
        "    rnn_cell='lstm',\n",
        "    rnn_dropout_p=0.3\n",
        ")\n",
        "\n",
        "checkpoint_path = '/content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_30.pt'\n",
        "checkpoint = torch.load(checkpoint_path, map_location='cuda')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model = model.eval().cuda()\n"
      ],
      "metadata": {
        "id": "AThfjvQdUlJs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c52fe8a-ebbe-4acd-ea5a-329633fbb6d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "video_feats = video_feats.cuda()\n",
        "\n",
        "with torch.no_grad():\n",
        "    _, predicted_ids = model(video_feats, mode='inference')\n",
        "\n",
        "# Decode prediction\n",
        "caption = []\n",
        "for tok in predicted_ids[0]:\n",
        "    word = vocab_rev.get(int(tok), '<UNK>')\n",
        "    if word == '<EOS>':\n",
        "        break\n",
        "    if word not in ['<SOS>', '<PAD>']:\n",
        "        caption.append(word)\n",
        "\n",
        "print(\"ðŸŽ¬ Caption for 04.mp4:\\n\", ' '.join(caption))\n"
      ],
      "metadata": {
        "id": "nlIiO5xJUqwl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab6c831c-1549-4cea-f220-55060f26b224"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸŽ¬ Caption for 04.mp4:\n",
            " a woman is slicing a potato\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i2k5R6WMR4w7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OX2QyawhRzs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vWUzEl5-RzwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HTA5mAqMRzzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#avi\n",
        "\n",
        "# Step 1: Extract frames from AVI video\n",
        "import os\n",
        "import shutil\n",
        "import subprocess\n",
        "\n",
        "video_path = '/content/Cxxx6wJ1jNo_0_10.avi'  # ðŸ‘ˆ Change filename if needed\n",
        "frame_dir = '/content/frames_your_video'\n",
        "\n",
        "if os.path.exists(frame_dir):\n",
        "    shutil.rmtree(frame_dir)\n",
        "os.makedirs(frame_dir, exist_ok=True)\n",
        "\n",
        "!ffmpeg -i \"$video_path\" -vf \"scale=400:300\" -qscale:v 2 \"$frame_dir/%06d.jpg\"\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7R-921qRz2H",
        "outputId": "f2a94682-e084-4e2e-ae76-e1513dbd40e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
            "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 70.100 / 56. 70.100\n",
            "  libavcodec     58.134.100 / 58.134.100\n",
            "  libavformat    58. 76.100 / 58. 76.100\n",
            "  libavdevice    58. 13.100 / 58. 13.100\n",
            "  libavfilter     7.110.100 /  7.110.100\n",
            "  libswscale      5.  9.100 /  5.  9.100\n",
            "  libswresample   3.  9.100 /  3.  9.100\n",
            "  libpostproc    55.  9.100 / 55.  9.100\n",
            "Input #0, avi, from '/content/Cxxx6wJ1jNo_0_10.avi':\n",
            "  Metadata:\n",
            "    software        : MEncoder SVN-r33477-4.2.1\n",
            "  Duration: 00:00:10.07, start: 0.000000, bitrate: 266 kb/s\n",
            "  Stream #0:0: Video: h264 (Main) (H264 / 0x34363248), yuv420p(progressive), 320x240 [SAR 1:1 DAR 4:3], 262 kb/s, 15 fps, 15 tbr, 15 tbn, 30 tbc\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (h264 (native) -> mjpeg (native))\n",
            "Press [q] to stop, [?] for help\n",
            "\u001b[1;34m[swscaler @ 0x5b2693dd2640] \u001b[0m\u001b[0;33mdeprecated pixel format used, make sure you did set range correctly\n",
            "\u001b[0mOutput #0, image2, to '/content/frames_your_video/%06d.jpg':\n",
            "  Metadata:\n",
            "    software        : MEncoder SVN-r33477-4.2.1\n",
            "    encoder         : Lavf58.76.100\n",
            "  Stream #0:0: Video: mjpeg, yuvj420p(pc, progressive), 400x300 [SAR 1:1 DAR 4:3], q=2-31, 200 kb/s, 15 fps, 15 tbn\n",
            "    Metadata:\n",
            "      encoder         : Lavc58.134.100 mjpeg\n",
            "    Side data:\n",
            "      cpb: bitrate max/min/avg: 0/0/200000 buffer size: 0 vbv_delay: N/A\n",
            "frame=    1 fps=0.0 q=0.0 size=N/A time=00:00:00.00 bitrate=N/A speed=   0x    \rframe=  151 fps=0.0 q=2.0 Lsize=N/A time=00:00:10.06 bitrate=N/A speed=29.2x    \n",
            "video:2224kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: unknown\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pretrainedmodels\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywCJEYxqUxZ9",
        "outputId": "c6f3f9a3-be82-4a1a-e5f3-ff24e4573a36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pretrainedmodels\n",
            "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n",
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/58.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from pretrainedmodels) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from pretrainedmodels) (0.21.0+cu124)\n",
            "Collecting munch (from pretrainedmodels)\n",
            "  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from pretrainedmodels) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->pretrainedmodels)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->pretrainedmodels)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->pretrainedmodels)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->pretrainedmodels)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->pretrainedmodels)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->pretrainedmodels)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->pretrainedmodels)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->pretrainedmodels)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->pretrainedmodels)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->pretrainedmodels)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->pretrainedmodels) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->pretrainedmodels) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->pretrainedmodels) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->pretrainedmodels) (3.0.2)\n",
            "Downloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m97.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pretrainedmodels\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60945 sha256=8490b591acedf90c060540523251f4e214d891d1d610a5ab0f979b2301f0ec05\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/5b/96/fd94bc35962d7c6b699e8814db545155ac91d2b95785e1b035\n",
            "Successfully built pretrainedmodels\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, munch, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, pretrainedmodels\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed munch-4.0.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pretrainedmodels-0.7.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Load ResNet152 for feature extraction\n",
        "import pretrainedmodels\n",
        "from pretrainedmodels import utils as pmutils\n",
        "import torch\n",
        "\n",
        "resnet = pretrainedmodels.resnet152(pretrained='imagenet')\n",
        "resnet.last_linear = pmutils.Identity()\n",
        "resnet = resnet.eval().cuda()\n",
        "load_image_fn = pmutils.LoadTransformImage(resnet)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "t9OS4kzbT7fc",
        "outputId": "723c0864-79c9-4dfb-ecaa-80f2be04fe01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet152-b121ed2d.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-b121ed2d.pth\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 230M/230M [00:01<00:00, 123MB/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-879d5c45ff3c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mresnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpretrainedmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet152\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'imagenet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mresnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_linear\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpmutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIdentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mresnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mload_image_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpmutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoadTransformImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m         \"\"\"\n\u001b[0;32m-> 1053\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mipu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m         \"\"\"\n\u001b[0;32m-> 1053\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mipu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"LAZY\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Extract features from 40 frames\n",
        "import glob\n",
        "import numpy as np\n",
        "\n",
        "frame_list = sorted(glob.glob(os.path.join(frame_dir, '*.jpg')))\n",
        "selected_indices = np.linspace(0, len(frame_list) - 1, 40, dtype=int)\n",
        "selected_frames = [frame_list[i] for i in selected_indices]\n",
        "\n",
        "C, H, W = 3, 224, 224\n",
        "features = torch.zeros((len(selected_frames), C, H, W)).cuda()\n",
        "for i, frame_path in enumerate(selected_frames):\n",
        "    img = load_image_fn(frame_path)\n",
        "    features[i] = img\n",
        "\n",
        "with torch.no_grad():\n",
        "    video_feats = resnet(features).unsqueeze(0).cpu()\n"
      ],
      "metadata": {
        "id": "OOmGgEZyUT_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Load trained S2VT model\n",
        "import json\n",
        "from drive.MyDrive.youtube_captioning_with_attention.S2VTModel_Attention import S2VTModel\n",
        "\n",
        "vocab_path = '/content/drive/MyDrive/msvd_split/vocab.json'\n",
        "with open(vocab_path, 'r') as f:\n",
        "    vocab = json.load(f)\n",
        "vocab_rev = {v: k for k, v in vocab.items()}\n",
        "\n",
        "model = S2VTModel(\n",
        "    vocab_size=len(vocab),\n",
        "    max_len=45,\n",
        "    dim_hidden=1024,\n",
        "    dim_word=512,\n",
        "    dim_vid=2048,\n",
        "    sos_id=vocab['<SOS>'],\n",
        "    eos_id=vocab['<EOS>'],\n",
        "    rnn_cell='lstm',\n",
        "    rnn_dropout_p=0.3\n",
        ")\n",
        "\n",
        "checkpoint_path = '/content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_30.pt'\n",
        "checkpoint = torch.load(checkpoint_path, map_location='cuda')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model = model.eval().cuda()\n"
      ],
      "metadata": {
        "id": "WrIbzPIGUXW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Generate caption\n",
        "video_feats = video_feats.cuda()\n",
        "\n",
        "with torch.no_grad():\n",
        "    _, predicted_ids = model(video_feats, mode='inference')\n",
        "\n",
        "caption = []\n",
        "for tok in predicted_ids[0]:\n",
        "    word = vocab_rev.get(int(tok), '<UNK>')\n",
        "    if word == '<EOS>':\n",
        "        break\n",
        "    if word not in ['<SOS>', '<PAD>']:\n",
        "        caption.append(word)\n",
        "\n",
        "print(\"ðŸŽ¬ Caption for your AVI video:\\n\", ' '.join(caption))\n"
      ],
      "metadata": {
        "id": "fUBD_lLtUZ8W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}