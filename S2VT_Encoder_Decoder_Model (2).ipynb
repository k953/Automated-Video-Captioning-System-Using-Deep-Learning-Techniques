{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUBPOivpABXs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06648ff7-d93f-4615-8dd6-45802d05681a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baFdE_w2cvX5",
        "outputId": "6e42e63a-f4ca-421c-ba15-b69f94725ffc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the required libraries\n",
        "!pip install numpy torch torchvision nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uj5eY5BrAq1w",
        "outputId": "74bf7607-a401-4132-9ca4-73133ab842c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UnXWRQgtAvOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpRH3Lfbh0yJ",
        "outputId": "4ee56eef-db73-4b59-dbb7-3b6fe0da77fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Video file #.avi not found in /content/drive/MyDrive/YouTubeClips.\n",
            "Data split and saved successfully.\n"
          ]
        }
      ],
      "source": [
        "#video and json split\n",
        "\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "def split_json_data(input_json_path, videos_dir, output_dir, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
        "    # Load JSON data\n",
        "    with open(input_json_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Shuffle the video data to ensure randomness\n",
        "    video_entries = data['videos']\n",
        "    random.shuffle(video_entries)\n",
        "\n",
        "    # Calculate the split sizes\n",
        "    total_videos = len(video_entries)\n",
        "    train_size = int(total_videos * train_ratio)\n",
        "    val_size = int(total_videos * val_ratio)\n",
        "    test_size = total_videos - train_size - val_size\n",
        "\n",
        "    # Split the data\n",
        "    train_videos = video_entries[:train_size]\n",
        "    val_videos = video_entries[train_size:train_size + val_size]\n",
        "    test_videos = video_entries[train_size + val_size:]\n",
        "\n",
        "    # Organize sentences based on video_ids for each split\n",
        "    video_sentences = {video['video_id']: [] for video in video_entries}\n",
        "    for sentence in data['sentences']:\n",
        "        video_sentences[sentence['video_id']].append(sentence)\n",
        "\n",
        "    def create_split_data(split_videos, split_name):\n",
        "        split_data = {\n",
        "            \"videos\": split_videos,\n",
        "            \"sentences\": []\n",
        "        }\n",
        "        split_videos_dir = os.path.join(output_dir, split_name, \"videos\")\n",
        "        os.makedirs(split_videos_dir, exist_ok=True)\n",
        "\n",
        "        # Add captions and copy video files\n",
        "        for video in split_videos:\n",
        "            video_id = video['video_id']\n",
        "            split_data['sentences'].extend(video_sentences[video_id])\n",
        "\n",
        "            # Copy video file to the split directory\n",
        "            video_filename = f\"{video_id}.avi\"\n",
        "            src_video_path = os.path.join(videos_dir, video_filename)\n",
        "            dst_video_path = os.path.join(split_videos_dir, video_filename)\n",
        "            if os.path.exists(src_video_path):\n",
        "                shutil.copy(src_video_path, dst_video_path)\n",
        "            else:\n",
        "                print(f\"Warning: Video file {video_filename} not found in {videos_dir}.\")\n",
        "\n",
        "        # Save the JSON file for the split\n",
        "        split_json_path = os.path.join(output_dir, split_name, f\"{split_name}_captions.json\")\n",
        "        with open(split_json_path, 'w') as f:\n",
        "            json.dump(split_data, f, indent=4)\n",
        "\n",
        "    # Create each split\n",
        "    create_split_data(train_videos, \"train\")\n",
        "    create_split_data(val_videos, \"val\")\n",
        "    create_split_data(test_videos, \"test\")\n",
        "\n",
        "    print(\"Data split and saved successfully.\")\n",
        "\n",
        "# Define paths based on your directory structure\n",
        "input_json_path = '/content/drive/MyDrive/msvd_captions.json'  # Path to the original JSON file\n",
        "videos_dir = '/content/drive/MyDrive/YouTubeClips'  # Directory where video files are stored\n",
        "output_dir = '/content/drive/MyDrive/msvd_split'  # Directory where you want to save the splits\n",
        "\n",
        "# Run the split function\n",
        "split_json_data(input_json_path, videos_dir, output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4qWoJn_CAynM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kY022BoYyTbH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64508d22-11da-4f45-c1fb-e04ebda45b64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train] Videos present in folder but missing in JSON 'videos' section: {'vZa13vJugGU_0_30', 'O_NWtDShLeg_21_25', 'BApIQn69EVE_10_16', '_WRC7HXBJpU_395_401', 'emblM4a76jg_5_15', 'VahnQw2gTQY_315_320', 'IHIa75B9AhI_2_26', '7NNg0_n-bS8_21_30', '9Q0JfdP36kI_167_170', 'ItFqogTmAvQ_389_395', 'lb8J2zCQTlo_3_8', '0SMOK2ql7Pg_5_12', 'ub-aYLzCF_Q_1_10', 'IhwPQL9dFYc_78_88', 'DKgHYLDebx0_0_8', '9IrWyZ0KZuk_160_166', 'EiylMb_mWk4_2_20', 'jjl2ZMdFCsw_17_35', 'm1NR0uNNs5Y_160_166', '6t0BpjwYKco_118_127', 'NMlKMfiHSho_1_15', 'R8FDJgVW3Vc_0_4', 'bLqmf8x7rLI_2_8', 'JK1R9k1WDpc_6_15', 'RMznbCn5sQs_0_10', 'De815YpTBic_41_48', 'ACOmKiJDkA4_121_128', 'UgUFP5baQ9Y_0_7', 'z0Si1XxMibg_0_30', 'zbAk0gX7kas_16_24', 'akXjIEoecNs_2_12', 'W6_XuNhgtrM_2_5', 'q5ZRMvjzhXQ_15_29', 'ywHBKayhyvQ_19_28', '9OvXb1fot74_21_29', 'PeUHy0A1GF0_68_73', 'B4foOe9kUgY_0_8', '4z3b4mnw5y4_56_60', 'sWqi41wyXcQ_68_79', 'cwkjJrGpoaU_30_41', 'HM-ZDoRWiH4_0_5', 'kWLNZzuo3do_217_222', 'fF89MasBFLw_321_326', '8MVo7fje_oE_125_130', 'jxdubZzQrio_33_46', '-zOrV-5vh1A_69_76', 'f_CvW22Eauc_16_23', 'hksxtbcS780_7_14', 'vE1gvaM3iAs_39_46', 'ZYaxg5NZW_g_21_27', 'UnWgz-mVMXU_2_6', 'r4qv_BNlQNk_93_98', 'ZsrPKtx0qyg_7_15', '4D1o4FC8YbA_207_216', 'tZmDWltBziM_42_47', 'yAD_TS5L2d4_4_11', 'itxzpFW1z7E_22_36', 'nPj6EcVGoUY_7_15', 'IiTzJQE67FA_36_46', '6t0BpjwYKco_261_284', 'AKVBK-0veE8_5_12', 'jI58q6rcNLc_8_18', '4_WZN7uW0NY_140_145', 'kJY5BRCNAs4_3_6', 'gnEE6oWoz7U_124_132', '6owu8Mow0_g_302_307', 'kWLNZzuo3do_145_151', 'gp8XjWSoP2k_0_10', 'kWLNZzuo3do_251_260', 'wSwwS_0MKEc_68_78', 'b4FM9OLBGyM_37_49', '0hyZ__3YhZc_380_384', '9HTUcMjWB3g_143_151', 'IWhrWLOAin0_1_4', '2YhDTpzxd3c_149_154', 'v5CeGLEnVFE_0_8', 'q7pOFn8s4zc_27_36', 'sJSmRik2c-c_1_7', 'fjDvKHkmxs0_119_126', 'Kms3EXosNS0_8_17', 'kWLNZzuo3do_76_80', 'ejgwQqCHN1E_7_12', '953PkxFNiko_82_85', 'EBWPZIjtnTM_1_6', 'Rf0TUCA3XNs_3_16', 'uppFvcVwqqY_5_15', 'K6xy6C-Hsa0_2_28', 'j2Dhf-xFUxU_13_20', '7HcYJKMxpcg_20_28', '9Q0JfdP36kI_23_28', 'R3a9r8qFlhQ_0_12', 'J4otiwOjQy8_98_112', 'pzq5fPfsPZg_51_57', 'N2Cm0SLr0ZE_18_29', '_sJ_09Mf1HY_49_72', 'XNV7hKVu-Xg_40_47', '6t0BpjwYKco_240_247', 'zuYZ5kPatJE_78_116', 'eyMYc-37Sk4_0_10', 'Xvy21KrDv8I_0_10', '-pUwIypksfE_13_23', 'yNy9jTeolUk_8_12', 'ao-9B8IV9_E_175_187', 'bb6V0Grtub4_174_185', 'u4kMN1jamdM_10_20', 'kWLNZzuo3do_38_42', 'veE0E79dEEc_18_45', '0hyZ__3YhZc_279_283', '3mbBbFH9fAg_21_29', 'd_BWuttLRFM_42_52', 'DKZg4kIEa0A_31_36', 'urNX3e1Wbc8_30_38', 'kEGmZDpZ_RE_248_293', '2YhDTpzxd3c_102_105', 'uGDuIyfJXXg_23_33', '04Gt01vatkk_248_265', 'a-cek0mvXxE_15_25', 'o2X6UCaNqKA_27_36', '0NPtAv3_uVI_46_54', 'yfMTsYcLt10_0_7', 'b_BuSVZwq6M_1_9', 'D1tTBncIsm8_841_848', 'BE_uxYK9uz0_150_168', 'MJoxGpEswOI_9_16', 'MW21lp833Vo_143_149', 'xy9LLUUZ6ic_50_60', 'cs33MNhpRNw_16_21', 'rw9h_574HxE_13_18', 'qypmR4O1Gwk_0_10', 'dc4UltkRJsw_53_59', 'n_Z0-giaspE_437_447', 'n2NLoLNecgI_168_178', 'e40bBP0_AbE_64_67', 'SKhmFSV-XB0_12_18', 'YRSOc193Pxc_68_73', 'xxHx6s_DbUo_41_44', 'kWLNZzuo3do_48_53', 'jvxUeT1Nlb8_100_115', 'AXL1oMdCFUM_45_59', 'IiIiY6o-C9k_59_64', 'JIoiUNOXGaI_98_104', '8jP8CC2rKj4_80_88', 'Qjbcibrv3L0_26_32', 'ApYvvQK7bBc_6_32', '-uT_1VDvXok_8_15', 'm1NR0uNNs5Y_57_64', 'C9LHk0AJI7U_196_205', 'W8_63NiTENw_23_30', 'lrZxpneS6Gk_0_12', 'DB-3-TVjrxU_58_75', 'VLBsAPZ2DDE_58_64', 'Cjf21Y19aUQ_82_86', 'BOv-2HzNPaA_7_12', 'gHyXstpe_N8_95_100', 'TZ860P4iTaM_11_20', 'RZL9irxnhZ0_34_40', 'CgWYN5e9K10_30_62', 'QTAqjSbMkXU_65_75', 'IhwPQL9dFYc_124_129', 'uxEhH6MPH28_69_85', 'qeKX-N1nKiM_52_59', '5L5MoemWC6g_10_13', 'pfQPRXcihkI_127_138', 'LacBy2NT3YQ_12_17', 'LjimLwAhAOQ_5_15', 'pGCRurvXQGM_45_56', 'OVEe1DMsOq0_43_52', 'zCf8NWJ8kzA_47_52', 'PNMsPz1701c_3_20', 'oBt257I-pL0_0_10', 'iarsmqA3dck_19_25', '6RlYLTU5gvM_18_24', 'SzEbtbNSg04_70_85', 'PeUHy0A1GF0_99_103', '0lh_UWF9ZP4_174_178', 'SzEbtbNSg04_28_48', '30GeJHYoerk_121_126', '7s0AXJtLL0Y_57_61', 'f_GnkweYzzI_35_41', 'YmXCfQm0_CA_277_284', 'aHiUM8uWxxo_17_25', 'kquB3rIgfGk_197_202', '9BScZRpF7SI_31_36', 'cFzSEIGrEfA_0_24', 'UbmZAe5u5FI_16_23', 'ulPvRmNfXK4_96_100', 'oG6LLGi_n48_51_55', 'lsanQj2yacs_80_85', '9W6aQNcU3PA_92_103', 's-dSFyz_5Ww_31_41', 'YXixZrSxefk_6_15', '8MVo7fje_oE_130_136', 'WTf5EgVY5uU_18_23', 'cJOZp2ZftCw_1_12', 'TsagxA4DKuc_28_41', '6t0BpjwYKco_142_151', 'e-j59PqJjSM_50_98', 's-dSFyz_5Ww_13_23', 'JM4913Fe-ic_4_15', 'Je3V7U5Ctj4_760_765', 'PuQVs2Ch1LY_5_15', '2YhDTpzxd3c_166_170', '0hyZ__3YhZc_388_394', 'drsPD5fI1IA_170_180', 'dZBIdRGKRhM_13_32', 'zzit5b_-ukg_5_20', 'ao-9B8IV9_E_108_119', 'xtbsD3PUua4_174_185', '53dc4z7HLyg_25_29', 'CcvJs77L-e8_15_22', 'wFPmKChNrhU_3_11', 'zulPFoY64wE_26_33', 'xOQn1z_oWDY_30_39', '08pVpBq706k_175_212', 'Lw0eGsYxElA_11_14', 'Ted3imMggC0_48_58', 'gqxpGOHUH9k_113_119', 'xGdoi3W3Z4o_2_40', '4ge5_V-xhP0_35_42', 'fcvW1vr8hAs_104_108', '4yf2WP6K1gQ_28_32', 'JnNe61UZsdM_42_51', 'EwTZ2xpQwpA_14_21', 'RWWvWFeE6_Q_2_26', '47D9-U8hn5I_11_16', '30GeJHYoerk_63_75', 'w28TljSqtW8_64_75', '5XxshEdcfAM_85_100', 'LF51fban_x4_300_325', 'UbmZAe5u5FI_132_141', 'mYzajpeAWuA_100_112', 's9TklvyLFyI_177_191', 'gHyXstpe_N8_116_125', 'z2kUc8wp9l8_40_46', 'eZLxohGP4IE_147_158', 'iTA0rWPE4nY_17_23', 'pqTWUzehCUM_42_47', 'u9prcUCHlqM_235_241', 'Puh1n8DTKw8_2_9', 'a8PAY4rvzRM_1_10', 'kquB3rIgfGk_525_532', '8BL9qmFRbd4_6_13', 'QGgJlHi3tjw_1_15', '88DOMJ11q2M_84_87', 'Y4BeJ-64294_20_30', '6Lzq1HN33lE_5_15', 'JsD6uEZsIsU_1_47', 'gCra4qOrjFw_1_17', 'umjc1CkO4JA_290_305', '_O9kWD8nuRU_45_49', 'Kxa0mnDj0bs_113_124', 'dMH0bHeiRNg_3_14', 'F44bLc6t7-I_1_8', 'SrDE9-cDz48_4_12', 'oDcd2JbOyzg_38_55', 'puineN1UMto_25_29', 't8Nf1MK7lts_0_10', 'gXVaC3gBWbc_22_30', '7OGizkf_JEo_191_194', '05gNigkqfNU_24_32', 'CGllPWAwmUo_1_15', 'PTxHJMM6hHE_0_15', 'uVPnDJKt1M0_0_6', 'm1c04pCYGxA_57_64', 'lmCrIZeob4w_23_26', 'ItFqogTmAvQ_361_365', 'XdB0pVjFYoU_1_8', 'WTf5EgVY5uU_66_75', '_OBlgSz8sSM_15_32', '5QJf-l4lPJw_133_138', 'pzq5fPfsPZg_29_33', 'N5wsl86P37Q_1_15', 'PCXHuseKwDc_17_20', 'HAjwXjwN9-A_16_24', 'aC-KOYQsIvU_25_31', 'Cxxx6wJ1jNo_0_10', 'ZL7P_XmSnEY_205_209', 'He7Ge7Sogrk_47_70', 'LYSPQqUvNO0_43_57', 'aJoDt4DMkFU_118_124', 'HkuOqWzXYVg_0_12', 'm1NR0uNNs5Y_192_198', '0lh_UWF9ZP4_138_145', '-9CUm-2cui8_39_44', '8yoUaH6wGLs_22_30', 'uy0HNWto0UY_18_25', 'M4It3DrhbE0_13_22', 'klteYv1Uv9A_27_33', 'DAmOfcvfKZ0_22_32', 'dfOuTx66bJU_11_14', 'MWzeInQaUk4_12_20', '5x_OGEdO6Z8_0_21', 'HqbVbPvlDoM_0_11', '-YI0cxuNcq8_262_272', 'J8cP93yG1Ao_14_24', 'pfQPRXcihkI_135_144', '9QI8cgBSGo8_28_41', 'WTx-K045yQM_85_98', '9VG7Elw9TDA_85_91', '6q1dX6thX3E_286_295', 'xlPyHMkpaQY_1_7', 'psXeA8sSYdI_25_30', '6GJ1DNOGDAM_223_233', 'Afmc6FeN-hM_20_24', 'jlahRlo4jlU_30_36', 'tIMDKdMnNYM_266_273', 'toE0QYZzJKE_1_8', '1dA1RQgPN3c_72_82', 'WTf5EgVY5uU_94_98', '0lh_UWF9ZP4_21_26', 'BVjvRpmHg0w_231_250', 'xxHx6s_DbUo_173_177', 'Tied-t1fFsk_15_20', '4ikH9ZRcF2Q_89_105', 'Lr8ds5iY5uk_8_15', 'UbmZAe5u5FI_88_98', 'MJNStEgDKXU_3_10', 'bmvD4HlPFxg_20_27', '3n_Kqak2JdA_48_71', 'NVGGgncVq-4_111_117', 'IhwPQL9dFYc_143_147', 'TvBsaMgqh5s_61_68', 'Goj131_GxYs_2_7', '00jrXRMlZOY_0_10', 'PHDmVhShE80_20_29', 'Ow_o78zjo14_42_57', 'klFyrnrUSck_25_36', 'WTf5EgVY5uU_33_39', 'v-9Gx0gJmfo_91_99', '3doK4FQM5-k_46_57', 'LqPIBHAmt4E_22_30', 'hW8TKz2Aea4_40_50', 'buJ5HDCinrM_150_166', 'NoCgvH4xZHo_68_74', 'HVR7KpItLfA_62_70', '-_hbPLsZvvo_5_8', 'Ya-MkA187c8_36_45', 'ao-9B8IV9_E_77_81', '-_hbPLsZvvo_18_25', 'BefkDBj5gSw_104_109', 'PBHZtoygOYg_450_465', 'CL49s8bO6Fg_1_11', '_6OTzzK7t9Y_73_78', 'N3A7944_UJw_63_70', 'wn9rDTZj-m4_8_18', 'kWLNZzuo3do_86_93', 'wzfkC2TjdeQ_0_31', 'IpHJffM3G1c_464_470', '_JVxurtGIhI_32_42', 'ri5AyXzxb4o_201_215', 'kWLNZzuo3do_262_270', 'kWLNZzuo3do_77_83', 'iwpnUHFhjWc_84_94', 'xxHx6s_DbUo_32_36', 'Xp6rxW4hFFo_32_40', 'HypgcCT1r68_38_46', 'LuQ0KiMMhoI_49_64', 'n6U-TGahwvs_100_110', 'jjl2ZMdFCsw_130_142', 'gMqKUPeTAkg_17_30', 'TPD5wO_LEPU_2_6', '7_XASfcYdBk_3_13', 'uJPupV4oLZ0_4_12', 'F2Ny7rq9RKs_139_148', 'klFyrnrUSck_42_46', 'qeKX-N1nKiM_133_142', 'c75SIlAjfjg_6_14', 'Ar70p1_LcfM_2_20', '0vmoZEaN_-o_4_12', 'hEOGZoYSvT4_82_86', 'NV6pq1W-I4g_7_16', 'hcCLIzzB1jQ_1_7', 'vfktGc_qx-w_2_18', '0lh_UWF9ZP4_148_155', 'unhiT2D6WvE_0_20', 'lGk1MA6YP-M_36_48', 'GY5Dl00LrEI_2_13', 'jmoT2we_rqo_0_5', '_O9kWD8nuRU_50_56', '4eJJF11bIxM_1_9', 'Ffv7fhL1EDY_177_184', 'auFXfsCzAvQ_1_15', 'fvBs0xpEZhQ_10_30', 'aDjNl5gs4nM_18_47', 'ao-9B8IV9_E_37_44', 'ZvJvNcukZ4w_0_10', 'yC4eEuURH8c_19_28', 'uqVCk2oDpSE_194_200', 'MY-rGamtAJc_6_22', 'WqwcWk1In_g_46_54', 'PeUHy0A1GF0_122_127', '-joBOHlg5J0_72_80', 'vulNlhUI6m0_7_27', 'UXs3eq68ZjE_382_387', 'sm0OYD0NSbQ_51_59', '30GeJHYoerk_80_85', 'zv2RIbUsnSw_335_341', 'T6lPRKRpu9w_32_39', 'EPXsiQw9vvo_1_12', 'J3_hkgu6MGc_43_52', 'qLwgb3F0aPU_298_305', 'omIPdpxg--4_39_46', 'UgUFP5baQ9Y_0_10', '1hPxGmTGarM_9_19', 'r0E-0ntoNWo_20_30', 'k4hhWYtaQ14_0_10', 'xxHx6s_DbUo_162_165', 'NVGGgncVq-4_12_20', 'kWLNZzuo3do_56_62', 'WWf0Z6ak3Dg_5_15', 'B7mP4zoDMwM_0_17', 'k5OKBX2e7xA_19_32', 'R1UrFBF_0iU_35_41', 'qeKX-N1nKiM_106_115'}\n",
            "[train] All entries in JSON 'videos' section have matching video files in folder.\n",
            "[train] Videos present in folder but missing captions in JSON 'sentences' section: {'vZa13vJugGU_0_30', 'O_NWtDShLeg_21_25', 'BApIQn69EVE_10_16', '_WRC7HXBJpU_395_401', 'emblM4a76jg_5_15', 'VahnQw2gTQY_315_320', 'IHIa75B9AhI_2_26', '7NNg0_n-bS8_21_30', '9Q0JfdP36kI_167_170', 'ItFqogTmAvQ_389_395', 'lb8J2zCQTlo_3_8', '0SMOK2ql7Pg_5_12', 'ub-aYLzCF_Q_1_10', 'IhwPQL9dFYc_78_88', 'DKgHYLDebx0_0_8', '9IrWyZ0KZuk_160_166', 'EiylMb_mWk4_2_20', 'jjl2ZMdFCsw_17_35', 'm1NR0uNNs5Y_160_166', '6t0BpjwYKco_118_127', 'NMlKMfiHSho_1_15', 'R8FDJgVW3Vc_0_4', 'bLqmf8x7rLI_2_8', 'JK1R9k1WDpc_6_15', 'RMznbCn5sQs_0_10', 'De815YpTBic_41_48', 'ACOmKiJDkA4_121_128', 'UgUFP5baQ9Y_0_7', 'z0Si1XxMibg_0_30', 'zbAk0gX7kas_16_24', 'akXjIEoecNs_2_12', 'W6_XuNhgtrM_2_5', 'q5ZRMvjzhXQ_15_29', 'ywHBKayhyvQ_19_28', '9OvXb1fot74_21_29', 'PeUHy0A1GF0_68_73', 'B4foOe9kUgY_0_8', '4z3b4mnw5y4_56_60', 'sWqi41wyXcQ_68_79', 'cwkjJrGpoaU_30_41', 'HM-ZDoRWiH4_0_5', 'kWLNZzuo3do_217_222', 'fF89MasBFLw_321_326', '8MVo7fje_oE_125_130', 'jxdubZzQrio_33_46', '-zOrV-5vh1A_69_76', 'f_CvW22Eauc_16_23', 'hksxtbcS780_7_14', 'vE1gvaM3iAs_39_46', 'ZYaxg5NZW_g_21_27', 'UnWgz-mVMXU_2_6', 'r4qv_BNlQNk_93_98', 'ZsrPKtx0qyg_7_15', '4D1o4FC8YbA_207_216', 'tZmDWltBziM_42_47', 'yAD_TS5L2d4_4_11', 'itxzpFW1z7E_22_36', 'nPj6EcVGoUY_7_15', 'IiTzJQE67FA_36_46', '6t0BpjwYKco_261_284', 'AKVBK-0veE8_5_12', 'jI58q6rcNLc_8_18', '4_WZN7uW0NY_140_145', 'kJY5BRCNAs4_3_6', 'gnEE6oWoz7U_124_132', '6owu8Mow0_g_302_307', 'kWLNZzuo3do_145_151', 'gp8XjWSoP2k_0_10', 'kWLNZzuo3do_251_260', 'wSwwS_0MKEc_68_78', 'b4FM9OLBGyM_37_49', '0hyZ__3YhZc_380_384', '9HTUcMjWB3g_143_151', 'IWhrWLOAin0_1_4', '2YhDTpzxd3c_149_154', 'v5CeGLEnVFE_0_8', 'q7pOFn8s4zc_27_36', 'sJSmRik2c-c_1_7', 'fjDvKHkmxs0_119_126', 'Kms3EXosNS0_8_17', 'kWLNZzuo3do_76_80', 'ejgwQqCHN1E_7_12', '953PkxFNiko_82_85', 'EBWPZIjtnTM_1_6', 'Rf0TUCA3XNs_3_16', 'uppFvcVwqqY_5_15', 'K6xy6C-Hsa0_2_28', 'j2Dhf-xFUxU_13_20', '7HcYJKMxpcg_20_28', '9Q0JfdP36kI_23_28', 'R3a9r8qFlhQ_0_12', 'J4otiwOjQy8_98_112', 'pzq5fPfsPZg_51_57', 'N2Cm0SLr0ZE_18_29', '_sJ_09Mf1HY_49_72', 'XNV7hKVu-Xg_40_47', '6t0BpjwYKco_240_247', 'zuYZ5kPatJE_78_116', 'eyMYc-37Sk4_0_10', 'Xvy21KrDv8I_0_10', '-pUwIypksfE_13_23', 'yNy9jTeolUk_8_12', 'ao-9B8IV9_E_175_187', 'bb6V0Grtub4_174_185', 'u4kMN1jamdM_10_20', 'kWLNZzuo3do_38_42', 'veE0E79dEEc_18_45', '0hyZ__3YhZc_279_283', '3mbBbFH9fAg_21_29', 'd_BWuttLRFM_42_52', 'DKZg4kIEa0A_31_36', 'urNX3e1Wbc8_30_38', 'kEGmZDpZ_RE_248_293', '2YhDTpzxd3c_102_105', 'uGDuIyfJXXg_23_33', '04Gt01vatkk_248_265', 'a-cek0mvXxE_15_25', 'o2X6UCaNqKA_27_36', '0NPtAv3_uVI_46_54', 'yfMTsYcLt10_0_7', 'b_BuSVZwq6M_1_9', 'D1tTBncIsm8_841_848', 'BE_uxYK9uz0_150_168', 'MJoxGpEswOI_9_16', 'MW21lp833Vo_143_149', 'xy9LLUUZ6ic_50_60', 'cs33MNhpRNw_16_21', 'rw9h_574HxE_13_18', 'qypmR4O1Gwk_0_10', 'dc4UltkRJsw_53_59', 'n_Z0-giaspE_437_447', 'n2NLoLNecgI_168_178', 'e40bBP0_AbE_64_67', 'SKhmFSV-XB0_12_18', 'YRSOc193Pxc_68_73', 'xxHx6s_DbUo_41_44', 'kWLNZzuo3do_48_53', 'jvxUeT1Nlb8_100_115', 'AXL1oMdCFUM_45_59', 'IiIiY6o-C9k_59_64', 'JIoiUNOXGaI_98_104', '8jP8CC2rKj4_80_88', 'Qjbcibrv3L0_26_32', 'ApYvvQK7bBc_6_32', '-uT_1VDvXok_8_15', 'm1NR0uNNs5Y_57_64', 'C9LHk0AJI7U_196_205', 'W8_63NiTENw_23_30', 'lrZxpneS6Gk_0_12', 'DB-3-TVjrxU_58_75', 'VLBsAPZ2DDE_58_64', 'Cjf21Y19aUQ_82_86', 'BOv-2HzNPaA_7_12', 'gHyXstpe_N8_95_100', 'TZ860P4iTaM_11_20', 'RZL9irxnhZ0_34_40', 'CgWYN5e9K10_30_62', 'QTAqjSbMkXU_65_75', 'IhwPQL9dFYc_124_129', 'uxEhH6MPH28_69_85', 'qeKX-N1nKiM_52_59', '5L5MoemWC6g_10_13', 'pfQPRXcihkI_127_138', 'LacBy2NT3YQ_12_17', 'LjimLwAhAOQ_5_15', 'pGCRurvXQGM_45_56', 'OVEe1DMsOq0_43_52', 'zCf8NWJ8kzA_47_52', 'PNMsPz1701c_3_20', 'oBt257I-pL0_0_10', 'iarsmqA3dck_19_25', '6RlYLTU5gvM_18_24', 'SzEbtbNSg04_70_85', 'PeUHy0A1GF0_99_103', '0lh_UWF9ZP4_174_178', 'SzEbtbNSg04_28_48', '30GeJHYoerk_121_126', '7s0AXJtLL0Y_57_61', 'f_GnkweYzzI_35_41', 'YmXCfQm0_CA_277_284', 'aHiUM8uWxxo_17_25', 'kquB3rIgfGk_197_202', '9BScZRpF7SI_31_36', 'cFzSEIGrEfA_0_24', 'UbmZAe5u5FI_16_23', 'ulPvRmNfXK4_96_100', 'oG6LLGi_n48_51_55', 'lsanQj2yacs_80_85', '9W6aQNcU3PA_92_103', 's-dSFyz_5Ww_31_41', 'YXixZrSxefk_6_15', '8MVo7fje_oE_130_136', 'WTf5EgVY5uU_18_23', 'cJOZp2ZftCw_1_12', 'TsagxA4DKuc_28_41', '6t0BpjwYKco_142_151', 'e-j59PqJjSM_50_98', 's-dSFyz_5Ww_13_23', 'JM4913Fe-ic_4_15', 'Je3V7U5Ctj4_760_765', 'PuQVs2Ch1LY_5_15', '2YhDTpzxd3c_166_170', '0hyZ__3YhZc_388_394', 'drsPD5fI1IA_170_180', 'dZBIdRGKRhM_13_32', 'zzit5b_-ukg_5_20', 'ao-9B8IV9_E_108_119', 'xtbsD3PUua4_174_185', '53dc4z7HLyg_25_29', 'CcvJs77L-e8_15_22', 'wFPmKChNrhU_3_11', 'zulPFoY64wE_26_33', 'xOQn1z_oWDY_30_39', '08pVpBq706k_175_212', 'Lw0eGsYxElA_11_14', 'Ted3imMggC0_48_58', 'gqxpGOHUH9k_113_119', 'xGdoi3W3Z4o_2_40', '4ge5_V-xhP0_35_42', 'fcvW1vr8hAs_104_108', '4yf2WP6K1gQ_28_32', 'JnNe61UZsdM_42_51', 'EwTZ2xpQwpA_14_21', 'RWWvWFeE6_Q_2_26', '47D9-U8hn5I_11_16', '30GeJHYoerk_63_75', 'w28TljSqtW8_64_75', '5XxshEdcfAM_85_100', 'LF51fban_x4_300_325', 'UbmZAe5u5FI_132_141', 'mYzajpeAWuA_100_112', 's9TklvyLFyI_177_191', 'gHyXstpe_N8_116_125', 'z2kUc8wp9l8_40_46', 'eZLxohGP4IE_147_158', 'iTA0rWPE4nY_17_23', 'pqTWUzehCUM_42_47', 'u9prcUCHlqM_235_241', 'Puh1n8DTKw8_2_9', 'a8PAY4rvzRM_1_10', 'kquB3rIgfGk_525_532', '8BL9qmFRbd4_6_13', 'QGgJlHi3tjw_1_15', '88DOMJ11q2M_84_87', 'Y4BeJ-64294_20_30', '6Lzq1HN33lE_5_15', 'JsD6uEZsIsU_1_47', 'gCra4qOrjFw_1_17', 'umjc1CkO4JA_290_305', '_O9kWD8nuRU_45_49', 'Kxa0mnDj0bs_113_124', 'dMH0bHeiRNg_3_14', 'F44bLc6t7-I_1_8', 'SrDE9-cDz48_4_12', 'oDcd2JbOyzg_38_55', 'puineN1UMto_25_29', 't8Nf1MK7lts_0_10', 'gXVaC3gBWbc_22_30', '7OGizkf_JEo_191_194', '05gNigkqfNU_24_32', 'CGllPWAwmUo_1_15', 'PTxHJMM6hHE_0_15', 'uVPnDJKt1M0_0_6', 'm1c04pCYGxA_57_64', 'lmCrIZeob4w_23_26', 'ItFqogTmAvQ_361_365', 'XdB0pVjFYoU_1_8', 'WTf5EgVY5uU_66_75', '_OBlgSz8sSM_15_32', '5QJf-l4lPJw_133_138', 'pzq5fPfsPZg_29_33', 'N5wsl86P37Q_1_15', 'PCXHuseKwDc_17_20', 'HAjwXjwN9-A_16_24', 'aC-KOYQsIvU_25_31', 'Cxxx6wJ1jNo_0_10', 'ZL7P_XmSnEY_205_209', 'He7Ge7Sogrk_47_70', 'LYSPQqUvNO0_43_57', 'aJoDt4DMkFU_118_124', 'HkuOqWzXYVg_0_12', 'm1NR0uNNs5Y_192_198', '0lh_UWF9ZP4_138_145', '-9CUm-2cui8_39_44', '8yoUaH6wGLs_22_30', 'uy0HNWto0UY_18_25', 'M4It3DrhbE0_13_22', 'klteYv1Uv9A_27_33', 'DAmOfcvfKZ0_22_32', 'dfOuTx66bJU_11_14', 'MWzeInQaUk4_12_20', '5x_OGEdO6Z8_0_21', 'HqbVbPvlDoM_0_11', '-YI0cxuNcq8_262_272', 'J8cP93yG1Ao_14_24', 'pfQPRXcihkI_135_144', '9QI8cgBSGo8_28_41', 'WTx-K045yQM_85_98', '9VG7Elw9TDA_85_91', '6q1dX6thX3E_286_295', 'xlPyHMkpaQY_1_7', 'psXeA8sSYdI_25_30', '6GJ1DNOGDAM_223_233', 'Afmc6FeN-hM_20_24', 'jlahRlo4jlU_30_36', 'tIMDKdMnNYM_266_273', 'toE0QYZzJKE_1_8', '1dA1RQgPN3c_72_82', 'WTf5EgVY5uU_94_98', '0lh_UWF9ZP4_21_26', 'BVjvRpmHg0w_231_250', 'xxHx6s_DbUo_173_177', 'Tied-t1fFsk_15_20', '4ikH9ZRcF2Q_89_105', 'Lr8ds5iY5uk_8_15', 'UbmZAe5u5FI_88_98', 'MJNStEgDKXU_3_10', 'bmvD4HlPFxg_20_27', '3n_Kqak2JdA_48_71', 'NVGGgncVq-4_111_117', 'IhwPQL9dFYc_143_147', 'TvBsaMgqh5s_61_68', 'Goj131_GxYs_2_7', '00jrXRMlZOY_0_10', 'PHDmVhShE80_20_29', 'Ow_o78zjo14_42_57', 'klFyrnrUSck_25_36', 'WTf5EgVY5uU_33_39', 'v-9Gx0gJmfo_91_99', '3doK4FQM5-k_46_57', 'LqPIBHAmt4E_22_30', 'hW8TKz2Aea4_40_50', 'buJ5HDCinrM_150_166', 'NoCgvH4xZHo_68_74', 'HVR7KpItLfA_62_70', '-_hbPLsZvvo_5_8', 'Ya-MkA187c8_36_45', 'ao-9B8IV9_E_77_81', '-_hbPLsZvvo_18_25', 'BefkDBj5gSw_104_109', 'PBHZtoygOYg_450_465', 'CL49s8bO6Fg_1_11', '_6OTzzK7t9Y_73_78', 'N3A7944_UJw_63_70', 'wn9rDTZj-m4_8_18', 'kWLNZzuo3do_86_93', 'wzfkC2TjdeQ_0_31', 'IpHJffM3G1c_464_470', '_JVxurtGIhI_32_42', 'ri5AyXzxb4o_201_215', 'kWLNZzuo3do_262_270', 'kWLNZzuo3do_77_83', 'iwpnUHFhjWc_84_94', 'xxHx6s_DbUo_32_36', 'Xp6rxW4hFFo_32_40', 'HypgcCT1r68_38_46', 'LuQ0KiMMhoI_49_64', 'n6U-TGahwvs_100_110', 'jjl2ZMdFCsw_130_142', 'gMqKUPeTAkg_17_30', 'TPD5wO_LEPU_2_6', '7_XASfcYdBk_3_13', 'uJPupV4oLZ0_4_12', 'F2Ny7rq9RKs_139_148', 'klFyrnrUSck_42_46', 'qeKX-N1nKiM_133_142', 'c75SIlAjfjg_6_14', 'Ar70p1_LcfM_2_20', '0vmoZEaN_-o_4_12', 'hEOGZoYSvT4_82_86', 'NV6pq1W-I4g_7_16', 'hcCLIzzB1jQ_1_7', 'vfktGc_qx-w_2_18', '0lh_UWF9ZP4_148_155', 'unhiT2D6WvE_0_20', 'lGk1MA6YP-M_36_48', 'GY5Dl00LrEI_2_13', 'jmoT2we_rqo_0_5', '_O9kWD8nuRU_50_56', '4eJJF11bIxM_1_9', 'Ffv7fhL1EDY_177_184', 'auFXfsCzAvQ_1_15', 'fvBs0xpEZhQ_10_30', 'aDjNl5gs4nM_18_47', 'ao-9B8IV9_E_37_44', 'ZvJvNcukZ4w_0_10', 'yC4eEuURH8c_19_28', 'uqVCk2oDpSE_194_200', 'MY-rGamtAJc_6_22', 'WqwcWk1In_g_46_54', 'PeUHy0A1GF0_122_127', '-joBOHlg5J0_72_80', 'vulNlhUI6m0_7_27', 'UXs3eq68ZjE_382_387', 'sm0OYD0NSbQ_51_59', '30GeJHYoerk_80_85', 'zv2RIbUsnSw_335_341', 'T6lPRKRpu9w_32_39', 'EPXsiQw9vvo_1_12', 'J3_hkgu6MGc_43_52', 'qLwgb3F0aPU_298_305', 'omIPdpxg--4_39_46', 'UgUFP5baQ9Y_0_10', '1hPxGmTGarM_9_19', 'r0E-0ntoNWo_20_30', 'k4hhWYtaQ14_0_10', 'xxHx6s_DbUo_162_165', 'NVGGgncVq-4_12_20', 'kWLNZzuo3do_56_62', 'WWf0Z6ak3Dg_5_15', 'B7mP4zoDMwM_0_17', 'k5OKBX2e7xA_19_32', 'R1UrFBF_0iU_35_41', 'qeKX-N1nKiM_106_115'}\n",
            "[train] All captions in JSON 'sentences' section have matching video files in folder.\n",
            "[val] Videos present in folder but missing in JSON 'videos' section: {'E61HNXjgyqA_22_32', 'GnwKcpfr_ng_10_20', 'VuCFgiWfVF4_0_6', 'q3I3R_gqy8M_73_79', '0hyZ__3YhZc_289_295', 'iUYWdCxvJCI_2_14', 'lfGlDg47How_361_367', 'UxhKb-zZoWE_0_15', 'FavUpD_IjVY_22_26', 'ScdUht-pM6s_53_63', '-_hbPLsZvvo_323_328', 'ge7OOILJA6U_20_25', 'pGsU4FekJQM_10_18', '4PcL6-mjRNk_47_55', 'Ud3Qy8m6OxQ_52_62', 'eTnlw7v8ea0_36_50', 'NjCqtzZ3OtU_303_307', 'c53HKs39i28_26_35', 'kWLNZzuo3do_167_181', 'qvg9eM4Hmzk_4_10', '0hyZ__3YhZc_364_370', '9wxB1UQi0MM_18_38', 'xEDCfcMZlZY_91_100', '8NFoN0K3bPg_20_28', 'swJ0zhVJ8DU_15_21', 'Je3V7U5Ctj4_634_639', '-wa0umYJVGg_139_157', 'ITNh8tWZPOA_5_25', 'ZdlG8fjGJlo_78_87', 'UXs3eq68ZjE_320_325', 'rw9h_574HxE_251_254', 'IhwPQL9dFYc_61_75', 'kZfBt5me3Pg_2_7', 'vz71JKcpeUU_0_10', 'pdrBPJYfTC8_33_39', 'gqSOvUH_njE_151_155', 'MrQd1zUVRUM_103_110', 'xxHx6s_DbUo_202_207', 'fw8qvK67jYY_50_97', 'EpMuCrbxE8A_107_115', 'p9g06ktIkJg_4_11', 'sT5Bzt9w354_11_34', '1dfR0A_BXjw_441_448', 'nb12bAaKzvA_0_10', '0piwbTivpDg_10_40', 'ACOmKiJDkA4_175_182', 'mJ9eRvxjLc4_0_16', 'rnawC5C8gSI_82_90', 'jfrrO5K_vKM_55_65', 'xPR0xFgCAZY_17_27', '88p7U5CnoYU_1_10', '5P6UU6m3cqk_57_75', 'jsEUFYhiqxU_121_128', 'x_7BrCQdVUs_172_177', '16iNk1hLJt4_37_40', 'W8l_ezoU8Lc_156_162', 'aeA-HN7BMdo_52_58', '5JLi7L1Kwp8_23_30', 'UbmZAe5u5FI_26_33', 'ao-9B8IV9_E_217_221', 'nTasT5h0LEg_12_14', 'ceOXCFUmxzA_100_110', 'WinaH9SkW9c_11_38', 'Gn4Iv5ARIXc_83_93', 'QGJy1K91gP4_90_100', 'WyIqGyj21Dk_0_10', 'xxHx6s_DbUo_158_161', 'dJ3ba9zwx6c_5_15', 'j2Dhf-xFUxU_20_29', 'WTf5EgVY5uU_15_19', '3eqFFRSXwGE_84_96', 'UDwvax7K57c_28_35', 'MPuKVz3sjBA_1_12', '5W02895vT8c_312_322', 'kEGmZDpZ_RE_352_370', '21uB8g-jDZI_70_79', 'QjI3Y8ZKSqs_30_38', 'mfJjIOfj6D8_3_10', '8PQiaurIiDM_94_99', '4kQnrKvOTNg_15_24', '2YhDTpzxd3c_174_180', 'FWzsXeXCwuc_106_110', 'Kxa0mnDj0bs_15_20', 'pW9DFPqoIsI_26_50', 'labytsb3gfI_146_154', 'k9Brw_0gncU_14_33', 'ngHDYzhDBk4_24_29', 'rq2p5ML8-WI_63_69', 'i2GgBwlwV0c_24_31', 'hM3jzlyNIpc_0_10', 'DIebwNHGjm8_27_38', 'bSIjZ75a50s_286_304', 'L9wD3kw-8FE_65_73', 'l5JJ2n2ggiQ_319_327', 'OCcy9TDRGKo_118_127', '-_hbPLsZvvo_19_26', 'Ms3QdGIzltU_1_16', 'CulG2SMC7DU_14_25', 'cnsjm3fNEec_4_10', 'ACK7SekJWts_140_150', '6gQu8PWhFoQ_37_41', 'Je3V7U5Ctj4_997_1004', 'nq4hG6qgyPs_184_195', '0k1Ak8aTMVI_4_12', 'fIaLVw_Gc_w_99_109', 'PHDmVhShE80_82_86', 'i2sRHf9m5KM_28_42', '1qU-kF-km1Q_6_16', 'PeUHy0A1GF0_74_77', 'ePujnD4qJO0_62_77', 'aeA-HN7BMdo_34_41', '1dfR0A_BXjw_524_532', '9Bxknsvy3jQ_53_58', 'za-9mBZyNfQ_330_336', '81fABEiwcIM_31_45', 'kWLNZzuo3do_222_227', 'q-a6NEotUX8_5_10', 'ViWQUOGIaSU_9_16', 'sFExO_PW22s_10_18', 'Je3V7U5Ctj4_488_495', '8PQiaurIiDM_173_180', 'Z10_CDPQKKs_1_6', 'NaX51emUDn4_0_20', 'MMnnqzOoMF0_68_72', 'u4T76jsPin0_0_11', 'Xh6rdpJNaf0_0_15', '3zrVqwV29V8_69_76', 'hNPZmTlY_3Q_0_8', 'TxzxPyfsSxY_0_10', 'z_qf7tOnHVg_81_89', '3lBHYoI5V8s_30_45', 'PHDmVhShE80_36_46', 'F3pmw2ga244_59_65', 'n_Z0-giaspE_379_387', 'QzulzuWr0Zw_20_45', 'X98fLZHIzWQ_15_20', '4xVGpDmA4lE_23_33', '0lh_UWF9ZP4_103_110', 'UXs3eq68ZjE_209_214', 'z9qbQX4J_2g_0_14', 'nq4hG6qgyPs_240_251', 'fnpp8v9NbmY_181_188', 'onW5hJXnI5s_10_16', 'jdAbpLooDgM_10_15', 'xpOYXbMDDBY_1_20', 'KPPCwmU5OHQ_258_266', 'aM-RcQj0a7I_37_55', '6KS8R3RxrV8_0_23', 'LwvyrDxM2G0_0_11', '4UOVKok7j1U_1_8', '2mUMTFnQWaw_1_9', 'FeTaKOPVaJg_60_70', 'P4IjNV3lZkQ_136_144', '-Cv5LsqKUXc_71_76', 'OIg10Bl7urs_0_10', 'kWLNZzuo3do_206_213', 'gjVBEJGHrXk_26_38', '-_aaMGK6GGw_57_61', 'lKADopH3qFY_0_10', '-t-ZWaJeH-o_0_15', 'WPG-BIWOrG4_635_640', 'BgoOihBb78w_38_40', 'klFyrnrUSck_79_85', 'f-24IxG9ijw_25_40', '-wa0umYJVGg_286_290', 'ItFqogTmAvQ_281_286', 'YUlG_PKzpvc_0_8', 'j4dMnAPZu70_11_18', 'WEJfT-oB4v4_20_35', 'FA3OfhJK0mI_195_203', 'UI3Cbj9fbxQ_2_23', 'jTnrm338_KY_34_42', 'y8SDRc4IOjs_94_99', 'Rq_VfjGH7kg_104_110', '_txL575S_OA_13_23', '_xf24TYgbuY_14_20', 'Zqmx-nXhLAg_0_10', 'nZSFn51l3hc_660_666', 'fd7Ky1lEPT8_40_50', '4s_0-zIWEog_1_15', 'jLgmCY1fEE8_16_26', 'X_NLV2KCnIE_60_70', '_SNE2MYAotU_41_49', 'FXWdmIHXS_U_3_10', 'pQYEZTwSVbQ_12_18', '2YhDTpzxd3c_240_245', '0hyZ__3YhZc_575_580', 'WV9K6MbLjHA_106_116', 'GopEjCCu2jo_2_9', '6t0BpjwYKco_53_59', 'pcjuCotJYj8_50_62', '0hyZ__3YhZc_598_603', 'xxHx6s_DbUo_49_56', 'Jj4uYHr65H8_28_35', 'Ce7equ9zCxk_4_19', '_KMoC6ZdeXs_10_16', '4Hhdr1IPOGs_1_10', 'o_mWZWcm2r4_47_54', 'gbW9f8xydks_0_10', 'qeKX-N1nKiM_74_77', '-bjOB4zS0uE_100_105', 'HzYtvOYOEoU_21_32', '4R-tumAGrtU_12_22', 'Pr5LQq42l4M_12_17', 'lR8RrUBhCQg_5_15', '98Alrg4pFXs_148_153', 'KrBeBabazDU_15_20', 'gIvetX_oXeI_85_90', 'MHIVmf42Ao4_31_36', 'ecm9gf2Pgkc_1_24', 'rOic25PnIx8_1_3', 'yyxtyCaEVqk_250_264', '9LSuyLyuUiM_3_6', '0lh_UWF9ZP4_50_60', 'Hd-NeIhbYGc_43_48', '4QdX5Q3qQsQ_38_44', '_O9kWD8nuRU_77_81', 'K1ZM0LSLVw8_481_487', 'lB1UPJ4leqs_0_6', 'wNv74rvkAw8_30_40', '2KrdBUFeFtY_22_27', 'c51L6ZxZGjQ_137_154', 'IiTzJQE67FA_74_86', 'IhwPQL9dFYc_171_175', '4QdX5Q3qQsQ_6_14', '0GXq1An3yHI_22_35', 'ybVb3t560oY_0_9', 'GcfWD62sbcs_13_33', 'po2tcrG6KzM_2_8', 'eZLxohGP4IE_15_25', 'ACOmKiJDkA4_75_81', 'WZTGqvbqOFE_28_34', 'KPPCwmU5OHQ_467_472', 'Q6HuQEIJqcA_9_16', 'BE_uxYK9uz0_49_56', 'p6T3XrnYtFk_4_13', 'FoL7aWvpvNs_199_204', 'pRpeEdMmmQ0_65_70', 'ID2xXlUENG0_19_24', 'xBW_uR3kGr4_42_55', '0lh_UWF9ZP4_157_160', 'Sq7Vt6KC9Yg_17_35', 'ecVwxlXc1PQ_0_12', 'NZWiKMZS_k8_39_48', 'zkTn5Ef1Oig_71_75', 's20OlIRK340_147_174', '4QYPZM01xFE_14_26', 'hJFBXHtxKIc_317_322', '5K7Ru5s6YIw_8_11', 'hPyU5KjpWVc_0_35', 'SFFfhAUEoj0_10_16'}\n",
            "[val] All entries in JSON 'videos' section have matching video files in folder.\n",
            "[val] Videos present in folder but missing captions in JSON 'sentences' section: {'E61HNXjgyqA_22_32', 'GnwKcpfr_ng_10_20', 'VuCFgiWfVF4_0_6', 'q3I3R_gqy8M_73_79', '0hyZ__3YhZc_289_295', 'iUYWdCxvJCI_2_14', 'lfGlDg47How_361_367', 'UxhKb-zZoWE_0_15', 'FavUpD_IjVY_22_26', 'ScdUht-pM6s_53_63', '-_hbPLsZvvo_323_328', 'ge7OOILJA6U_20_25', 'pGsU4FekJQM_10_18', '4PcL6-mjRNk_47_55', 'Ud3Qy8m6OxQ_52_62', 'eTnlw7v8ea0_36_50', 'NjCqtzZ3OtU_303_307', 'c53HKs39i28_26_35', 'kWLNZzuo3do_167_181', 'qvg9eM4Hmzk_4_10', '0hyZ__3YhZc_364_370', '9wxB1UQi0MM_18_38', 'xEDCfcMZlZY_91_100', '8NFoN0K3bPg_20_28', 'swJ0zhVJ8DU_15_21', 'Je3V7U5Ctj4_634_639', '-wa0umYJVGg_139_157', 'ITNh8tWZPOA_5_25', 'ZdlG8fjGJlo_78_87', 'UXs3eq68ZjE_320_325', 'rw9h_574HxE_251_254', 'IhwPQL9dFYc_61_75', 'kZfBt5me3Pg_2_7', 'vz71JKcpeUU_0_10', 'pdrBPJYfTC8_33_39', 'gqSOvUH_njE_151_155', 'MrQd1zUVRUM_103_110', 'xxHx6s_DbUo_202_207', 'fw8qvK67jYY_50_97', 'EpMuCrbxE8A_107_115', 'p9g06ktIkJg_4_11', 'sT5Bzt9w354_11_34', '1dfR0A_BXjw_441_448', 'nb12bAaKzvA_0_10', '0piwbTivpDg_10_40', 'ACOmKiJDkA4_175_182', 'mJ9eRvxjLc4_0_16', 'rnawC5C8gSI_82_90', 'jfrrO5K_vKM_55_65', 'xPR0xFgCAZY_17_27', '88p7U5CnoYU_1_10', '5P6UU6m3cqk_57_75', 'jsEUFYhiqxU_121_128', 'x_7BrCQdVUs_172_177', '16iNk1hLJt4_37_40', 'W8l_ezoU8Lc_156_162', 'aeA-HN7BMdo_52_58', '5JLi7L1Kwp8_23_30', 'UbmZAe5u5FI_26_33', 'ao-9B8IV9_E_217_221', 'nTasT5h0LEg_12_14', 'ceOXCFUmxzA_100_110', 'WinaH9SkW9c_11_38', 'Gn4Iv5ARIXc_83_93', 'QGJy1K91gP4_90_100', 'WyIqGyj21Dk_0_10', 'xxHx6s_DbUo_158_161', 'dJ3ba9zwx6c_5_15', 'j2Dhf-xFUxU_20_29', 'WTf5EgVY5uU_15_19', '3eqFFRSXwGE_84_96', 'UDwvax7K57c_28_35', 'MPuKVz3sjBA_1_12', '5W02895vT8c_312_322', 'kEGmZDpZ_RE_352_370', '21uB8g-jDZI_70_79', 'QjI3Y8ZKSqs_30_38', 'mfJjIOfj6D8_3_10', '8PQiaurIiDM_94_99', '4kQnrKvOTNg_15_24', '2YhDTpzxd3c_174_180', 'FWzsXeXCwuc_106_110', 'Kxa0mnDj0bs_15_20', 'pW9DFPqoIsI_26_50', 'labytsb3gfI_146_154', 'k9Brw_0gncU_14_33', 'ngHDYzhDBk4_24_29', 'rq2p5ML8-WI_63_69', 'i2GgBwlwV0c_24_31', 'hM3jzlyNIpc_0_10', 'DIebwNHGjm8_27_38', 'bSIjZ75a50s_286_304', 'L9wD3kw-8FE_65_73', 'l5JJ2n2ggiQ_319_327', 'OCcy9TDRGKo_118_127', '-_hbPLsZvvo_19_26', 'Ms3QdGIzltU_1_16', 'CulG2SMC7DU_14_25', 'cnsjm3fNEec_4_10', 'ACK7SekJWts_140_150', '6gQu8PWhFoQ_37_41', 'Je3V7U5Ctj4_997_1004', 'nq4hG6qgyPs_184_195', '0k1Ak8aTMVI_4_12', 'fIaLVw_Gc_w_99_109', 'PHDmVhShE80_82_86', 'i2sRHf9m5KM_28_42', '1qU-kF-km1Q_6_16', 'PeUHy0A1GF0_74_77', 'ePujnD4qJO0_62_77', 'aeA-HN7BMdo_34_41', '1dfR0A_BXjw_524_532', '9Bxknsvy3jQ_53_58', 'za-9mBZyNfQ_330_336', '81fABEiwcIM_31_45', 'kWLNZzuo3do_222_227', 'q-a6NEotUX8_5_10', 'ViWQUOGIaSU_9_16', 'sFExO_PW22s_10_18', 'Je3V7U5Ctj4_488_495', '8PQiaurIiDM_173_180', 'Z10_CDPQKKs_1_6', 'NaX51emUDn4_0_20', 'MMnnqzOoMF0_68_72', 'u4T76jsPin0_0_11', 'Xh6rdpJNaf0_0_15', '3zrVqwV29V8_69_76', 'hNPZmTlY_3Q_0_8', 'TxzxPyfsSxY_0_10', 'z_qf7tOnHVg_81_89', '3lBHYoI5V8s_30_45', 'PHDmVhShE80_36_46', 'F3pmw2ga244_59_65', 'n_Z0-giaspE_379_387', 'QzulzuWr0Zw_20_45', 'X98fLZHIzWQ_15_20', '4xVGpDmA4lE_23_33', '0lh_UWF9ZP4_103_110', 'UXs3eq68ZjE_209_214', 'z9qbQX4J_2g_0_14', 'nq4hG6qgyPs_240_251', 'fnpp8v9NbmY_181_188', 'onW5hJXnI5s_10_16', 'jdAbpLooDgM_10_15', 'xpOYXbMDDBY_1_20', 'KPPCwmU5OHQ_258_266', 'aM-RcQj0a7I_37_55', '6KS8R3RxrV8_0_23', 'LwvyrDxM2G0_0_11', '4UOVKok7j1U_1_8', '2mUMTFnQWaw_1_9', 'FeTaKOPVaJg_60_70', 'P4IjNV3lZkQ_136_144', '-Cv5LsqKUXc_71_76', 'OIg10Bl7urs_0_10', 'kWLNZzuo3do_206_213', 'gjVBEJGHrXk_26_38', '-_aaMGK6GGw_57_61', 'lKADopH3qFY_0_10', '-t-ZWaJeH-o_0_15', 'WPG-BIWOrG4_635_640', 'BgoOihBb78w_38_40', 'klFyrnrUSck_79_85', 'f-24IxG9ijw_25_40', '-wa0umYJVGg_286_290', 'ItFqogTmAvQ_281_286', 'YUlG_PKzpvc_0_8', 'j4dMnAPZu70_11_18', 'WEJfT-oB4v4_20_35', 'FA3OfhJK0mI_195_203', 'UI3Cbj9fbxQ_2_23', 'jTnrm338_KY_34_42', 'y8SDRc4IOjs_94_99', 'Rq_VfjGH7kg_104_110', '_txL575S_OA_13_23', '_xf24TYgbuY_14_20', 'Zqmx-nXhLAg_0_10', 'nZSFn51l3hc_660_666', 'fd7Ky1lEPT8_40_50', '4s_0-zIWEog_1_15', 'jLgmCY1fEE8_16_26', 'X_NLV2KCnIE_60_70', '_SNE2MYAotU_41_49', 'FXWdmIHXS_U_3_10', 'pQYEZTwSVbQ_12_18', '2YhDTpzxd3c_240_245', '0hyZ__3YhZc_575_580', 'WV9K6MbLjHA_106_116', 'GopEjCCu2jo_2_9', '6t0BpjwYKco_53_59', 'pcjuCotJYj8_50_62', '0hyZ__3YhZc_598_603', 'xxHx6s_DbUo_49_56', 'Jj4uYHr65H8_28_35', 'Ce7equ9zCxk_4_19', '_KMoC6ZdeXs_10_16', '4Hhdr1IPOGs_1_10', 'o_mWZWcm2r4_47_54', 'gbW9f8xydks_0_10', 'qeKX-N1nKiM_74_77', '-bjOB4zS0uE_100_105', 'HzYtvOYOEoU_21_32', '4R-tumAGrtU_12_22', 'Pr5LQq42l4M_12_17', 'lR8RrUBhCQg_5_15', '98Alrg4pFXs_148_153', 'KrBeBabazDU_15_20', 'gIvetX_oXeI_85_90', 'MHIVmf42Ao4_31_36', 'ecm9gf2Pgkc_1_24', 'rOic25PnIx8_1_3', 'yyxtyCaEVqk_250_264', '9LSuyLyuUiM_3_6', '0lh_UWF9ZP4_50_60', 'Hd-NeIhbYGc_43_48', '4QdX5Q3qQsQ_38_44', '_O9kWD8nuRU_77_81', 'K1ZM0LSLVw8_481_487', 'lB1UPJ4leqs_0_6', 'wNv74rvkAw8_30_40', '2KrdBUFeFtY_22_27', 'c51L6ZxZGjQ_137_154', 'IiTzJQE67FA_74_86', 'IhwPQL9dFYc_171_175', '4QdX5Q3qQsQ_6_14', '0GXq1An3yHI_22_35', 'ybVb3t560oY_0_9', 'GcfWD62sbcs_13_33', 'po2tcrG6KzM_2_8', 'eZLxohGP4IE_15_25', 'ACOmKiJDkA4_75_81', 'WZTGqvbqOFE_28_34', 'KPPCwmU5OHQ_467_472', 'Q6HuQEIJqcA_9_16', 'BE_uxYK9uz0_49_56', 'p6T3XrnYtFk_4_13', 'FoL7aWvpvNs_199_204', 'pRpeEdMmmQ0_65_70', 'ID2xXlUENG0_19_24', 'xBW_uR3kGr4_42_55', '0lh_UWF9ZP4_157_160', 'Sq7Vt6KC9Yg_17_35', 'ecVwxlXc1PQ_0_12', 'NZWiKMZS_k8_39_48', 'zkTn5Ef1Oig_71_75', 's20OlIRK340_147_174', '4QYPZM01xFE_14_26', 'hJFBXHtxKIc_317_322', '5K7Ru5s6YIw_8_11', 'hPyU5KjpWVc_0_35', 'SFFfhAUEoj0_10_16'}\n",
            "[val] All captions in JSON 'sentences' section have matching video files in folder.\n",
            "[test] All videos in folder have matching entries in JSON 'videos' section.\n",
            "[test] All entries in JSON 'videos' section have matching video files in folder.\n",
            "[test] All videos in folder have matching captions in JSON 'sentences' section.\n",
            "[test] All captions in JSON 'sentences' section have matching video files in folder.\n"
          ]
        }
      ],
      "source": [
        "#verification for split\n",
        "\n",
        "import os\n",
        "import json\n",
        "\n",
        "def verify_split_integrity(split_name, base_dir):\n",
        "    \"\"\"\n",
        "    Verifies that each video in the split has a corresponding entry in the JSON file\n",
        "    and each entry in the JSON file has a corresponding video file.\n",
        "\n",
        "    Args:\n",
        "        split_name (str): Name of the split (e.g., 'train', 'val', 'test').\n",
        "        base_dir (str): Base directory where splits are stored (e.g., '/content/drive/MyDrive/msvd_split/').\n",
        "    \"\"\"\n",
        "    video_dir = os.path.join(base_dir, split_name, 'videos')\n",
        "    json_path = os.path.join(base_dir, split_name, f\"{split_name}_captions.json\")\n",
        "\n",
        "    # Load JSON data\n",
        "    with open(json_path, 'r') as f:\n",
        "        json_data = json.load(f)\n",
        "\n",
        "    # Get the list of video files and remove file extensions\n",
        "    video_files = {os.path.splitext(file)[0] for file in os.listdir(video_dir) if file.endswith('.avi')}\n",
        "\n",
        "    # Get the list of video_ids from JSON \"videos\" section\n",
        "    json_video_ids = {video['video_id'] for video in json_data['videos']}\n",
        "\n",
        "    # Check that each video file has a corresponding entry in JSON \"videos\" section\n",
        "    missing_in_json_videos = video_files - json_video_ids\n",
        "    if missing_in_json_videos:\n",
        "        print(f\"[{split_name}] Videos present in folder but missing in JSON 'videos' section: {missing_in_json_videos}\")\n",
        "    else:\n",
        "        print(f\"[{split_name}] All videos in folder have matching entries in JSON 'videos' section.\")\n",
        "\n",
        "    # Check that each JSON entry in \"videos\" section has a corresponding video file in the folder\n",
        "    missing_in_videos_folder = json_video_ids - video_files\n",
        "    if missing_in_videos_folder:\n",
        "        print(f\"[{split_name}] Entries in JSON 'videos' section but missing video files: {missing_in_videos_folder}\")\n",
        "    else:\n",
        "        print(f\"[{split_name}] All entries in JSON 'videos' section have matching video files in folder.\")\n",
        "\n",
        "    # Get the list of video_ids from JSON \"sentences\" section\n",
        "    json_caption_video_ids = {sentence['video_id'] for sentence in json_data['sentences']}\n",
        "\n",
        "    # Check that each video file has a corresponding caption in JSON \"sentences\" section\n",
        "    missing_captions_for_videos = video_files - json_caption_video_ids\n",
        "    if missing_captions_for_videos:\n",
        "        print(f\"[{split_name}] Videos present in folder but missing captions in JSON 'sentences' section: {missing_captions_for_videos}\")\n",
        "    else:\n",
        "        print(f\"[{split_name}] All videos in folder have matching captions in JSON 'sentences' section.\")\n",
        "\n",
        "    # Check that each JSON entry in \"sentences\" section has a corresponding video file in the folder\n",
        "    missing_videos_for_captions = json_caption_video_ids - video_files\n",
        "    if missing_videos_for_captions:\n",
        "        print(f\"[{split_name}] Captions in JSON 'sentences' section but missing video files: {missing_videos_for_captions}\")\n",
        "    else:\n",
        "        print(f\"[{split_name}] All captions in JSON 'sentences' section have matching video files in folder.\")\n",
        "\n",
        "# Run verification for each split\n",
        "base_dir = '/content/drive/MyDrive/msvd_split'  # Change to your base directory if different\n",
        "for split in ['train', 'val', 'test']:\n",
        "    verify_split_integrity(split, base_dir)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z25tS76EA1Fr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivF7r3xRzIJv",
        "outputId": "e161753e-77b8-465b-c90d-4c010448f881"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning train JSON file...\n",
            "Cleaned JSON file saved to /content/drive/MyDrive/msvd_split/train/train_captions.json\n",
            "train JSON file cleaned.\n",
            "\n",
            "Cleaning val JSON file...\n",
            "Cleaned JSON file saved to /content/drive/MyDrive/msvd_split/val/val_captions.json\n",
            "val JSON file cleaned.\n",
            "\n",
            "Cleaning test JSON file...\n",
            "Cleaned JSON file saved to /content/drive/MyDrive/msvd_split/test/test_captions.json\n",
            "test JSON file cleaned.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#removing caption of which video is not present\n",
        "\n",
        "import os\n",
        "import json\n",
        "\n",
        "def clean_json_for_existing_videos(json_path, video_dir):\n",
        "    # Load JSON data\n",
        "    with open(json_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Get the set of video_ids from the actual video files in the folder\n",
        "    video_files = {os.path.splitext(file)[0] for file in os.listdir(video_dir) if file.endswith('.avi')}\n",
        "\n",
        "    # Filter out entries in 'videos' and 'sentences' that don't have a corresponding video file\n",
        "    data['videos'] = [video for video in data['videos'] if video['video_id'] in video_files]\n",
        "    data['sentences'] = [sentence for sentence in data['sentences'] if sentence['video_id'] in video_files]\n",
        "\n",
        "    # Save the cleaned JSON data back to the file\n",
        "    with open(json_path, 'w') as f:\n",
        "        json.dump(data, f, indent=4)\n",
        "\n",
        "    print(f\"Cleaned JSON file saved to {json_path}\")\n",
        "\n",
        "# Define paths to each split's JSON file and video folder\n",
        "base_dir = '/content/drive/MyDrive/msvd_split'\n",
        "splits = {\n",
        "    'train': {'json_path': os.path.join(base_dir, 'train', 'train_captions.json'),\n",
        "              'video_dir': os.path.join(base_dir, 'train', 'videos')},\n",
        "    'val': {'json_path': os.path.join(base_dir, 'val', 'val_captions.json'),\n",
        "            'video_dir': os.path.join(base_dir, 'val', 'videos')},\n",
        "    'test': {'json_path': os.path.join(base_dir, 'test', 'test_captions.json'),\n",
        "             'video_dir': os.path.join(base_dir, 'test', 'videos')}\n",
        "}\n",
        "\n",
        "# Clean each JSON file based on existing video files\n",
        "for split, paths in splits.items():\n",
        "    print(f\"Cleaning {split} JSON file...\")\n",
        "    clean_json_for_existing_videos(paths['json_path'], paths['video_dir'])\n",
        "    print(f\"{split} JSON file cleaned.\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m5fYn6G2A3kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pejwd2C5zO2S",
        "outputId": "e5ff9ba7-8dbe-4537-9c95-8fe8c29be4fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary built with 10109 words. Saved to /content/drive/MyDrive/msvd_split/vocab.json\n"
          ]
        }
      ],
      "source": [
        "#vocabulary building\n",
        "\n",
        "import json\n",
        "from collections import Counter\n",
        "\n",
        "def build_vocabulary(json_paths, min_freq=1, save_path='/content/drive/MyDrive/msvd_split/vocab.json'):\n",
        "    special_tokens = ['<PAD>', '<SOS>', '<EOS>', '<UNK>']\n",
        "    word_counter = Counter()\n",
        "\n",
        "    # Process each JSON file to gather word frequencies\n",
        "    for json_path in json_paths:\n",
        "        with open(json_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "            for sentence in data['sentences']:\n",
        "                words = sentence['caption'].lower().split()\n",
        "                word_counter.update(words)\n",
        "\n",
        "    # Create vocabulary by adding special tokens and frequent words\n",
        "    vocab = {token: idx for idx, token in enumerate(special_tokens)}\n",
        "    for word, freq in word_counter.items():\n",
        "        if freq >= min_freq:\n",
        "            vocab[word] = len(vocab)\n",
        "\n",
        "    # Save vocabulary as a JSON file\n",
        "    with open(save_path, 'w') as f:\n",
        "        json.dump(vocab, f, indent=4)\n",
        "\n",
        "    print(f\"Vocabulary built with {len(vocab)} words. Saved to {save_path}\")\n",
        "\n",
        "# Define paths to JSON files\n",
        "json_paths = [\n",
        "    '/content/drive/MyDrive/msvd_split/train/train_captions.json',\n",
        "    '/content/drive/MyDrive/msvd_split/val/val_captions.json',\n",
        "    '/content/drive/MyDrive/msvd_split/test/test_captions.json'\n",
        "]\n",
        "\n",
        "# Build vocabulary\n",
        "build_vocabulary(json_paths)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZYzoNdC3A6L4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJ0Jql7PdlJH",
        "outputId": "0e9f60b6-e317-4dc5-f990-43049436e188"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.6.0+cu124)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (2.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchvision\n",
        "!pip install opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pretrainedmodels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLs5W1_mBAJ-",
        "outputId": "368ecdf4-4007-4fb9-fc8e-fe1dec1c9f6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pretrainedmodels in /usr/local/lib/python3.11/dist-packages (0.7.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from pretrainedmodels) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from pretrainedmodels) (0.21.0+cu124)\n",
            "Requirement already satisfied: munch in /usr/local/lib/python3.11/dist-packages (from pretrainedmodels) (4.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from pretrainedmodels) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->pretrainedmodels) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->pretrainedmodels) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->pretrainedmodels) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->pretrainedmodels) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cjvFcYizbj1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aaOhiAxDbj40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2crQ3kyRzY0z",
        "outputId": "d8fd925e-5ee9-45b9-86cf-b66ea8e7e3ee"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet152-b121ed2d.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-b121ed2d.pth\n",
            "100%|| 230M/230M [00:01<00:00, 204MB/s]\n",
            "Processing train videos:   0%|          | 0/1535 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "#feature extraction\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import subprocess\n",
        "import glob\n",
        "import numpy as np\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import pretrainedmodels\n",
        "from pretrainedmodels import utils\n",
        "\n",
        "# Constants for input dimensions\n",
        "C, H, W = 3, 224, 224\n",
        "\n",
        "# Function to extract frames from a video\n",
        "def extract_frames(video_path, dst):\n",
        "    if os.path.exists(dst):\n",
        "        shutil.rmtree(dst)\n",
        "    os.makedirs(dst)\n",
        "    video_to_frames_command = [\n",
        "        \"ffmpeg\",\n",
        "        '-y',\n",
        "        '-i', video_path,\n",
        "        '-vf', \"scale=400:300\",\n",
        "        '-qscale:v', \"2\",\n",
        "        f\"{dst}/%06d.jpg\"\n",
        "    ]\n",
        "    subprocess.call(video_to_frames_command, stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)\n",
        "\n",
        "# Function to extract features from frames\n",
        "def extract_feats(params, model, load_image_fn, split):\n",
        "    model.eval()\n",
        "    dir_fc = os.path.join(params['output_dir'], split, 'features')  # Store features in respective split folder\n",
        "    os.makedirs(dir_fc, exist_ok=True)\n",
        "\n",
        "    # Load video list from JSON file for the split\n",
        "    json_path = params[f'{split}_json']\n",
        "    with open(json_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "        video_ids = {video['video_id'] for video in data['videos']}\n",
        "\n",
        "    # Process each video in the specified directory\n",
        "    video_dir = os.path.join(params['video_path'], split, 'videos')  # Use respective split folder\n",
        "    video_list = glob.glob(os.path.join(video_dir, '*.avi'))\n",
        "    for video in tqdm(video_list, desc=f\"Processing {split} videos\"):\n",
        "        video_id = os.path.splitext(os.path.basename(video))[0]\n",
        "        if video_id not in video_ids:\n",
        "            continue\n",
        "\n",
        "        # Extract frames\n",
        "        dst = os.path.join(params['tmp_dir'], video_id)\n",
        "        extract_frames(video, dst)\n",
        "\n",
        "        # Load frames and extract features\n",
        "        image_list = sorted(glob.glob(os.path.join(dst, '*.jpg')))\n",
        "        samples = np.round(np.linspace(0, len(image_list) - 1, params['n_frame_steps'])).astype(int)\n",
        "        image_list = [image_list[sample] for sample in samples]\n",
        "        images = torch.zeros((len(image_list), C, H, W))\n",
        "\n",
        "        for i, img_path in enumerate(image_list):\n",
        "            img = load_image_fn(img_path)\n",
        "            images[i] = img\n",
        "\n",
        "        # Move images to GPU for feature extraction\n",
        "        images = images.cuda()\n",
        "        with torch.no_grad():\n",
        "            fc_feats = model(images).cpu().squeeze()\n",
        "\n",
        "        # Save features\n",
        "        outfile = os.path.join(dir_fc, f\"{video_id}.npy\")\n",
        "        np.save(outfile, fc_feats.numpy())\n",
        "\n",
        "        # Clean up\n",
        "        shutil.rmtree(dst)\n",
        "\n",
        "    print(f\"Feature extraction for {split} set is complete.\")\n",
        "\n",
        "# Main code setup with hardcoded parameters\n",
        "params = {\n",
        "    'output_dir': '/content/drive/MyDrive/msvd_split',\n",
        "    'video_path': '/content/drive/MyDrive/msvd_split',\n",
        "    'n_frame_steps': 40,\n",
        "    'tmp_dir': '/content/tmp_frames',\n",
        "    'train_json': '/content/drive/MyDrive/msvd_split/train/train_captions.json',\n",
        "    'val_json': '/content/drive/MyDrive/msvd_split/val/val_captions.json',\n",
        "    'test_json': '/content/drive/MyDrive/msvd_split/test/test_captions.json',\n",
        "    'model': 'resnet152'  # Set your model choice here (resnet152, inception_v3, or inception_v4)\n",
        "}\n",
        "\n",
        "# Set up model and image loader\n",
        "if params['model'] == 'inception_v3':\n",
        "    C, H, W = 3, 299, 299\n",
        "    model = pretrainedmodels.inceptionv3(pretrained='imagenet')\n",
        "    load_image_fn = utils.LoadTransformImage(model)\n",
        "elif params['model'] == 'resnet152':\n",
        "    C, H, W = 3, 224, 224\n",
        "    model = pretrainedmodels.resnet152(pretrained='imagenet')\n",
        "    load_image_fn = utils.LoadTransformImage(model)\n",
        "elif params['model'] == 'inception_v4':\n",
        "    C, H, W = 3, 299, 299\n",
        "    model = pretrainedmodels.inceptionv4(num_classes=1000, pretrained='imagenet')\n",
        "    load_image_fn = utils.LoadTransformImage(model)\n",
        "else:\n",
        "    raise ValueError(f\"Model {params['model']} is not supported\")\n",
        "\n",
        "model.last_linear = utils.Identity()  # Remove final classification layer\n",
        "model = model.cuda()  # Use GPU\n",
        "\n",
        "# Extract features for each split\n",
        "for split in ['train', 'val', 'test']:\n",
        "    extract_feats(params, model, load_image_fn, split)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "myGqtMqZBBKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NL3b52zs02dc",
        "outputId": "64ae78f4-eb4b-443e-ebe4-68ec7c23d2de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train] All video files have corresponding features.\n",
            "[train] All JSON entries have corresponding features.\n",
            "[train] No extra feature files found.\n",
            "[val] All video files have corresponding features.\n",
            "[val] All JSON entries have corresponding features.\n",
            "[val] No extra feature files found.\n",
            "[test] All video files have corresponding features.\n",
            "[test] All JSON entries have corresponding features.\n",
            "[test] No extra feature files found.\n"
          ]
        }
      ],
      "source": [
        "#verifying feature extracted correctly or not\n",
        "\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Function to verify features extraction for each split by cross-checking both video files and JSON entries\n",
        "def verify_features_extraction(split, params):\n",
        "    # Paths for the split\n",
        "    video_dir = os.path.join(params['output_dir'], split, 'videos')\n",
        "    features_dir = os.path.join(params['output_dir'], split, 'features')\n",
        "    json_path = os.path.join(params['output_dir'], split, f\"{split}_captions.json\")\n",
        "\n",
        "    # Load JSON file for the split\n",
        "    with open(json_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "        video_ids_in_json = {video['video_id'] for video in data['videos']}\n",
        "\n",
        "    # Check from Video Files\n",
        "    missing_features_from_videos = []\n",
        "    extra_features_in_folder_from_videos = []\n",
        "    video_files = {os.path.splitext(video)[0] for video in os.listdir(video_dir) if video.endswith('.avi')}\n",
        "    for video_id in video_files:\n",
        "        feature_path = os.path.join(features_dir, f\"{video_id}.npy\")\n",
        "        if not os.path.exists(feature_path):\n",
        "            missing_features_from_videos.append(video_id)\n",
        "\n",
        "    # Check from JSON Entries\n",
        "    missing_features_from_json = []\n",
        "    for video_id in video_ids_in_json:\n",
        "        feature_path = os.path.join(features_dir, f\"{video_id}.npy\")\n",
        "        if not os.path.exists(feature_path):\n",
        "            missing_features_from_json.append(video_id)\n",
        "\n",
        "    # Check for extra feature files\n",
        "    extra_features = []\n",
        "    for feature_file in os.listdir(features_dir):\n",
        "        video_id = os.path.splitext(feature_file)[0]\n",
        "        if video_id not in video_files and video_id not in video_ids_in_json:\n",
        "            extra_features.append(video_id)\n",
        "\n",
        "    # Print results\n",
        "    if missing_features_from_videos:\n",
        "        print(f\"[{split}] Missing features for videos in the video folder: {len(missing_features_from_videos)}\")\n",
        "        print(\"Missing video IDs from video files:\", missing_features_from_videos)\n",
        "    else:\n",
        "        print(f\"[{split}] All video files have corresponding features.\")\n",
        "\n",
        "    if missing_features_from_json:\n",
        "        print(f\"[{split}] Missing features for videos in the JSON file: {len(missing_features_from_json)}\")\n",
        "        print(\"Missing video IDs from JSON:\", missing_features_from_json)\n",
        "    else:\n",
        "        print(f\"[{split}] All JSON entries have corresponding features.\")\n",
        "\n",
        "    if extra_features:\n",
        "        print(f\"[{split}] Extra feature files found that do not match any video or JSON entry: {len(extra_features)}\")\n",
        "        print(\"Extra feature file video IDs:\", extra_features)\n",
        "    else:\n",
        "        print(f\"[{split}] No extra feature files found.\")\n",
        "\n",
        "# Define your parameters\n",
        "params = {\n",
        "    'output_dir': '/content/drive/MyDrive/msvd_split',\n",
        "}\n",
        "\n",
        "# Verify for each split\n",
        "for split in ['train', 'val', 'test']:\n",
        "    verify_features_extraction(split, params)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uvPCocdnBDtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pNwb0p0GBF-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kA-HbIAwBGBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xXUjG2pvBGEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GOiQAiB3BGG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ScOtcvrNnsrc"
      },
      "outputs": [],
      "source": [
        "#video and json split\n",
        "\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "def split_json_data(input_json_path, videos_dir, output_dir, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
        "    # Load JSON data\n",
        "    with open(input_json_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Shuffle the video data to ensure randomness\n",
        "    video_entries = data['videos']\n",
        "    random.shuffle(video_entries)\n",
        "\n",
        "    # Calculate the split sizes\n",
        "    total_videos = len(video_entries)\n",
        "    train_size = int(total_videos * train_ratio)\n",
        "    val_size = int(total_videos * val_ratio)\n",
        "    test_size = total_videos - train_size - val_size\n",
        "\n",
        "    # Split the data\n",
        "    train_videos = video_entries[:train_size]\n",
        "    val_videos = video_entries[train_size:train_size + val_size]\n",
        "    test_videos = video_entries[train_size + val_size:]\n",
        "\n",
        "    # Organize sentences based on video_ids for each split\n",
        "    video_sentences = {video['video_id']: [] for video in video_entries}\n",
        "    for sentence in data['sentences']:\n",
        "        video_sentences[sentence['video_id']].append(sentence)\n",
        "\n",
        "    def create_split_data(split_videos, split_name):\n",
        "        split_data = {\n",
        "            \"videos\": split_videos,\n",
        "            \"sentences\": []\n",
        "        }\n",
        "        split_videos_dir = os.path.join(output_dir, split_name, \"videos\")\n",
        "        os.makedirs(split_videos_dir, exist_ok=True)\n",
        "\n",
        "        # Add captions and copy video files\n",
        "        for video in split_videos:\n",
        "            video_id = video['video_id']\n",
        "            split_data['sentences'].extend(video_sentences[video_id])\n",
        "\n",
        "            # Copy video file to the split directory\n",
        "            video_filename = f\"{video_id}.avi\"\n",
        "            src_video_path = os.path.join(videos_dir, video_filename)\n",
        "            dst_video_path = os.path.join(split_videos_dir, video_filename)\n",
        "            if os.path.exists(src_video_path):\n",
        "                shutil.copy(src_video_path, dst_video_path)\n",
        "            else:\n",
        "                print(f\"Warning: Video file {video_filename} not found in {videos_dir}.\")\n",
        "\n",
        "        # Save the JSON file for the split\n",
        "        split_json_path = os.path.join(output_dir, split_name, f\"{split_name}_captions.json\")\n",
        "        with open(split_json_path, 'w') as f:\n",
        "            json.dump(split_data, f, indent=4)\n",
        "\n",
        "    # Create each split\n",
        "    create_split_data(train_videos, \"train\")\n",
        "    create_split_data(val_videos, \"val\")\n",
        "    create_split_data(test_videos, \"test\")\n",
        "\n",
        "    print(\"Data split and saved successfully.\")\n",
        "\n",
        "# Define paths based on your directory structure\n",
        "input_json_path = '/content/drive/MyDrive/msvd_captions.json'  # Path to the original JSON file\n",
        "videos_dir = '/content/drive/MyDrive/YouTubeClips'  # Directory where video files are stored\n",
        "output_dir = '/content/drive/MyDrive/msvd_split'  # Directory where you want to save the splits\n",
        "\n",
        "# Run the split function\n",
        "split_json_data(input_json_path, videos_dir, output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qY19wtX6BInH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eni0vvQkeeIU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "731be8d1-3be5-47ca-d6e2-f8a1421a088c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Vocabulary built with 10109 tokens.\n",
            " Saved vocab to: /content/drive/MyDrive/msvd_split/vocab.json\n",
            " Saved reverse vocab to: /content/drive/MyDrive/msvd_split/vocab_rev.json\n",
            " Top 10 frequent words:\n",
            "   a               : 60338\n",
            "   is              : 29601\n",
            "   the             : 19526\n",
            "   man             : 15648\n",
            "   woman           : 6619\n",
            "   on              : 6274\n",
            "   in              : 6214\n",
            "   playing         : 5341\n",
            "   are             : 4795\n",
            "   of              : 4436\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from collections import Counter\n",
        "\n",
        "def build_vocabulary(\n",
        "    json_paths,\n",
        "    min_freq=1,\n",
        "    save_path='/content/drive/MyDrive/msvd_split/vocab.json',\n",
        "    reverse_path='/content/drive/MyDrive/msvd_split/vocab_rev.json',\n",
        "    top_k_preview=10\n",
        "):\n",
        "    special_tokens = ['<PAD>', '<SOS>', '<EOS>', '<UNK>']\n",
        "    word_counter = Counter()\n",
        "\n",
        "    # Step 1: Count all words\n",
        "    for json_path in json_paths:\n",
        "        with open(json_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "            for sentence in data['sentences']:\n",
        "                words = sentence['caption'].lower().split()\n",
        "                word_counter.update(words)\n",
        "\n",
        "    # Step 2: Initialize vocab with special tokens\n",
        "    vocab = {token: idx for idx, token in enumerate(special_tokens)}\n",
        "\n",
        "    # Step 3: Add frequent words sorted by frequency (descending)\n",
        "    for word, freq in word_counter.most_common():\n",
        "        if freq >= min_freq:\n",
        "            if word not in vocab:  # Avoid conflict with special tokens\n",
        "                vocab[word] = len(vocab)\n",
        "\n",
        "    # Step 4: Create reverse vocab\n",
        "    vocab_rev = {idx: word for word, idx in vocab.items()}\n",
        "\n",
        "    # Step 5: Save both files\n",
        "    with open(save_path, 'w') as f:\n",
        "        json.dump(vocab, f, indent=4)\n",
        "    with open(reverse_path, 'w') as f:\n",
        "        json.dump(vocab_rev, f, indent=4)\n",
        "\n",
        "    # Step 6: Show summary\n",
        "    print(f\" Vocabulary built with {len(vocab)} tokens.\")\n",
        "    print(f\" Saved vocab to: {save_path}\")\n",
        "    print(f\" Saved reverse vocab to: {reverse_path}\")\n",
        "    print(f\" Top {top_k_preview} frequent words:\")\n",
        "    for word, freq in word_counter.most_common(top_k_preview):\n",
        "        print(f\"   {word:<15} : {freq}\")\n",
        "\n",
        "# === Paths to your cleaned split JSON files\n",
        "json_paths = [\n",
        "    '/content/drive/MyDrive/msvd_split/train/train_captions.json',\n",
        "    '/content/drive/MyDrive/msvd_split/val/val_captions.json',\n",
        "    '/content/drive/MyDrive/msvd_split/test/test_captions.json'\n",
        "]\n",
        "\n",
        "# === Run vocabulary builder\n",
        "build_vocabulary(json_paths, min_freq=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zJRcdH-QBLe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U79BgOCT6NLP",
        "outputId": "60283ff6-6d83-4451-dae4-6bdbf3b0493b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing train videos: 100%|| 1789/1789 [35:07<00:00,  1.18s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature extraction for train set is complete.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing val videos: 100%|| 542/542 [07:32<00:00,  1.20it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature extraction for val set is complete.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing test videos: 100%|| 546/546 [07:24<00:00,  1.23it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature extraction for test set is complete.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#feature extraction\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import subprocess\n",
        "import glob\n",
        "import numpy as np\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import pretrainedmodels\n",
        "from pretrainedmodels import utils\n",
        "\n",
        "# Constants for input dimensions\n",
        "C, H, W = 3, 224, 224\n",
        "\n",
        "# Function to extract frames from a video\n",
        "def extract_frames(video_path, dst):\n",
        "    if os.path.exists(dst):\n",
        "        shutil.rmtree(dst)\n",
        "    os.makedirs(dst)\n",
        "    video_to_frames_command = [\n",
        "        \"ffmpeg\",\n",
        "        '-y',\n",
        "        '-i', video_path,\n",
        "        '-vf', \"scale=400:300\",\n",
        "        '-qscale:v', \"2\",\n",
        "        f\"{dst}/%06d.jpg\"\n",
        "    ]\n",
        "    subprocess.call(video_to_frames_command, stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)\n",
        "\n",
        "# Function to extract features from frames\n",
        "def extract_feats(params, model, load_image_fn, split):\n",
        "    model.eval()\n",
        "    dir_fc = os.path.join(params['output_dir'], split, 'features')  # Store features in respective split folder\n",
        "    os.makedirs(dir_fc, exist_ok=True)\n",
        "\n",
        "    # Load video list from JSON file for the split\n",
        "    json_path = params[f'{split}_json']\n",
        "    with open(json_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "        video_ids = {video['video_id'] for video in data['videos']}\n",
        "\n",
        "    # Process each video in the specified directory\n",
        "    video_dir = os.path.join(params['video_path'], split, 'videos')  # Use respective split folder\n",
        "    video_list = glob.glob(os.path.join(video_dir, '*.avi'))\n",
        "    for video in tqdm(video_list, desc=f\"Processing {split} videos\"):\n",
        "        video_id = os.path.splitext(os.path.basename(video))[0]\n",
        "        if video_id not in video_ids:\n",
        "            continue\n",
        "\n",
        "        # Extract frames\n",
        "        dst = os.path.join(params['tmp_dir'], video_id)\n",
        "        extract_frames(video, dst)\n",
        "\n",
        "        # Load frames and extract features\n",
        "        # image_list = sorted(glob.glob(os.path.join(dst, '*.jpg')))\n",
        "        # samples = np.round(np.linspace(0, len(image_list) - 1, params['n_frame_steps'])).astype(int)\n",
        "        # image_list = [image_list[sample] for sample in samples]\n",
        "        image_list = sorted(glob.glob(os.path.join(dst, '*.jpg')))\n",
        "        if len(image_list) == 0:\n",
        "            print(f\"[WARNING] No frames extracted for video {video_id}. Skipping...\")\n",
        "            shutil.rmtree(dst)\n",
        "            continue  # Skip to next video\n",
        "\n",
        "        # Safe sampling\n",
        "        n_frames = len(image_list)\n",
        "        sample_count = min(params['n_frame_steps'], n_frames)\n",
        "        samples = np.round(np.linspace(0, n_frames - 1, sample_count)).astype(int)\n",
        "        image_list = [image_list[sample] for sample in samples]\n",
        "\n",
        "        images = torch.zeros((len(image_list), C, H, W))\n",
        "\n",
        "        for i, img_path in enumerate(image_list):\n",
        "            img = load_image_fn(img_path)\n",
        "            images[i] = img\n",
        "\n",
        "        # Move images to GPU for feature extraction\n",
        "        images = images.cuda()\n",
        "        with torch.no_grad():\n",
        "            fc_feats = model(images).cpu().squeeze()\n",
        "\n",
        "        # Save features\n",
        "        outfile = os.path.join(dir_fc, f\"{video_id}.npy\")\n",
        "        np.save(outfile, fc_feats.numpy())\n",
        "\n",
        "        # Clean up\n",
        "        shutil.rmtree(dst)\n",
        "\n",
        "    print(f\"Feature extraction for {split} set is complete.\")\n",
        "\n",
        "# Main code setup with hardcoded parameters\n",
        "params = {\n",
        "    'output_dir': '/content/drive/MyDrive/VideoCaptioning/msvd_split',\n",
        "    'video_path': '/content/drive/MyDrive/VideoCaptioning/msvd_split',\n",
        "    'n_frame_steps': 40,\n",
        "    'tmp_dir': '/content/tmp_frames',\n",
        "    'train_json': '/content/drive/MyDrive/VideoCaptioning/msvd_split/train/train_captions.json',\n",
        "    'val_json': '/content/drive/MyDrive/VideoCaptioning/msvd_split/val/val_captions.json',\n",
        "    'test_json': '/content/drive/MyDrive/VideoCaptioning/msvd_split/test/test_captions.json',\n",
        "    'model': 'resnet152'  # Set your model choice here (resnet152, inception_v3, or inception_v4)\n",
        "}\n",
        "\n",
        "# Set up model and image loader\n",
        "if params['model'] == 'inception_v3':\n",
        "    C, H, W = 3, 299, 299\n",
        "    model = pretrainedmodels.inceptionv3(pretrained='imagenet')\n",
        "    load_image_fn = utils.LoadTransformImage(model)\n",
        "elif params['model'] == 'resnet152':\n",
        "    C, H, W = 3, 224, 224\n",
        "    model = pretrainedmodels.resnet152(pretrained='imagenet')\n",
        "    load_image_fn = utils.LoadTransformImage(model)\n",
        "elif params['model'] == 'inception_v4':\n",
        "    C, H, W = 3, 299, 299\n",
        "    model = pretrainedmodels.inceptionv4(num_classes=1000, pretrained='imagenet')\n",
        "    load_image_fn = utils.LoadTransformImage(model)\n",
        "else:\n",
        "    raise ValueError(f\"Model {params['model']} is not supported\")\n",
        "\n",
        "model.last_linear = utils.Identity()  # Remove final classification layer\n",
        "model = model.cuda()  # Use GPU\n",
        "\n",
        "# Extract features for each split\n",
        "for split in ['train', 'val', 'test']:\n",
        "    extract_feats(params, model, load_image_fn, split)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gZmRBEJzBOy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpISv9fCep4m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea02fe46-30be-4fec-de76-7695c4d76335"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Cleaned JSON for [train]\n",
            " Removed 0 invalid video entries\n",
            " Removed 0 invalid captions\n",
            " [train] Verified and fixed. All valid videos now have features and JSON alignment.\n",
            "\n",
            " Cleaned JSON for [val]\n",
            " Removed 0 invalid video entries\n",
            " Removed 0 invalid captions\n",
            " [val] Verified and fixed. All valid videos now have features and JSON alignment.\n",
            "\n",
            " Cleaned JSON for [test]\n",
            " Removed 0 invalid video entries\n",
            " Removed 0 invalid captions\n",
            " [test] Verified and fixed. All valid videos now have features and JSON alignment.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#cleaning feature ,which is not match\n",
        "import os\n",
        "import json\n",
        "\n",
        "def verify_and_fix_features(split, params):\n",
        "    video_dir = os.path.join(params['output_dir'], split, 'videos')\n",
        "    features_dir = os.path.join(params['output_dir'], split, 'features')\n",
        "    json_path = os.path.join(params['output_dir'], split, f\"{split}_captions.json\")\n",
        "\n",
        "    # Load JSON\n",
        "    with open(json_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "        json_video_ids = {video['video_id'] for video in data['videos']}\n",
        "\n",
        "    # All video and feature files\n",
        "    video_files = {os.path.splitext(f)[0] for f in os.listdir(video_dir) if f.endswith('.avi')}\n",
        "    feature_files = {os.path.splitext(f)[0] for f in os.listdir(features_dir) if f.endswith('.npy')}\n",
        "\n",
        "    # Identify corrupted or extra items\n",
        "    missing_feats_from_videos = video_files - feature_files\n",
        "    missing_feats_from_json = json_video_ids - feature_files\n",
        "    extra_feats = feature_files - video_files - json_video_ids\n",
        "\n",
        "    # === ACTION 1: Delete corrupted video files (with no features)\n",
        "    for vid in missing_feats_from_videos:\n",
        "        path = os.path.join(video_dir, f\"{vid}.avi\")\n",
        "        if os.path.exists(path):\n",
        "            os.remove(path)\n",
        "            print(f\" Deleted incomplete video (no features): {path}\")\n",
        "\n",
        "    # === ACTION 2: Remove JSON entries with missing features\n",
        "    original_video_count = len(data['videos'])\n",
        "    original_caption_count = len(data['sentences'])\n",
        "    data['videos'] = [v for v in data['videos'] if v['video_id'] in feature_files]\n",
        "    data['sentences'] = [s for s in data['sentences'] if s['video_id'] in feature_files]\n",
        "\n",
        "    with open(json_path, 'w') as f:\n",
        "        json.dump(data, f, indent=4)\n",
        "\n",
        "    print(f\" Cleaned JSON for [{split}]\")\n",
        "    print(f\" Removed {original_video_count - len(data['videos'])} invalid video entries\")\n",
        "    print(f\" Removed {original_caption_count - len(data['sentences'])} invalid captions\")\n",
        "\n",
        "    # === ACTION 3: Delete stray feature files\n",
        "    for vid in extra_feats:\n",
        "        path = os.path.join(features_dir, f\"{vid}.npy\")\n",
        "        if os.path.exists(path):\n",
        "            os.remove(path)\n",
        "            print(f\" Deleted stray feature file: {path}\")\n",
        "\n",
        "    print(f\" [{split}] Verified and fixed. All valid videos now have features and JSON alignment.\\n\")\n",
        "\n",
        "# Run for all splits\n",
        "params = {\n",
        "    'output_dir': '/content/drive/MyDrive/msvd_split',\n",
        "}\n",
        "\n",
        "for split in ['train', 'val', 'test']:\n",
        "    verify_and_fix_features(split, params)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CzNUWmB-BR-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32MO5vnShuPP"
      },
      "outputs": [],
      "source": [
        "#dataset for captioning\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class VideoCaptionDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset class for loading video features and their associated captions.\n",
        "\n",
        "    Args:\n",
        "        feature_dir (str): Directory containing .npy feature files.\n",
        "        json_path (str): Path to JSON file with \"videos\" and \"sentences\".\n",
        "        vocab (dict): Vocabulary mapping words to indices.\n",
        "        max_caption_length (int): Max length of tokenized captions (including <SOS> and <EOS>).\n",
        "        verbose (bool): If True, prints sample-level debug info.\n",
        "    \"\"\"\n",
        "    def __init__(self, feature_dir, json_path, vocab, max_caption_length=15, verbose=False):\n",
        "        self.feature_dir = feature_dir\n",
        "        self.vocab = vocab\n",
        "        self.max_caption_length = max_caption_length\n",
        "        self.verbose = verbose\n",
        "\n",
        "        # Load JSON\n",
        "        with open(json_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        # Map video_id to all its captions\n",
        "        self.video_captions = {}\n",
        "        for item in data['sentences']:\n",
        "            vid = item['video_id']\n",
        "            if vid in self.video_captions:\n",
        "                self.video_captions[vid].append(item['caption'])\n",
        "            else:\n",
        "                self.video_captions[vid] = [item['caption']]\n",
        "\n",
        "        # Keep video_ids that have both captions and feature files\n",
        "        all_video_ids = [v['video_id'] for v in data['videos']]\n",
        "        self.video_ids = [\n",
        "            vid for vid in all_video_ids\n",
        "            if vid in self.video_captions and os.path.exists(os.path.join(self.feature_dir, f\"{vid}.npy\"))\n",
        "        ]\n",
        "\n",
        "        if self.verbose:\n",
        "            print(\" Initialized VideoCaptionDataset\")\n",
        "            print(f\" Total valid samples: {len(self.video_ids)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_id = self.video_ids[idx]\n",
        "        feature_path = os.path.join(self.feature_dir, f\"{video_id}.npy\")\n",
        "\n",
        "        try:\n",
        "            video_features = np.load(feature_path)\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\" Failed to load features for {video_id}: {e}\")\n",
        "\n",
        "        # Choose a random caption and tokenize\n",
        "        caption = np.random.choice(self.video_captions[video_id])\n",
        "        tokens = [self.vocab['<SOS>']] + [\n",
        "            self.vocab.get(word, self.vocab['<UNK>']) for word in caption.lower().split()\n",
        "        ] + [self.vocab['<EOS>']]\n",
        "\n",
        "        # Truncate and pad\n",
        "        tokens = tokens[:self.max_caption_length]\n",
        "        tokens += [self.vocab['<PAD>']] * (self.max_caption_length - len(tokens))\n",
        "\n",
        "        caption_tensor = torch.tensor(tokens, dtype=torch.long)\n",
        "        video_tensor = torch.tensor(video_features, dtype=torch.float32)\n",
        "\n",
        "        # Debug preview\n",
        "        if self.verbose and idx == 0:\n",
        "            reverse_vocab = {v: k for k, v in self.vocab.items()}\n",
        "            print(f\"\\n Sample [{video_id}]\")\n",
        "            print(\" Caption:\", caption)\n",
        "            print(\" Tokens :\", tokens)\n",
        "            print(\" Decoded:\", [reverse_vocab.get(t, '<UNK>') for t in tokens])\n",
        "            print(\" Video Features Shape:\", video_tensor.shape)\n",
        "            print(\" Caption Tensor:\", caption_tensor)\n",
        "\n",
        "        return video_tensor, caption_tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hu6i7tHrBU9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2wsXk8Nh3s0",
        "outputId": "2cbc2448-b35a-43af-acd2-67a49c0bf52f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Initialized VideoCaptionDataset\n",
            " Total valid samples: 965\n",
            "Dataset size: 965\n",
            "\n",
            " Sample [1sffYOXq4Iw_23_49]\n",
            " Caption: a man is slicing a potato\n",
            " Tokens : [1, 4, 7, 5, 33, 4, 65, 2, 0, 0, 0, 0, 0, 0, 0]\n",
            " Decoded: ['<SOS>', 'a', 'man', 'is', 'slicing', 'a', 'potato', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            " Video Features Shape: torch.Size([40, 2048])\n",
            " Caption Tensor: tensor([ 1,  4,  7,  5, 33,  4, 65,  2,  0,  0,  0,  0,  0,  0,  0])\n",
            "Sample Video Features Shape: torch.Size([40, 2048])\n",
            "Sample Caption Tensor: tensor([ 1,  4,  7,  5, 33,  4, 65,  2,  0,  0,  0,  0,  0,  0,  0])\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Define directories based on your structure\n",
        "    feature_dir = '/content/drive/MyDrive/msvd_split/train/features'  # Path to train features directory\n",
        "    json_path = '/content/drive/MyDrive/msvd_split/train/train_captions.json'  # Path to train captions JSON\n",
        "    vocab_path = '/content/drive/MyDrive/msvd_split/vocab.json'  # Path to vocabulary JSON\n",
        "\n",
        "    # Load vocabulary\n",
        "    with open(vocab_path, 'r') as f:\n",
        "        vocab = json.load(f)\n",
        "\n",
        "    # Initialize dataset\n",
        "    dataset = VideoCaptionDataset(feature_dir, json_path, vocab, verbose=True)\n",
        "    print(f\"Dataset size: {len(dataset)}\")\n",
        "\n",
        "    # Access a sample item to verify\n",
        "    video_features, caption_tensor = dataset[0]\n",
        "    print(\"Sample Video Features Shape:\", video_features.shape)\n",
        "    print(\"Sample Caption Tensor:\", caption_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rb8RUiD4BX0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTj9UfbViMq5",
        "outputId": "6aa073d4-9591-4c24-9b50-30bc9e579d79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Initialized VideoCaptionDataset\n",
            " Total valid samples: 965\n",
            "Dataset Size: 965\n",
            "\n",
            " Sample [1sffYOXq4Iw_23_49]\n",
            " Caption: someone is slicing potatoes\n",
            " Tokens : [1, 28, 5, 33, 189, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " Decoded: ['<SOS>', 'someone', 'is', 'slicing', 'potatoes', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            " Video Features Shape: torch.Size([40, 2048])\n",
            " Caption Tensor: tensor([  1,  28,   5,  33, 189,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0])\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    feature_dir = '/content/drive/MyDrive/msvd_split/train/features'\n",
        "    json_path = '/content/drive/MyDrive/msvd_split/train/train_captions.json'\n",
        "    vocab_path = '/content/drive/MyDrive/msvd_split/vocab.json'\n",
        "\n",
        "    with open(vocab_path, 'r') as f:\n",
        "        vocab = json.load(f)\n",
        "\n",
        "    dataset = VideoCaptionDataset(\n",
        "        feature_dir=feature_dir,\n",
        "        json_path=json_path,\n",
        "        vocab=vocab,\n",
        "        max_caption_length=15,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    print(f\"Dataset Size: {len(dataset)}\")\n",
        "\n",
        "    # Test sample access\n",
        "    video_feat, cap_tensor = dataset[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X2WeEcglBaVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aa163cc8"
      },
      "outputs": [],
      "source": [
        "# Vanilla S2VT Encoder-Decoder Model (No Attention)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class S2VTModel(nn.Module):\n",
        "    def __init__(self, vocab_size, max_len=45, dim_hidden=1024, dim_word=512, dim_vid=2048,\n",
        "                 sos_id=1, eos_id=0, n_layers=1, rnn_cell='lstm', rnn_dropout_p=0.3):\n",
        "        super(S2VTModel, self).__init__()\n",
        "\n",
        "        self.rnn_cell_type = rnn_cell.lower()\n",
        "        self.rnn_cell = nn.LSTM if self.rnn_cell_type == 'lstm' else nn.GRU\n",
        "\n",
        "        self.rnn1 = self.rnn_cell(dim_vid, dim_hidden, n_layers, batch_first=True, dropout=rnn_dropout_p)\n",
        "        self.rnn2 = self.rnn_cell(dim_hidden + dim_word, dim_hidden, n_layers, batch_first=True, dropout=rnn_dropout_p)\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, dim_word)\n",
        "        self.out = nn.Linear(dim_hidden, vocab_size)\n",
        "\n",
        "        self.dim_vid = dim_vid\n",
        "        self.dim_hidden = dim_hidden\n",
        "        self.dim_word = dim_word\n",
        "        self.max_length = max_len\n",
        "        self.vocab_size = vocab_size\n",
        "        self.sos_id = sos_id\n",
        "        self.eos_id = eos_id\n",
        "\n",
        "    def forward(self, vid_feats, target_variable=None, mode='train', beam_width=3):\n",
        "        batch_size, n_frames, _ = vid_feats.shape\n",
        "        device = vid_feats.device\n",
        "\n",
        "        encoder_outputs, state1 = self.rnn1(vid_feats)  # [B, T, H]\n",
        "\n",
        "        if mode == 'train':\n",
        "            seq_probs = []\n",
        "            for t in range(self.max_length - 1):\n",
        "                current_word = self.embedding(target_variable[:, t])\n",
        "                input2 = torch.cat((encoder_outputs[:, -1], current_word), dim=1).unsqueeze(1)\n",
        "                output2, state2 = self.rnn2(input2, state1)\n",
        "                logits = self.out(output2.squeeze(1))\n",
        "                logits = F.log_softmax(logits, dim=1)\n",
        "                seq_probs.append(logits.unsqueeze(1))\n",
        "                state1 = state2\n",
        "            return torch.cat(seq_probs, dim=1), None\n",
        "\n",
        "        else:  # Inference mode\n",
        "            generated = torch.full((batch_size, 1), self.sos_id, dtype=torch.long, device=device)\n",
        "            seqs = []\n",
        "            for t in range(self.max_length - 1):\n",
        "                emb = self.embedding(generated[:, -1])\n",
        "                input2 = torch.cat((encoder_outputs[:, -1], emb), dim=1).unsqueeze(1)\n",
        "                output2, state1 = self.rnn2(input2, state1)\n",
        "                logits = self.out(output2.squeeze(1))\n",
        "                next_word = torch.argmax(F.log_softmax(logits, dim=1), dim=1).unsqueeze(1)\n",
        "                generated = torch.cat([generated, next_word], dim=1)\n",
        "            return None, generated\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z-w12IK2Bcx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmtDaRjbkgd3",
        "outputId": "07fad734-0002-4010-968b-1e1ae28ae326"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Model saved as S2VTModel.py in your Drive.\n"
          ]
        }
      ],
      "source": [
        "# Define the full model code as a string save the model\n",
        "model_code = '''import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BahdanauAttention(nn.Module):\n",
        "    def __init__(self, hidden_size, encoder_output_size):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.W2 = nn.Linear(encoder_output_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        hidden = hidden.unsqueeze(1)  # [B, 1, H]\n",
        "        score = self.V(torch.tanh(self.W1(hidden) + self.W2(encoder_outputs)))  # [B, T, 1]\n",
        "        attention_weights = F.softmax(score, dim=1)  # [B, T, 1]\n",
        "        context = torch.sum(attention_weights * encoder_outputs, dim=1)  # [B, H]\n",
        "        return context, attention_weights\n",
        "\n",
        "class S2VTModel(nn.Module):\n",
        "    def __init__(self, vocab_size, max_len=45, dim_hidden=1024, dim_word=512, dim_vid=2048,\n",
        "                 sos_id=1, eos_id=0, n_layers=1, rnn_cell='lstm', rnn_dropout_p=0.3):\n",
        "        super(S2VTModel, self).__init__()\n",
        "\n",
        "        self.rnn_cell_type = rnn_cell.lower()\n",
        "        self.rnn_cell = nn.LSTM if self.rnn_cell_type == 'lstm' else nn.GRU\n",
        "\n",
        "        self.rnn1 = self.rnn_cell(dim_vid, dim_hidden, n_layers, batch_first=True, dropout=rnn_dropout_p)\n",
        "        self.rnn2 = self.rnn_cell(dim_hidden + dim_word, dim_hidden, n_layers, batch_first=True, dropout=rnn_dropout_p)\n",
        "\n",
        "        self.attention = BahdanauAttention(dim_hidden, dim_hidden)\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, dim_word)\n",
        "        self.out = nn.Linear(dim_hidden, vocab_size)\n",
        "\n",
        "        self.dim_vid = dim_vid\n",
        "        self.dim_hidden = dim_hidden\n",
        "        self.dim_word = dim_word\n",
        "        self.max_length = max_len\n",
        "        self.vocab_size = vocab_size\n",
        "        self.sos_id = sos_id\n",
        "        self.eos_id = eos_id\n",
        "\n",
        "    def forward(self, vid_feats, target_variable=None, mode='train', beam_width=3):\n",
        "        batch_size, n_frames, _ = vid_feats.shape\n",
        "        device = vid_feats.device\n",
        "\n",
        "        encoder_outputs, state1 = self.rnn1(vid_feats)  # [B, T, H]\n",
        "\n",
        "        if mode == 'train':\n",
        "            seq_probs = []\n",
        "            for t in range(self.max_length - 1):\n",
        "                current_word = self.embedding(target_variable[:, t])\n",
        "                context, _ = self.attention(state1[0][-1] if self.rnn_cell_type == 'lstm' else state1[-1], encoder_outputs)\n",
        "                input2 = torch.cat((context, current_word), dim=1).unsqueeze(1)\n",
        "                output2, state2 = self.rnn2(input2, state1)\n",
        "                logits = self.out(output2.squeeze(1))\n",
        "                logits = F.log_softmax(logits, dim=1)\n",
        "                seq_probs.append(logits.unsqueeze(1))\n",
        "                state1 = state2\n",
        "            return torch.cat(seq_probs, dim=1), None\n",
        "\n",
        "        else:  # Beam Search Inference\n",
        "            beams = [(torch.tensor([self.sos_id], device=device), 0.0, state1)]\n",
        "            completed = []\n",
        "\n",
        "            for _ in range(self.max_length - 1):\n",
        "                new_beams = []\n",
        "                for seq, score, state in beams:\n",
        "                    last_word = seq[-1].unsqueeze(0)\n",
        "                    if last_word.item() == self.eos_id:\n",
        "                        completed.append((seq, score))\n",
        "                        continue\n",
        "\n",
        "                    emb = self.embedding(last_word).unsqueeze(0)  # [1, 1, D]\n",
        "                    context, _ = self.attention(state[0][-1] if self.rnn_cell_type == 'lstm' else state[-1], encoder_outputs)\n",
        "                    input2 = torch.cat((context, emb.squeeze(1)), dim=-1).unsqueeze(1)\n",
        "                    output2, new_state = self.rnn2(input2, state)\n",
        "                    logits = self.out(output2.squeeze(1))\n",
        "                    log_probs = F.log_softmax(logits, dim=1)\n",
        "                    topk_log_probs, topk_indices = torch.topk(log_probs, beam_width)\n",
        "\n",
        "                    for k in range(beam_width):\n",
        "                        new_seq = torch.cat([seq, topk_indices[0, k].unsqueeze(0)])\n",
        "                        new_score = score + topk_log_probs[0, k].item()\n",
        "                        new_beams.append((new_seq, new_score, new_state))\n",
        "\n",
        "                beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_width]\n",
        "\n",
        "            best_seq = max(completed or beams, key=lambda x: x[1])[0]\n",
        "            return None, best_seq.unsqueeze(0)\n",
        "\n",
        "'''\n",
        "\n",
        "# Save to Google Drive\n",
        "with open('/content/drive/MyDrive/S2VTModel.py', 'w') as f:\n",
        "    f.write(model_code)\n",
        "\n",
        "print(\" Model saved as S2VTModel.py in your Drive.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1fXLd2lvBfX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yx00QdBZk3ds"
      },
      "outputs": [],
      "source": [
        "#training model with cheackpoint\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train_model(model, train_dataset, val_dataset, vocab, device,\n",
        "                num_epochs=10, batch_size=8, learning_rate=1e-4, checkpoint_dir=None):\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    model = model.to(device)\n",
        "    criterion = nn.NLLLoss(ignore_index=vocab['<PAD>'])\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "\n",
        "        print(f\"\\n Epoch [{epoch}/{num_epochs}]\")\n",
        "\n",
        "        for video_feats, captions in tqdm(train_loader, desc='Training'):\n",
        "            video_feats, captions = video_feats.to(device), captions.to(device)\n",
        "\n",
        "            # Prepare input and target\n",
        "            inputs = captions[:, :-1]\n",
        "            targets = captions[:, 1:]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs, _ = model(video_feats, target_variable=inputs, mode='train')  # [B, T, V]\n",
        "\n",
        "            outputs = outputs.view(-1, outputs.size(-1))       # [B*T, V]\n",
        "            targets = targets.reshape(-1)                      # [B*T]\n",
        "\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "        print(f\" Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # === Validation ===\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for video_feats, captions in tqdm(val_loader, desc='Validation'):\n",
        "                video_feats, captions = video_feats.to(device), captions.to(device)\n",
        "                inputs = captions[:, :-1]\n",
        "                targets = captions[:, 1:]\n",
        "\n",
        "                outputs, _ = model(video_feats, target_variable=inputs, mode='train')\n",
        "                outputs = outputs.view(-1, outputs.size(-1))\n",
        "                targets = targets.reshape(-1)\n",
        "\n",
        "                loss = criterion(outputs, targets)\n",
        "                total_val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "        print(f\" Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Save checkpoint\n",
        "        if checkpoint_dir:\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'train_loss': avg_train_loss,\n",
        "                'val_loss': avg_val_loss,\n",
        "            }, f\"{checkpoint_dir}/checkpoint_epoch_{epoch}.pt\")\n",
        "            print(f\" Saved checkpoint to {checkpoint_dir}/checkpoint_epoch_{epoch}.pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5WS28HyhBij0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rGmYJpumQor"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import json\n",
        "\n",
        "# === Paths ===\n",
        "feature_dir_train = '/content/drive/MyDrive/msvd_split/train/features'\n",
        "feature_dir_val = '/content/drive/MyDrive/msvd_split/val/features'\n",
        "json_train = '/content/drive/MyDrive/msvd_split/train/train_captions.json'\n",
        "json_val = '/content/drive/MyDrive/msvd_split/val/val_captions.json'\n",
        "vocab_path = '/content/drive/MyDrive/msvd_split/vocab.json'\n",
        "\n",
        "# === Load vocab ===\n",
        "with open(vocab_path, 'r') as f:\n",
        "    vocab = json.load(f)\n",
        "\n",
        "# === Initialize datasets ===\n",
        "train_dataset = VideoCaptionDataset(\n",
        "    feature_dir=feature_dir_train,\n",
        "    json_path=json_train,\n",
        "    vocab=vocab,\n",
        "    max_caption_length=45\n",
        ")\n",
        "\n",
        "val_dataset = VideoCaptionDataset(\n",
        "    feature_dir=feature_dir_val,\n",
        "    json_path=json_val,\n",
        "    vocab=vocab,\n",
        "    max_caption_length=45\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-pAtiHOBBlMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1sX8NmMltFH",
        "outputId": "f73750e9-0ca4-463f-a4b6-1c42bee77785"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch [1/30]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|| 121/121 [09:53<00:00,  4.91s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Training Loss: 5.7049\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|| 6/6 [00:20<00:00,  3.37s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Validation Loss: 4.5177\n",
            " Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_1.pt\n",
            "\n",
            " Epoch [2/30]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|| 121/121 [00:55<00:00,  2.17it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Training Loss: 4.7778\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|| 6/6 [00:00<00:00, 12.52it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Validation Loss: 4.3640\n",
            " Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_2.pt\n",
            "\n",
            " Epoch [3/30]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|| 121/121 [00:54<00:00,  2.21it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Training Loss: 4.5346\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|| 6/6 [00:00<00:00, 12.33it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Validation Loss: 4.3532\n",
            " Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_3.pt\n",
            "\n",
            " Epoch [4/30]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|| 121/121 [00:55<00:00,  2.16it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Training Loss: 4.4049\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|| 6/6 [00:00<00:00, 12.42it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Validation Loss: 4.1124\n",
            " Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_4.pt\n",
            "\n",
            " Epoch [5/30]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|| 121/121 [00:54<00:00,  2.23it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Training Loss: 4.1829\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|| 6/6 [00:00<00:00, 12.37it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Validation Loss: 3.7684\n",
            " Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_5.pt\n",
            "\n",
            " Epoch [6/30]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|| 121/121 [00:54<00:00,  2.21it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Training Loss: 4.2357\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|| 6/6 [00:00<00:00, 12.76it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Validation Loss: 3.8354\n",
            " Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_6.pt\n",
            "\n",
            " Epoch [7/30]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|| 121/121 [00:54<00:00,  2.21it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Training Loss: 4.1720\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|| 6/6 [00:00<00:00, 11.97it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Validation Loss: 3.6494\n",
            " Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_7.pt\n",
            "\n",
            " Epoch [8/30]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|| 121/121 [00:55<00:00,  2.20it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Training Loss: 4.1088\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|| 6/6 [00:00<00:00, 12.56it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Validation Loss: 3.7368\n",
            " Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_8.pt\n",
            "\n",
            " Epoch [9/30]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|| 121/121 [00:55<00:00,  2.18it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Training Loss: 4.0950\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|| 6/6 [00:00<00:00, 11.74it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Validation Loss: 4.2432\n",
            " Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_9.pt\n",
            "\n",
            " Epoch [10/30]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|| 121/121 [00:55<00:00,  2.20it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Training Loss: 4.0317\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|| 6/6 [00:00<00:00, 12.59it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Validation Loss: 3.8027\n",
            " Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_10.pt\n",
            "\n",
            " Epoch [11/30]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|| 121/121 [00:56<00:00,  2.14it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Training Loss: 3.9290\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|| 6/6 [00:00<00:00, 12.23it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Validation Loss: 3.4410\n",
            " Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_11.pt\n",
            "\n",
            " Epoch [12/30]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|| 121/121 [00:55<00:00,  2.18it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Training Loss: 4.0207\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|| 6/6 [00:00<00:00, 11.94it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Validation Loss: 3.7534\n",
            " Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_12.pt\n",
            "\n",
            " Epoch [13/30]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|| 121/121 [00:57<00:00,  2.10it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Training Loss: 3.9466\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|| 6/6 [00:00<00:00, 11.32it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Validation Loss: 4.1549\n",
            " Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_13.pt\n",
            "\n",
            " Epoch [14/30]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|| 121/121 [00:57<00:00,  2.10it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Training Loss: 3.8821\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 6/6 [00:00<00:00, 10.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Validation Loss: 3.8751\n",
            " Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_14.pt\n",
            "\n",
            " Epoch [15/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 121/121 [00:55<00:00,  2.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Training Loss: 3.9862\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 6/6 [00:00<00:00, 12.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Validation Loss: 3.1661\n",
            " Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_15.pt\n",
            "\n",
            " Epoch [16/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 121/121 [00:54<00:00,  2.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Training Loss: 3.9115\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 6/6 [00:00<00:00, 11.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Validation Loss: 3.7140\n",
            " Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_16.pt\n",
            "\n",
            " Epoch [17/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 121/121 [00:54<00:00,  2.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Training Loss: 3.8798\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 6/6 [00:00<00:00, 12.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Validation Loss: 3.5871\n",
            " Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_17.pt\n",
            "\n",
            " Epoch [18/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 121/121 [00:54<00:00,  2.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Training Loss: 3.8448\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 6/6 [00:00<00:00, 12.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Validation Loss: 3.6615\n",
            " Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_18.pt\n",
            "\n",
            " Epoch [19/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 121/121 [00:54<00:00,  2.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Training Loss: 3.8619\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 6/6 [00:00<00:00, 12.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Validation Loss: 3.5195\n",
            " Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_19.pt\n",
            "\n",
            " Epoch [20/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 121/121 [00:55<00:00,  2.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Training Loss: 3.8771\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 6/6 [00:00<00:00, 12.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Validation Loss: 3.4527\n",
            " Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_20.pt\n",
            "\n",
            " Epoch [21/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 121/121 [00:56<00:00,  2.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Training Loss: 3.7955\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 6/6 [00:00<00:00, 12.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Validation Loss: 3.4630\n",
            " Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_21.pt\n",
            "\n",
            " Epoch [22/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 121/121 [00:54<00:00,  2.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Training Loss: 3.8904\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 6/6 [00:00<00:00, 11.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Validation Loss: 3.4151\n",
            " Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_22.pt\n",
            "\n",
            " Epoch [23/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 121/121 [01:00<00:00,  2.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Training Loss: 3.8360\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 6/6 [00:00<00:00, 11.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Validation Loss: 3.6488\n",
            " Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_23.pt\n",
            "\n",
            " Epoch [24/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 121/121 [00:57<00:00,  2.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Training Loss: 3.7513\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 6/6 [00:00<00:00, 12.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Validation Loss: 3.2991\n",
            " Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_24.pt\n",
            "\n",
            " Epoch [25/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 121/121 [00:56<00:00,  2.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Training Loss: 3.8303\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 6/6 [00:00<00:00, 12.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Validation Loss: 3.5951\n",
            " Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_25.pt\n",
            "\n",
            " Epoch [26/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 121/121 [00:54<00:00,  2.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Training Loss: 3.7781\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 6/6 [00:00<00:00, 11.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Validation Loss: 3.5658\n",
            " Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_26.pt\n",
            "\n",
            " Epoch [27/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 121/121 [00:57<00:00,  2.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Training Loss: 3.7780\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 6/6 [00:00<00:00, 11.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Validation Loss: 3.4334\n",
            " Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_27.pt\n",
            "\n",
            " Epoch [28/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 121/121 [00:54<00:00,  2.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Training Loss: 3.7344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 6/6 [00:00<00:00, 12.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Validation Loss: 3.7232\n",
            " Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_28.pt\n",
            "\n",
            " Epoch [29/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 121/121 [01:00<00:00,  1.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Training Loss: 3.7922\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 6/6 [00:00<00:00, 12.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Validation Loss: 3.2920\n",
            " Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_29.pt\n",
            "\n",
            " Epoch [30/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 121/121 [00:55<00:00,  2.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Training Loss: 3.7795\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 6/6 [00:00<00:00, 12.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Validation Loss: 3.3980\n",
            " Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_30.pt\n"
          ]
        }
      ],
      "source": [
        "from drive.MyDrive.S2VTModel import S2VTModel\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = S2VTModel(\n",
        "    vocab_size=len(vocab),\n",
        "    max_len=45,\n",
        "    dim_hidden=1024,\n",
        "    dim_word=512,\n",
        "    dim_vid=2048,\n",
        "    sos_id=vocab['<SOS>'],\n",
        "    eos_id=vocab['<EOS>'],\n",
        "    n_layers=2,\n",
        "    rnn_cell='lstm',\n",
        "    rnn_dropout_p=0.2\n",
        ")\n",
        "\n",
        "train_model(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    val_dataset=val_dataset,\n",
        "    vocab=vocab,\n",
        "    device=device,\n",
        "    num_epochs=30,\n",
        "    batch_size=8,\n",
        "    learning_rate=1e-4,\n",
        "    checkpoint_dir=\"/content/drive/MyDrive/msvd_split/checkpoints\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge-score\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOxNibH0ZKq1",
        "outputId": "97c84806-843f-4fcf-d519-833377b01831"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge-score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (4.67.1)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=8bb20a4b5f3289096de90e719d4d9a73219f28b09dddfc6bfb2f3b121141716b\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/tylin/coco-caption\n",
        "!cp -r coco-caption/pycocoevalcap /usr/local/lib/python3.*/dist-packages/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVJF7mJFZOXR",
        "outputId": "61f3ea1a-bd42-4e7e-a3d6-79a6e53572c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'coco-caption'...\n",
            "remote: Enumerating objects: 736, done.\u001b[K\n",
            "remote: Total 736 (delta 0), reused 0 (delta 0), pack-reused 736 (from 1)\u001b[K\n",
            "Receiving objects: 100% (736/736), 130.04 MiB | 15.39 MiB/s, done.\n",
            "Resolving deltas: 100% (390/390), done.\n",
            "Updating files: 100% (47/47), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Install dependencies for metrics ===\n",
        "!pip install nltk rouge-score\n",
        "!git clone https://github.com/tylin/coco-caption\n",
        "!pip install git+https://github.com/salaniz/pycocoevalcap\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 723
        },
        "id": "4Yxqkj1uZlod",
        "outputId": "6243ff17-e65e-4519-9b5f-7c10d308f87d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge-score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
            "fatal: destination path 'coco-caption' already exists and is not an empty directory.\n",
            "Collecting git+https://github.com/salaniz/pycocoevalcap\n",
            "  Cloning https://github.com/salaniz/pycocoevalcap to /tmp/pip-req-build-6k2d6tmn\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/salaniz/pycocoevalcap /tmp/pip-req-build-6k2d6tmn\n",
            "  Resolved https://github.com/salaniz/pycocoevalcap to commit a24f74c408c918f1f4ec34e9514bc8a76ce41ffd\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from pycocoevalcap==1.2) (2.0.8)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pycocotools>=2.0.2->pycocoevalcap==1.2) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pycocotools>=2.0.2->pycocoevalcap==1.2) (2.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (4.58.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (1.17.0)\n",
            "Building wheels for collected packages: pycocoevalcap\n",
            "  Building wheel for pycocoevalcap (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycocoevalcap: filename=pycocoevalcap-1.2-py3-none-any.whl size=104312245 sha256=484bf33c8e0b25c47a4b7362cca15f8596a6d77d510b401f75d94653b4fb0ab7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-6m3_nd7r/wheels/d2/1f/44/6485e566f8ae3d42b56e7c05fd50a3bbb70a50b0e6e7c55212\n",
            "Successfully built pycocoevalcap\n",
            "Installing collected packages: pycocoevalcap\n",
            "Successfully installed pycocoevalcap-1.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pycocoevalcap"
                ]
              },
              "id": "9e30b2228cb6409ab54341b719ca8e2c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/tylin/coco-caption\n",
        "!pip install git+https://github.com/salaniz/pycocoevalcap\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-esvOBXBZoWF",
        "outputId": "5df16eb0-0404-4bc4-cc95-41fb46d0bf63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'coco-caption' already exists and is not an empty directory.\n",
            "Collecting git+https://github.com/salaniz/pycocoevalcap\n",
            "  Cloning https://github.com/salaniz/pycocoevalcap to /tmp/pip-req-build-8or2s3ui\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/salaniz/pycocoevalcap /tmp/pip-req-build-8or2s3ui\n",
            "  Resolved https://github.com/salaniz/pycocoevalcap to commit a24f74c408c918f1f4ec34e9514bc8a76ce41ffd\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from pycocoevalcap==1.2) (2.0.8)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pycocotools>=2.0.2->pycocoevalcap==1.2) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pycocotools>=2.0.2->pycocoevalcap==1.2) (2.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (4.58.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from rouge_score import rouge_scorer\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "from pycocoevalcap.cider.cider import Cider\n",
        "\n",
        "def evaluate_all_metrics(pred_path, ref_json_path):\n",
        "    # Load predicted captions\n",
        "    with open(pred_path, 'r') as f:\n",
        "        preds = json.load(f)\n",
        "\n",
        "    # Load ground truth captions\n",
        "    with open(ref_json_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Prepare refs dict: video_id  list of reference captions\n",
        "    refs = defaultdict(list)\n",
        "    for item in data['sentences']:\n",
        "        refs[item['video_id']].append(item['caption'].lower())\n",
        "\n",
        "    # Collect predictions and matching references\n",
        "    gt, pr = [], []\n",
        "    meteor_scores = []\n",
        "    rouge_scores = []\n",
        "    cider_refs = []\n",
        "    cider_hyps = []\n",
        "\n",
        "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "    smooth_fn = SmoothingFunction().method1\n",
        "\n",
        "    for video_id, pred_caption in tqdm(preds.items(), desc=\"Evaluating\"):\n",
        "        pred_caption = pred_caption.lower()\n",
        "        references = refs.get(video_id, [])\n",
        "        if not references:\n",
        "            continue\n",
        "\n",
        "        gt.append([ref.split() for ref in references])\n",
        "        pr.append(pred_caption.split())\n",
        "\n",
        "        meteor_scores.append(meteor_score(references, pred_caption))\n",
        "        rouge_scores.append(scorer.score(' '.join(references), pred_caption)['rougeL'].fmeasure)\n",
        "\n",
        "        cider_refs.append({'image_id': video_id, 'captions': references})\n",
        "        cider_hyps.append({'image_id': video_id, 'caption': pred_caption})\n",
        "\n",
        "    # Compute scores\n",
        "    bleu1 = corpus_bleu(gt, pr, weights=(1, 0, 0, 0), smoothing_function=smooth_fn)\n",
        "    bleu2 = corpus_bleu(gt, pr, weights=(0.5, 0.5, 0, 0), smoothing_function=smooth_fn)\n",
        "    bleu3 = corpus_bleu(gt, pr, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smooth_fn)\n",
        "    bleu4 = corpus_bleu(gt, pr, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smooth_fn)\n",
        "    avg_meteor = sum(meteor_scores) / len(meteor_scores)\n",
        "    avg_rouge = sum(rouge_scores) / len(rouge_scores)\n",
        "    cider_scorer = Cider()\n",
        "    cider_score, _ = cider_scorer.compute_score(cider_refs, cider_hyps)\n",
        "\n",
        "    # Show Results\n",
        "    print(\"\\n Evaluation Results:\")\n",
        "    print(f\"BLEU-1  : {bleu1:.4f}\")\n",
        "    print(f\"BLEU-2  : {bleu2:.4f}\")\n",
        "    print(f\"BLEU-3  : {bleu3:.4f}\")\n",
        "    print(f\"BLEU-4  : {bleu4:.4f}\")\n",
        "    print(f\"METEOR  : {avg_meteor:.4f}\")\n",
        "    print(f\"ROUGE-L : {avg_rouge:.4f}\")\n",
        "    print(f\"CIDEr   : {cider_score:.4f}\")\n"
      ],
      "metadata": {
        "id": "MfqcVjGYzCeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "def generate_predictions(model, dataset, vocab_rev, device):\n",
        "    model.eval()\n",
        "    predictions = {}\n",
        "\n",
        "    for idx in tqdm(range(len(dataset)), desc=\"Generating Captions\"):\n",
        "        video_feats, _ = dataset[idx]\n",
        "        video_feats = video_feats.unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            _, output_seq = model(video_feats, mode='inference')\n",
        "\n",
        "        tokens = output_seq.squeeze().tolist()\n",
        "        caption = []\n",
        "        for tok in tokens:\n",
        "            word = vocab_rev.get(str(tok), '<UNK>')\n",
        "            if word in ['<EOS>', '<PAD>']:\n",
        "                break\n",
        "            if word != '<SOS>':\n",
        "                caption.append(word)\n",
        "\n",
        "        video_id = dataset.video_ids[idx]\n",
        "        predictions[video_id] = ' '.join(caption)\n",
        "\n",
        "    return predictions\n"
      ],
      "metadata": {
        "id": "Lfadh8doaHeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TqhddxycBn9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQMs5sWZ1v8A",
        "outputId": "7b697c39-dcdd-4145-8a7b-6a82e147112f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch [1/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [00:37<00:00,  4.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Training Loss: 4.5071\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 12/12 [00:00<00:00, 14.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Validation Loss: 4.1655\n",
            " Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_1.pt\n",
            "\n",
            " Epoch [2/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [00:38<00:00,  4.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Training Loss: 3.9500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 12/12 [00:00<00:00, 14.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Validation Loss: 3.8747\n",
            " Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_2.pt\n",
            "\n",
            " Epoch [3/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [00:37<00:00,  4.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Training Loss: 3.8790\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 12/12 [00:00<00:00, 14.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Validation Loss: 4.3683\n",
            " Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_3.pt\n",
            "\n",
            " Epoch [4/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [00:38<00:00,  4.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Training Loss: 3.7757\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 12/12 [00:00<00:00, 14.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Validation Loss: 4.1915\n",
            " Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_4.pt\n",
            "\n",
            " Epoch [5/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [00:39<00:00,  4.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Training Loss: 3.6728\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 12/12 [00:00<00:00, 14.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Validation Loss: 3.8768\n",
            " Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_5.pt\n",
            "\n",
            " Epoch [6/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [00:37<00:00,  4.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Training Loss: 3.5943\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 12/12 [00:00<00:00, 14.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Validation Loss: 3.7895\n",
            " Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_6.pt\n",
            "\n",
            " Epoch [7/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [00:38<00:00,  4.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Training Loss: 3.5981\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 12/12 [00:00<00:00, 14.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Validation Loss: 3.5976\n",
            " Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_7.pt\n",
            "\n",
            " Epoch [8/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [00:38<00:00,  4.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Training Loss: 3.4965\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 12/12 [00:00<00:00, 14.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Validation Loss: 3.4450\n",
            " Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_8.pt\n",
            "\n",
            " Epoch [9/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [00:38<00:00,  4.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Training Loss: 3.4422\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 12/12 [00:00<00:00, 13.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Validation Loss: 3.5100\n",
            " Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_9.pt\n",
            "\n",
            " Epoch [10/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [00:37<00:00,  4.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Training Loss: 3.4471\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 12/12 [00:00<00:00, 14.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Validation Loss: 3.7160\n",
            " Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_10.pt\n",
            "\n",
            " Epoch [11/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [00:45<00:00,  3.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Training Loss: 3.3544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 12/12 [00:00<00:00, 14.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Validation Loss: 3.5924\n",
            " Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_11.pt\n",
            "\n",
            " Epoch [12/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [00:38<00:00,  4.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Training Loss: 3.4078\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 12/12 [00:00<00:00, 14.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Validation Loss: 3.4277\n",
            " Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_12.pt\n",
            "\n",
            " Epoch [13/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [00:38<00:00,  4.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Training Loss: 3.3239\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 12/12 [00:00<00:00, 14.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Validation Loss: 3.4064\n",
            " Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_13.pt\n",
            "\n",
            " Epoch [14/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [00:37<00:00,  4.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Training Loss: 3.3630\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 12/12 [00:00<00:00, 14.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Validation Loss: 3.3922\n",
            " Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_14.pt\n",
            "\n",
            " Epoch [15/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [00:37<00:00,  4.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Training Loss: 3.3459\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 12/12 [00:00<00:00, 14.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Validation Loss: 3.5659\n",
            " Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_15.pt\n",
            "\n",
            " Epoch [16/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [00:38<00:00,  4.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Training Loss: 3.2915\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 12/12 [00:00<00:00, 13.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Validation Loss: 3.5713\n",
            " Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_16.pt\n",
            "\n",
            " Epoch [17/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [00:38<00:00,  4.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Training Loss: 3.2815\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 12/12 [00:00<00:00, 14.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Validation Loss: 3.5616\n",
            " Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_17.pt\n",
            "\n",
            " Epoch [18/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [00:38<00:00,  4.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Training Loss: 3.2674\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 12/12 [00:00<00:00, 14.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Validation Loss: 3.6604\n",
            " Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_18.pt\n",
            "\n",
            " Epoch [19/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [00:37<00:00,  4.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Training Loss: 3.2831\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 12/12 [00:00<00:00, 14.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Validation Loss: 3.4520\n",
            " Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_19.pt\n",
            "\n",
            " Epoch [20/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [00:37<00:00,  4.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Training Loss: 3.2817\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 12/12 [00:00<00:00, 14.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Validation Loss: 3.5978\n",
            " Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_20.pt\n",
            "\n",
            " Epoch [21/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [00:38<00:00,  4.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Training Loss: 3.2260\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 12/12 [00:00<00:00, 13.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Validation Loss: 3.4773\n",
            " Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_21.pt\n",
            "\n",
            " Epoch [22/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [00:38<00:00,  4.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Training Loss: 3.1978\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 12/12 [00:00<00:00, 13.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Validation Loss: 3.3894\n",
            " Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_22.pt\n",
            "\n",
            " Epoch [23/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [00:38<00:00,  4.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Training Loss: 3.2879\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 12/12 [00:00<00:00, 14.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Validation Loss: 3.3571\n",
            " Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_23.pt\n",
            "\n",
            " Epoch [24/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [00:38<00:00,  4.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Training Loss: 3.2479\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 12/12 [00:00<00:00, 14.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Validation Loss: 3.6071\n",
            " Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_24.pt\n",
            "\n",
            " Epoch [25/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [00:38<00:00,  4.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Training Loss: 3.2296\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 12/12 [00:00<00:00, 14.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Validation Loss: 3.2628\n",
            " Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_25.pt\n",
            "\n",
            " Epoch [26/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [00:37<00:00,  4.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Training Loss: 3.1683\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 12/12 [00:00<00:00, 14.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Validation Loss: 3.4404\n",
            " Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_26.pt\n",
            "\n",
            " Epoch [27/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [00:37<00:00,  4.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Training Loss: 3.2643\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 12/12 [00:00<00:00, 14.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Validation Loss: 3.4990\n",
            " Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_27.pt\n",
            "\n",
            " Epoch [28/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [00:38<00:00,  4.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Training Loss: 3.2574\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 12/12 [00:00<00:00, 14.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Validation Loss: 3.3379\n",
            " Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_28.pt\n",
            "\n",
            " Epoch [29/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [00:38<00:00,  4.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Training Loss: 3.1511\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 12/12 [00:00<00:00, 14.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Validation Loss: 3.5642\n",
            " Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_29.pt\n",
            "\n",
            " Epoch [30/30]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [00:38<00:00,  4.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Training Loss: 3.2400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 12/12 [00:00<00:00, 14.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Validation Loss: 3.4547\n",
            " Saved checkpoint to /content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_30.pt\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "checkpoint_path = \"/content/drive/MyDrive/msvd_split/checkpoints\"\n",
        "os.makedirs(checkpoint_path, exist_ok=True)\n",
        "\n",
        "# Then pass this to training\n",
        "train_model(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    val_dataset=val_dataset,\n",
        "    vocab=vocab,\n",
        "    device=device,\n",
        "    num_epochs=30,\n",
        "    batch_size=8,\n",
        "    learning_rate=1e-4,\n",
        "    checkpoint_dir=checkpoint_path\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/vrama91/cider"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bgtqjE6KPCY",
        "outputId": "88d98a7f-6c95-4c84-b62f-7578e377d18d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/vrama91/cider\n",
            "  Cloning https://github.com/vrama91/cider to /tmp/pip-req-build-dam9k5rl\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/vrama91/cider /tmp/pip-req-build-dam9k5rl\n",
            "  Resolved https://github.com/vrama91/cider to commit f281464c60d496b8fabb89a7a8c120f655a3a2bd\n",
            "\u001b[31mERROR: git+https://github.com/vrama91/cider does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Install pycocoevalcap from working fork\n",
        "!git clone https://github.com/salaniz/pycocoevalcap.git\n",
        "!pip install -e ./pycocoevalcap\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qdq9s_oYKPNH",
        "outputId": "9956fa38-dac1-4052-c36d-c3872587464a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pycocoevalcap'...\n",
            "remote: Enumerating objects: 821, done.\u001b[K\n",
            "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 821 (delta 4), reused 3 (delta 3), pack-reused 809 (from 2)\u001b[K\n",
            "Receiving objects: 100% (821/821), 130.06 MiB | 25.70 MiB/s, done.\n",
            "Resolving deltas: 100% (424/424), done.\n",
            "Obtaining file:///content/pycocoevalcap\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from pycocoevalcap==1.2) (2.0.8)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pycocotools>=2.0.2->pycocoevalcap==1.2) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pycocotools>=2.0.2->pycocoevalcap==1.2) (2.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (4.58.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (1.17.0)\n",
            "Installing collected packages: pycocoevalcap\n",
            "  Running setup.py develop for pycocoevalcap\n",
            "Successfully installed pycocoevalcap-1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pycocoevalcap.cider.cider import Cider\n"
      ],
      "metadata": {
        "id": "1_Kg87GsKPQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, torch, torch.nn as nn, torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from nltk.translate.meteor_score import single_meteor_score\n",
        "from pycocoevalcap.cider.cider import Cider"
      ],
      "metadata": {
        "id": "mVkU3ImBKPTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XbPLAUvkKmQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "439374a7"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class S2VTModel(nn.Module):\n",
        "    def __init__(self, vocab_size, max_len=45, dim_hidden=1024, dim_word=512, dim_vid=2048,\n",
        "                 sos_id=1, eos_id=0, n_layers=1, rnn_cell='lstm', rnn_dropout_p=0.3):\n",
        "        super(S2VTModel, self).__init__()\n",
        "\n",
        "        self.rnn_cell_type = rnn_cell.lower()\n",
        "        self.rnn_cell = nn.LSTM if self.rnn_cell_type == 'lstm' else nn.GRU\n",
        "\n",
        "        self.rnn1 = self.rnn_cell(dim_vid, dim_hidden, n_layers, batch_first=True, dropout=rnn_dropout_p)\n",
        "        self.rnn2 = self.rnn_cell(dim_hidden + dim_word, dim_hidden, n_layers, batch_first=True, dropout=rnn_dropout_p)\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, dim_word)\n",
        "        self.out = nn.Linear(dim_hidden, vocab_size)\n",
        "\n",
        "        self.dim_vid = dim_vid\n",
        "        self.dim_hidden = dim_hidden\n",
        "        self.dim_word = dim_word\n",
        "        self.max_length = max_len\n",
        "        self.vocab_size = vocab_size\n",
        "        self.sos_id = sos_id\n",
        "        self.eos_id = eos_id\n",
        "\n",
        "    def forward(self, vid_feats, target_variable=None, mode='train', beam_width=3):\n",
        "        batch_size, n_frames, _ = vid_feats.shape\n",
        "        device = vid_feats.device\n",
        "\n",
        "        encoder_outputs, state1 = self.rnn1(vid_feats)\n",
        "\n",
        "        if mode == 'train':\n",
        "            seq_probs = []\n",
        "            for t in range(self.max_length - 1):\n",
        "                current_word = self.embedding(target_variable[:, t])\n",
        "                input2 = torch.cat((encoder_outputs[:, -1], current_word), dim=1).unsqueeze(1)\n",
        "                output2, state2 = self.rnn2(input2, state1)\n",
        "                logits = self.out(output2.squeeze(1))\n",
        "                logits = F.log_softmax(logits, dim=1)\n",
        "                seq_probs.append(logits.unsqueeze(1))\n",
        "                state1 = state2\n",
        "            return torch.cat(seq_probs, dim=1), None\n",
        "\n",
        "        else:\n",
        "            generated = torch.full((batch_size, 1), self.sos_id, dtype=torch.long, device=device)\n",
        "            seqs = []\n",
        "            for t in range(self.max_length - 1):\n",
        "                emb = self.embedding(generated[:, -1])\n",
        "                input2 = torch.cat((encoder_outputs[:, -1], emb), dim=1).unsqueeze(1)\n",
        "                output2, state1 = self.rnn2(input2, state1)\n",
        "                logits = self.out(output2.squeeze(1))\n",
        "                next_word = torch.argmax(F.log_softmax(logits, dim=1), dim=1).unsqueeze(1)\n",
        "                generated = torch.cat([generated, next_word], dim=1)\n",
        "            return None, generated\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dlv_p2n7Kppb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ed7e609"
      },
      "outputs": [],
      "source": [
        "class VideoCaptionDataset(Dataset):\n",
        "    def __init__(self, feature_dir, json_path, vocab, max_caption_length=45):\n",
        "        self.feature_dir = feature_dir\n",
        "        self.vocab = vocab\n",
        "        self.max_caption_length = max_caption_length\n",
        "        with open(json_path, 'r') as f:\n",
        "            self.annotations = json.load(f)['annotations']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.annotations[idx]\n",
        "        feature_path = os.path.join(self.feature_dir, item['video_id'] + '.npy')\n",
        "        features = torch.tensor(np.load(feature_path)).float()\n",
        "        caption = [self.vocab.get(tok, self.vocab['<UNK>']) for tok in item['caption']]\n",
        "        caption = torch.tensor(caption[:self.max_caption_length])\n",
        "        return features, caption\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CM0h_zQAKtVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5095d485"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, device, vocab, num_epochs=10, lr=1e-4):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.NLLLoss(ignore_index=vocab['<PAD>'])\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for video_feats, captions in train_loader:\n",
        "            video_feats, captions = video_feats.to(device), captions.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs, _ = model(video_feats, captions)\n",
        "            loss = criterion(outputs.view(-1, outputs.size(-1)), captions[:, 1:].reshape(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1} | Loss: {total_loss / len(train_loader):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4f_ykqRmKwrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57c51bb7"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, test_loader, vocab, device):\n",
        "    vocab_rev = {v: k for k, v in vocab.items()}\n",
        "    all_hypotheses, all_references = [], []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for video_feats, captions in test_loader:\n",
        "            video_feats, captions = video_feats.to(device), captions.to(device)\n",
        "            _, predicted_ids = model(video_feats, mode='inference')\n",
        "            pred_tokens = [vocab_rev.get(int(tok), '<UNK>') for tok in predicted_ids[0] if vocab_rev.get(int(tok), '<UNK>') not in ['<SOS>', '<PAD>', '<EOS>']]\n",
        "            ref_tokens = [vocab_rev.get(int(tok), '<UNK>') for tok in captions[0] if vocab_rev.get(int(tok), '<UNK>') not in ['<SOS>', '<PAD>', '<EOS>']]\n",
        "            all_hypotheses.append(pred_tokens)\n",
        "            all_references.append([ref_tokens])\n",
        "    bleu = corpus_bleu(all_references, all_hypotheses)\n",
        "    meteor_scores = [single_meteor_score(\" \".join(ref[0]), \" \".join(hyp)) for ref, hyp in zip(all_references, all_hypotheses)]\n",
        "    cider_score, _ = Cider().compute_score({i:[\" \".join(r[0])] for i,r in enumerate(all_references)}, {i:[\" \".join(h)] for i,h in enumerate(all_hypotheses)})\n",
        "    print(f\"BLEU-4: {bleu:.4f}\\nMETEOR: {sum(meteor_scores)/len(meteor_scores):.4f}\\nCIDEr: {cider_score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(train_json_path, 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "print(data.keys())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ub2SfVmrLFuD",
        "outputId": "e8033e5a-e5ac-4750-95b9-532e3fec4f50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['videos', 'sentences'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QxeSsR9mKz0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1638a6c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "d1f538ce-9afc-48f3-85fb-c1226e4e7e40"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "stack expects each tensor to be equal size, but got [42] at entry 0 and [21] at entry 1",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-c8c772f43d1b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-a09963041ded>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, device, vocab, num_epochs, lr)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mvideo_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0mvideo_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo_feats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Handle `CustomType` automatically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m     \"\"\"\n\u001b[0;32m--> 398\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_collate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             return [\n\u001b[0m\u001b[1;32m    212\u001b[0m                 \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             return [\n\u001b[0;32m--> 212\u001b[0;31m                 \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             ]  # Backwards compatibility.\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcollate_fn_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0melem_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcollate_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_typed_storage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [42] at entry 0 and [21] at entry 1"
          ]
        }
      ],
      "source": [
        "# === Sample Usage ===\n",
        "# Update these paths as needed\n",
        "vocab_path = '/content/drive/MyDrive/msvd_split/vocab.json'\n",
        "train_json_path = '/content/drive/MyDrive/msvd_split/train/train_captions.json'\n",
        "val_json_path = '/content/drive/MyDrive/msvd_split/val/val_captions.json'\n",
        "test_json_path = '/content/drive/MyDrive/msvd_split/test/test_captions.json'\n",
        "train_feat_dir = '/content/drive/MyDrive/msvd_split/train/features'\n",
        "val_feat_dir = '/content/drive/MyDrive/msvd_split/val/features'\n",
        "test_feat_dir = '/content/drive/MyDrive/msvd_split/test/features'\n",
        "\n",
        "with open(vocab_path, 'r') as f:\n",
        "    vocab = json.load(f)\n",
        "\n",
        "train_dataset = VideoCaptionDataset(train_feat_dir, train_json_path, vocab)\n",
        "val_dataset = VideoCaptionDataset(val_feat_dir, val_json_path, vocab)\n",
        "test_dataset = VideoCaptionDataset(test_feat_dir, test_json_path, vocab)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = S2VTModel(\n",
        "    vocab_size=len(vocab),\n",
        "    max_len=45,\n",
        "    dim_hidden=512,\n",
        "    dim_word=256,\n",
        "    dim_vid=1024,\n",
        "    sos_id=vocab['<SOS>'],\n",
        "    eos_id=vocab['<EOS>'],\n",
        "    n_layers=1,\n",
        "    rnn_cell='lstm',\n",
        "    rnn_dropout_p=0.2\n",
        ")\n",
        "\n",
        "train_model(model, train_loader, val_loader, device, vocab, num_epochs=30)\n",
        "evaluate_model(model, test_loader, vocab, device)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class VideoCaptionDataset(Dataset):\n",
        "    def __init__(self, feature_dir, json_path, vocab, max_caption_length=45):\n",
        "        self.feature_dir = feature_dir\n",
        "        self.vocab = vocab\n",
        "        self.max_caption_length = max_caption_length\n",
        "\n",
        "        with open(json_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        video_id_to_caption = {}\n",
        "        for sent in data['sentences']:\n",
        "            vid = sent['video_id']\n",
        "            caption = sent['caption']\n",
        "            if vid not in video_id_to_caption:\n",
        "                video_id_to_caption[vid] = []\n",
        "            video_id_to_caption[vid].append(caption)\n",
        "\n",
        "        self.annotations = []\n",
        "        for video in data['videos']:\n",
        "            vid = video['video_id']\n",
        "            if vid in video_id_to_caption:\n",
        "                for cap in video_id_to_caption[vid]:\n",
        "                    self.annotations.append({'video_id': vid, 'caption': cap})\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.annotations[idx]\n",
        "        feature_path = os.path.join(self.feature_dir, item['video_id'] + '.npy')\n",
        "        features = torch.tensor(np.load(feature_path)).float()\n",
        "        caption = [self.vocab.get(tok, self.vocab['<UNK>']) for tok in item['caption']]\n",
        "        caption = torch.tensor(caption[:self.max_caption_length])\n",
        "        return features, caption\n"
      ],
      "metadata": {
        "id": "wzqOrDaLLbcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q5GTsosVKPZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-amwD-66BtOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "Mpr4DSIosEK0",
        "outputId": "b11c233c-e8ff-4bf6-b532-4e2a88090afb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for S2VTModel:\n\tUnexpected key(s) in state_dict: \"rnn1.weight_ih_l1\", \"rnn1.weight_hh_l1\", \"rnn1.bias_ih_l1\", \"rnn1.bias_hh_l1\", \"rnn2.weight_ih_l1\", \"rnn2.weight_hh_l1\", \"rnn2.bias_ih_l1\", \"rnn2.bias_hh_l1\". ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-15f0dd8714e1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2581\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   2582\u001b[0m                 \"Error(s) in loading state_dict for {}:\\n\\t{}\".format(\n\u001b[1;32m   2583\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\\t\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for S2VTModel:\n\tUnexpected key(s) in state_dict: \"rnn1.weight_ih_l1\", \"rnn1.weight_hh_l1\", \"rnn1.bias_ih_l1\", \"rnn1.bias_hh_l1\", \"rnn2.weight_ih_l1\", \"rnn2.weight_hh_l1\", \"rnn2.bias_ih_l1\", \"rnn2.bias_hh_l1\". "
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from drive.MyDrive.S2VTModel import S2VTModel\n",
        "\n",
        "# === Paths ===\n",
        "vocab_path = '/content/drive/MyDrive/msvd_split/vocab.json'\n",
        "test_json_path = '/content/drive/MyDrive/msvd_split/test/test_captions.json'\n",
        "test_feature_dir = '/content/drive/MyDrive/msvd_split/test/features'\n",
        "checkpoint_path = '/content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_30.pt'\n",
        "\n",
        "# === Load vocab ===\n",
        "with open(vocab_path, 'r') as f:\n",
        "    vocab = json.load(f)\n",
        "vocab_rev = {v: k for k, v in vocab.items()}\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# === Load model (must match training config) ===\n",
        "model = S2VTModel(\n",
        "    vocab_size=vocab_size,\n",
        "    max_len=45,\n",
        "    dim_hidden=1024,\n",
        "    dim_word=512,\n",
        "    dim_vid=2048,\n",
        "    sos_id=vocab['<SOS>'],\n",
        "    eos_id=vocab['<EOS>'],\n",
        "    n_layers=1,\n",
        "    rnn_cell='lstm',\n",
        "    rnn_dropout_p=0.3\n",
        ")\n",
        "\n",
        "# === Load checkpoint ===\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# === Load test data ===\n",
        "test_dataset = VideoCaptionDataset(test_feature_dir, test_json_path, vocab, max_caption_length=45)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# === Inference + BLEU computation ===\n",
        "print(\"Evaluating on test set...\")\n",
        "\n",
        "all_references = []\n",
        "all_hypotheses = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for idx, (video_feats, captions) in enumerate(test_loader):\n",
        "        video_feats, captions = video_feats.to(device), captions.to(device)\n",
        "\n",
        "        # Model inference\n",
        "        _, predicted_ids = model(video_feats, mode='inference')  # [1, T]\n",
        "\n",
        "        # Decode generated caption\n",
        "        pred_tokens = []\n",
        "        for tok in predicted_ids[0]:\n",
        "            word = vocab_rev.get(int(tok), '<UNK>')\n",
        "            if word == '<EOS>':\n",
        "                break\n",
        "            if word not in ['<SOS>', '<PAD>']:\n",
        "                pred_tokens.append(word)\n",
        "        all_hypotheses.append(pred_tokens)\n",
        "\n",
        "        # Decode ground truth caption\n",
        "        ref_tokens = []\n",
        "        for tok in captions[0]:\n",
        "            word = vocab_rev.get(int(tok), '<UNK>')\n",
        "            if word == '<EOS>':\n",
        "                break\n",
        "            if word not in ['<SOS>', '<PAD>']:\n",
        "                ref_tokens.append(word)\n",
        "        all_references.append([ref_tokens])  # list of references per hypothesis\n",
        "\n",
        "        if idx < 5:\n",
        "            print(f\"\\nExample {idx + 1}\")\n",
        "            print(f\"Predicted    : {' '.join(pred_tokens)}\")\n",
        "            print(f\"Ground Truth : {' '.join(ref_tokens)}\")\n",
        "\n",
        "# === Compute BLEU score ===\n",
        "bleu_score = corpus_bleu(all_references, all_hypotheses)\n",
        "print(f\"\\nFinal BLEU-4 Score: {bleu_score:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xPrXg-FqJh9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ue4t55HiBw0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qW4sqh6zBxAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zd-G7wJsBxDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "szFaxyLOBxGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ntvohxNdBxJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kn3ByhCr8_pB",
        "outputId": "fe15ff9e-90b9-4a84-9498-6482c6e39b55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/sliders.py:61: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  if event.key is 'enter':\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import subprocess\n",
        "import tempfile\n",
        "import glob\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "from moviepy.editor import VideoFileClip\n",
        "\n",
        "video_path = '/content/drive/MyDrive/YouTubeClips/-8y1Q0rA3n8_108_115.avi'\n",
        "\n",
        "def extract_frames_ffmpeg(video_path, output_dir, frame_rate=1):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    cmd = [\n",
        "        'ffmpeg', '-i', video_path, '-vf', f\"fps={frame_rate},scale=224:224\",\n",
        "        os.path.join(output_dir, '%06d.jpg'), '-hide_banner', '-loglevel', 'error'\n",
        "    ]\n",
        "    subprocess.run(cmd)\n",
        "\n",
        "def load_and_sample_frames(frame_dir, n_frames=40):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    frames = sorted(glob.glob(os.path.join(frame_dir, '*.jpg')))\n",
        "    total = len(frames)\n",
        "    if total == 0:\n",
        "        raise ValueError(\"No frames extracted\")\n",
        "\n",
        "    indices = np.linspace(0, total - 1, min(n_frames, total)).astype(int)\n",
        "    sampled = [frames[i] for i in indices]\n",
        "\n",
        "    images = [transform(Image.open(f).convert('RGB')) for f in sampled]\n",
        "    return torch.stack(images)  # [T, 3, H, W]\n",
        "\n",
        "def extract_video_tensor(video_path, n_frames=40):\n",
        "    with tempfile.TemporaryDirectory() as tmpdir:\n",
        "        extract_frames_ffmpeg(video_path, tmpdir)\n",
        "        tensor = load_and_sample_frames(tmpdir, n_frames)\n",
        "    return tensor\n",
        "\n",
        "def generate_caption_from_video(video_path, model, feature_extractor, vocab_rev, device, max_len=45):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    video_tensor = extract_video_tensor(video_path).to(device)  # [T, 3, 224, 224]\n",
        "    with torch.no_grad():\n",
        "        features = feature_extractor(video_tensor)  # e.g. ResNet: [T, 2048]\n",
        "        features = features.unsqueeze(0)  # Add batch dim: [1, T, 2048]\n",
        "        _, predicted_ids = model(features, mode='inference')\n",
        "\n",
        "    tokens = []\n",
        "    for idx in predicted_ids[0]:\n",
        "        word = vocab_rev.get(int(idx), '<UNK>')\n",
        "        if word == '<EOS>':\n",
        "            break\n",
        "        if word not in ['<SOS>', '<PAD>']:\n",
        "            tokens.append(word)\n",
        "\n",
        "    return ' '.join(tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QSwbZ8oYBx5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install pretrainedmodels for ResNet152\n",
        "!pip install pretrainedmodels\n",
        "\n",
        "# Upload a video (e.g., example_video.avi)\n",
        "from google.colab import files\n",
        "uploaded = files.upload()  # upload your .avi file\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 579
        },
        "id": "dNqmO1budQHS",
        "outputId": "9f1a0c26-da50-404d-ba30-7b85915d8961"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pretrainedmodels in /usr/local/lib/python3.11/dist-packages (0.7.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from pretrainedmodels) (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from pretrainedmodels) (0.20.1+cu124)\n",
            "Requirement already satisfied: munch in /usr/local/lib/python3.11/dist-packages (from pretrainedmodels) (4.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from pretrainedmodels) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->pretrainedmodels) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->pretrainedmodels) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->pretrainedmodels) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->pretrainedmodels) (3.0.2)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ddf8a0fb-7eb8-4e92-b1b8-e8988913ea68\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ddf8a0fb-7eb8-4e92-b1b8-e8988913ea68\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving aa.mp4 to aa.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aH78Ya89NIyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pretrainedmodels\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_ptiFI4NI_2",
        "outputId": "a3cd1bc9-e8b2-476b-b34d-63412d075d79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pretrainedmodels in /usr/local/lib/python3.11/dist-packages (0.7.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from pretrainedmodels) (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from pretrainedmodels) (0.20.1+cu124)\n",
            "Requirement already satisfied: munch in /usr/local/lib/python3.11/dist-packages (from pretrainedmodels) (4.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from pretrainedmodels) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->pretrainedmodels) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->pretrainedmodels) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->pretrainedmodels) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->pretrainedmodels) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0w7AVDgqm962"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_CP_BNMwnAPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h5W5uJ7WnAR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import subprocess\n",
        "\n",
        "video_path = '/content/t1.mp4'\n",
        "frame_dir = '/content/frames_t1mp4'\n",
        "\n",
        "if os.path.exists(frame_dir):\n",
        "    shutil.rmtree(frame_dir)\n",
        "os.makedirs(frame_dir, exist_ok=True)\n",
        "\n",
        "!ffmpeg -i {video_path} -vf \"scale=400:300\" -qscale:v 2 {frame_dir}/%06d.jpg\n"
      ],
      "metadata": {
        "id": "S9PB72UKnAVe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f018b17-b635-4a37-bc55-651ffc35d4ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
            "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 70.100 / 56. 70.100\n",
            "  libavcodec     58.134.100 / 58.134.100\n",
            "  libavformat    58. 76.100 / 58. 76.100\n",
            "  libavdevice    58. 13.100 / 58. 13.100\n",
            "  libavfilter     7.110.100 /  7.110.100\n",
            "  libswscale      5.  9.100 /  5.  9.100\n",
            "  libswresample   3.  9.100 /  3.  9.100\n",
            "  libpostproc    55.  9.100 / 55.  9.100\n",
            "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '/content/t1.mp4':\n",
            "  Metadata:\n",
            "    major_brand     : mp42\n",
            "    minor_version   : 0\n",
            "    compatible_brands: mp42isom\n",
            "  Duration: 00:00:04.16, start: 0.000000, bitrate: 1739 kb/s\n",
            "  Stream #0:0(und): Video: h264 (High) (avc1 / 0x31637661), yuv420p(tv, bt709), 848x480, 1504 kb/s, 20.07 fps, 20 tbr, 30k tbn, 60k tbc (default)\n",
            "    Metadata:\n",
            "      vendor_id       : [0][0][0][0]\n",
            "  Stream #0:1(und): Audio: aac (LC) (mp4a / 0x6134706D), 48000 Hz, stereo, fltp, 256 kb/s (default)\n",
            "    Metadata:\n",
            "      vendor_id       : [0][0][0][0]\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (h264 (native) -> mjpeg (native))\n",
            "Press [q] to stop, [?] for help\n",
            "\u001b[1;34m[swscaler @ 0x596459540c80] \u001b[0m\u001b[0;33mdeprecated pixel format used, make sure you did set range correctly\n",
            "\u001b[0mOutput #0, image2, to '/content/frames_t1mp4/%06d.jpg':\n",
            "  Metadata:\n",
            "    major_brand     : mp42\n",
            "    minor_version   : 0\n",
            "    compatible_brands: mp42isom\n",
            "    encoder         : Lavf58.76.100\n",
            "  Stream #0:0(und): Video: mjpeg, yuvj420p(pc, bt709, progressive), 400x300, q=2-31, 200 kb/s, 20 fps, 20 tbn (default)\n",
            "    Metadata:\n",
            "      vendor_id       : [0][0][0][0]\n",
            "      encoder         : Lavc58.134.100 mjpeg\n",
            "    Side data:\n",
            "      cpb: bitrate max/min/avg: 0/0/200000 buffer size: 0 vbv_delay: N/A\n",
            "frame=    1 fps=0.0 q=0.0 size=N/A time=00:00:00.00 bitrate=N/A speed=   0x    \rframe=   82 fps=0.0 q=2.0 Lsize=N/A time=00:00:04.10 bitrate=N/A speed=14.6x    \n",
            "video:822kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: unknown\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pretrainedmodels\n",
        "from pretrainedmodels import utils as pmutils\n",
        "\n",
        "# Load pretrained ResNet-152 and remove classification layer\n",
        "resnet = pretrainedmodels.resnet152(pretrained='imagenet')\n",
        "resnet.last_linear = pmutils.Identity()\n",
        "resnet = resnet.eval().cuda()\n",
        "\n",
        "# Load image preprocessing function\n",
        "load_image_fn = pmutils.LoadTransformImage(resnet)\n"
      ],
      "metadata": {
        "id": "NQ6DPP6AQx9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pretrainedmodels\n",
        "from pretrainedmodels import utils as pmutils\n",
        "\n",
        "resnet = pretrainedmodels.resnet152(pretrained='imagenet')\n",
        "resnet.last_linear = pmutils.Identity()\n",
        "resnet = resnet.eval().cuda()\n",
        "load_image_fn = pmutils.LoadTransformImage(resnet)\n"
      ],
      "metadata": {
        "id": "8bRQpBixQ5qs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "frame_dir = '/content/frames_t1mp4'\n",
        "frame_list = sorted(glob.glob(os.path.join(frame_dir, '*.jpg')))\n",
        "selected_indices = np.linspace(0, len(frame_list) - 1, 40, dtype=int)\n",
        "selected_frames = [frame_list[i] for i in selected_indices]\n",
        "\n",
        "# Feature extraction\n",
        "C, H, W = 3, 224, 224\n",
        "features = torch.zeros((len(selected_frames), C, H, W)).cuda()\n",
        "for i, frame_path in enumerate(selected_frames):\n",
        "    img = load_image_fn(frame_path)\n",
        "    features[i] = img\n",
        "\n",
        "with torch.no_grad():\n",
        "    video_feats = resnet(features).unsqueeze(0).cpu()  # [1, 40, 2048]\n"
      ],
      "metadata": {
        "id": "-XVx_g-JQ6im"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hhaT4yDYQyEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "frame_dir = '/content/frames_t1mp4'  #  ADD THIS LINE\n",
        "\n",
        "# Select 40 frames\n",
        "frame_list = sorted(glob.glob(os.path.join(frame_dir, '*.jpg')))\n",
        "selected_indices = np.linspace(0, len(frame_list) - 1, 40, dtype=int)\n",
        "selected_frames = [frame_list[i] for i in selected_indices]\n",
        "\n",
        "# Extract features\n",
        "C, H, W = 3, 224, 224\n",
        "features = torch.zeros((len(selected_frames), C, H, W)).cuda()\n",
        "for i, frame_path in enumerate(selected_frames):\n",
        "    img = load_image_fn(frame_path)\n",
        "    features[i] = img\n",
        "\n",
        "with torch.no_grad():\n",
        "    video_feats = resnet(features).unsqueeze(0).cpu()  # [1, 40, 2048]\n"
      ],
      "metadata": {
        "id": "BYwqr2UaVES3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from drive.MyDrive.S2VTModel_Attention import S2VTModel\n",
        "\n",
        "# Load vocab\n",
        "vocab_path = '/content/drive/MyDrive/msvd_split/vocab.json'\n",
        "with open(vocab_path, 'r') as f:\n",
        "    vocab = json.load(f)\n",
        "vocab_rev = {v: k for k, v in vocab.items()}\n",
        "\n",
        "# Load model\n",
        "model = S2VTModel(\n",
        "    vocab_size=len(vocab),\n",
        "    max_len=45,\n",
        "    dim_hidden=1024,\n",
        "    dim_word=512,\n",
        "    dim_vid=2048,\n",
        "    sos_id=vocab['<SOS>'],\n",
        "    eos_id=vocab['<EOS>'],\n",
        "    rnn_cell='lstm',\n",
        "    rnn_dropout_p=0.3\n",
        ")\n",
        "\n",
        "checkpoint_path = '/content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_30.pt'\n",
        "checkpoint = torch.load(checkpoint_path, map_location='cuda')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model = model.eval().cuda()\n"
      ],
      "metadata": {
        "id": "AThfjvQdUlJs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4255206f-0a6a-47b9-90b1-432ec31572d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:<ipython-input-30-b2bf01ccfd60>:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path, map_location='cuda')\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "video_feats = video_feats.cuda()\n",
        "\n",
        "with torch.no_grad():\n",
        "    _, predicted_ids = model(video_feats, mode='inference')\n",
        "\n",
        "# Decode prediction\n",
        "caption = []\n",
        "for tok in predicted_ids[0]:\n",
        "    word = vocab_rev.get(int(tok), '<UNK>')\n",
        "    if word == '<EOS>':\n",
        "        break\n",
        "    if word not in ['<SOS>', '<PAD>']:\n",
        "        caption.append(word)\n",
        "\n",
        "print(\" Caption for 04.mp4:\\n\", ' '.join(caption))\n"
      ],
      "metadata": {
        "id": "nlIiO5xJUqwl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20bb591e-cdfc-4b64-e366-83af0e0baa2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Caption for 04.mp4:\n",
            " a woman is slicing a potato\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i2k5R6WMR4w7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OX2QyawhRzs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vWUzEl5-RzwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HTA5mAqMRzzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#avi video\n",
        "\n",
        "# Step 1: Extract frames from AVI video\n",
        "import os\n",
        "import shutil\n",
        "import subprocess\n",
        "\n",
        "video_path = '/content/_9iG5Ge01PM_3_11.avi'  #  Change filename if needed\n",
        "frame_dir = '/content/frames_your_video'\n",
        "\n",
        "if os.path.exists(frame_dir):\n",
        "    shutil.rmtree(frame_dir)\n",
        "os.makedirs(frame_dir, exist_ok=True)\n",
        "\n",
        "!ffmpeg -i \"$video_path\" -vf \"scale=400:300\" -qscale:v 2 \"$frame_dir/%06d.jpg\"\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7R-921qRz2H",
        "outputId": "8019f6e4-e385-497e-a686-d2f833ff7953"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
            "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 70.100 / 56. 70.100\n",
            "  libavcodec     58.134.100 / 58.134.100\n",
            "  libavformat    58. 76.100 / 58. 76.100\n",
            "  libavdevice    58. 13.100 / 58. 13.100\n",
            "  libavfilter     7.110.100 /  7.110.100\n",
            "  libswscale      5.  9.100 /  5.  9.100\n",
            "  libswresample   3.  9.100 /  3.  9.100\n",
            "  libpostproc    55.  9.100 / 55.  9.100\n",
            "Input #0, avi, from '/content/_9iG5Ge01PM_3_11.avi':\n",
            "  Metadata:\n",
            "    software        : MEncoder SVN-r33477-4.2.1\n",
            "  Duration: 00:00:08.04, start: 0.000000, bitrate: 122 kb/s\n",
            "  Stream #0:0: Video: h264 (Main) (H264 / 0x34363248), yuv420p(progressive), 320x240 [SAR 1:1 DAR 4:3], 113 kb/s, 25 fps, 25 tbr, 25 tbn, 50 tbc\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (h264 (native) -> mjpeg (native))\n",
            "Press [q] to stop, [?] for help\n",
            "\u001b[1;34m[swscaler @ 0x5a4b494c8600] \u001b[0m\u001b[0;33mdeprecated pixel format used, make sure you did set range correctly\n",
            "\u001b[0mOutput #0, image2, to '/content/frames_your_video/%06d.jpg':\n",
            "  Metadata:\n",
            "    software        : MEncoder SVN-r33477-4.2.1\n",
            "    encoder         : Lavf58.76.100\n",
            "  Stream #0:0: Video: mjpeg, yuvj420p(pc, progressive), 400x300 [SAR 1:1 DAR 4:3], q=2-31, 200 kb/s, 25 fps, 25 tbn\n",
            "    Metadata:\n",
            "      encoder         : Lavc58.134.100 mjpeg\n",
            "    Side data:\n",
            "      cpb: bitrate max/min/avg: 0/0/200000 buffer size: 0 vbv_delay: N/A\n",
            "frame=    1 fps=0.0 q=0.0 size=N/A time=00:00:00.00 bitrate=N/A speed=   0x    \rframe=  201 fps=0.0 q=2.0 Lsize=N/A time=00:00:08.04 bitrate=N/A speed=24.8x    \n",
            "video:2808kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: unknown\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pretrainedmodels\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywCJEYxqUxZ9",
        "outputId": "72697f79-c709-4fe9-8c21-18bec457869b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pretrainedmodels in /usr/local/lib/python3.11/dist-packages (0.7.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from pretrainedmodels) (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from pretrainedmodels) (0.20.1+cu124)\n",
            "Requirement already satisfied: munch in /usr/local/lib/python3.11/dist-packages (from pretrainedmodels) (4.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from pretrainedmodels) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->pretrainedmodels) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->pretrainedmodels) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->pretrainedmodels) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->pretrainedmodels) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Load ResNet152 for feature extraction\n",
        "import pretrainedmodels\n",
        "from pretrainedmodels import utils as pmutils\n",
        "import torch\n",
        "\n",
        "resnet = pretrainedmodels.resnet152(pretrained='imagenet')\n",
        "resnet.last_linear = pmutils.Identity()\n",
        "resnet = resnet.eval().cuda()\n",
        "load_image_fn = pmutils.LoadTransformImage(resnet)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9OS4kzbT7fc",
        "outputId": "54a788bb-46f3-46dc-ebdd-822bbfb2797b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Extract features from 40 frames\n",
        "import glob\n",
        "import numpy as np\n",
        "\n",
        "frame_list = sorted(glob.glob(os.path.join(frame_dir, '*.jpg')))\n",
        "selected_indices = np.linspace(0, len(frame_list) - 1, 40, dtype=int)\n",
        "selected_frames = [frame_list[i] for i in selected_indices]\n",
        "\n",
        "C, H, W = 3, 224, 224\n",
        "features = torch.zeros((len(selected_frames), C, H, W)).cuda()\n",
        "for i, frame_path in enumerate(selected_frames):\n",
        "    img = load_image_fn(frame_path)\n",
        "    features[i] = img\n",
        "\n",
        "with torch.no_grad():\n",
        "    video_feats = resnet(features).unsqueeze(0).cpu()\n"
      ],
      "metadata": {
        "id": "OOmGgEZyUT_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Load trained S2VT model\n",
        "import json\n",
        "from drive.MyDrive.youtube_captioning_with_attention.S2VTModel_Attention import S2VTModel\n",
        "\n",
        "vocab_path = '/content/drive/MyDrive/msvd_split/vocab.json'\n",
        "with open(vocab_path, 'r') as f:\n",
        "    vocab = json.load(f)\n",
        "vocab_rev = {v: k for k, v in vocab.items()}\n",
        "\n",
        "model = S2VTModel(\n",
        "    vocab_size=len(vocab),\n",
        "    max_len=45,\n",
        "    dim_hidden=1024,\n",
        "    dim_word=512,\n",
        "    dim_vid=2048,\n",
        "    sos_id=vocab['<SOS>'],\n",
        "    eos_id=vocab['<EOS>'],\n",
        "    rnn_cell='lstm',\n",
        "    rnn_dropout_p=0.3\n",
        ")\n",
        "\n",
        "checkpoint_path = '/content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_30.pt'\n",
        "checkpoint = torch.load(checkpoint_path, map_location='cuda')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model = model.eval().cuda()\n"
      ],
      "metadata": {
        "id": "WrIbzPIGUXW7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02959519-8258-45c6-9d6c-5528de551fec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:<ipython-input-38-bc5b83f4db4e>:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path, map_location='cuda')\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Generate caption\n",
        "video_feats = video_feats.cuda()\n",
        "\n",
        "with torch.no_grad():\n",
        "    _, predicted_ids = model(video_feats, mode='inference')\n",
        "\n",
        "caption = []\n",
        "for tok in predicted_ids[0]:\n",
        "    word = vocab_rev.get(int(tok), '<UNK>')\n",
        "    if word == '<EOS>':\n",
        "        break\n",
        "    if word not in ['<SOS>', '<PAD>']:\n",
        "        caption.append(word)\n",
        "\n",
        "print(\" Caption for your AVI video:\\n\", ' '.join(caption))\n"
      ],
      "metadata": {
        "id": "fUBD_lLtUZ8W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4adf466b-1e34-4547-8587-938d88d49cf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Caption for your AVI video:\n",
            " a cat is playing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wzV1LJ2ObPiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V52mQB2cbPvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk rouge-score\n",
        "!git clone https://github.com/tylin/coco-caption\n",
        "!pip install git+https://github.com/salaniz/pycocoevalcap\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otEEOMucbPye",
        "outputId": "b45dda75-3123-408f-c8ec-257cd7b5d343"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge-score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
            "fatal: destination path 'coco-caption' already exists and is not an empty directory.\n",
            "Collecting git+https://github.com/salaniz/pycocoevalcap\n",
            "  Cloning https://github.com/salaniz/pycocoevalcap to /tmp/pip-req-build-xhip2uvw\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/salaniz/pycocoevalcap /tmp/pip-req-build-xhip2uvw\n",
            "  Resolved https://github.com/salaniz/pycocoevalcap to commit a24f74c408c918f1f4ec34e9514bc8a76ce41ffd\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from pycocoevalcap==1.2) (2.0.8)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pycocotools>=2.0.2->pycocoevalcap==1.2) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pycocotools>=2.0.2->pycocoevalcap==1.2) (2.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (4.58.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frY9T0JsdoGX",
        "outputId": "d22314cc-2320-4724-9556-4df05098c61d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk rouge-score\n",
        "!git clone https://github.com/tylin/coco-caption\n",
        "!pip install git+https://github.com/salaniz/pycocoevalcap\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beS1RH3LeQSS",
        "outputId": "34f453c4-d8d6-4016-fea6-0dc9b2c2ec88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge-score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
            "fatal: destination path 'coco-caption' already exists and is not an empty directory.\n",
            "Collecting git+https://github.com/salaniz/pycocoevalcap\n",
            "  Cloning https://github.com/salaniz/pycocoevalcap to /tmp/pip-req-build-bgswltcp\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/salaniz/pycocoevalcap /tmp/pip-req-build-bgswltcp\n",
            "  Resolved https://github.com/salaniz/pycocoevalcap to commit a24f74c408c918f1f4ec34e9514bc8a76ce41ffd\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from pycocoevalcap==1.2) (2.0.8)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pycocotools>=2.0.2->pycocoevalcap==1.2) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pycocotools>=2.0.2->pycocoevalcap==1.2) (2.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (4.58.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive')\n",
        "\n",
        "from S2VTModel import S2VTModel\n"
      ],
      "metadata": {
        "id": "u0NYLp8EeTGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class VideoCaptionDataset(Dataset):\n",
        "    def __init__(self, feature_dir, json_path, vocab, max_caption_length=15, verbose=False):\n",
        "        self.feature_dir = feature_dir\n",
        "        self.vocab = vocab\n",
        "        self.max_caption_length = max_caption_length\n",
        "        self.verbose = verbose\n",
        "\n",
        "        with open(json_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        self.video_captions = {}\n",
        "        for item in data['sentences']:\n",
        "            vid = item['video_id']\n",
        "            if vid not in self.video_captions:\n",
        "                self.video_captions[vid] = []\n",
        "            self.video_captions[vid].append(item['caption'])\n",
        "\n",
        "        all_video_ids = [v['video_id'] for v in data['videos']]\n",
        "        self.video_ids = [\n",
        "            vid for vid in all_video_ids\n",
        "            if vid in self.video_captions and os.path.exists(os.path.join(self.feature_dir, f\"{vid}.npy\"))\n",
        "        ]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_id = self.video_ids[idx]\n",
        "        feature_path = os.path.join(self.feature_dir, f\"{video_id}.npy\")\n",
        "        video_features = np.load(feature_path)\n",
        "\n",
        "        caption = np.random.choice(self.video_captions[video_id])\n",
        "        tokens = [self.vocab['<SOS>']] + [self.vocab.get(w, self.vocab['<UNK>']) for w in caption.lower().split()] + [self.vocab['<EOS>']]\n",
        "        tokens = tokens[:self.max_caption_length]\n",
        "        tokens += [self.vocab['<PAD>']] * (self.max_caption_length - len(tokens))\n",
        "\n",
        "        return torch.tensor(video_features, dtype=torch.float32), torch.tensor(tokens, dtype=torch.long)\n"
      ],
      "metadata": {
        "id": "ZTmtDlTReWme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('/content/drive/MyDrive/msvd_split/vocab.json', 'r') as f:\n",
        "    vocab = json.load(f)\n",
        "\n",
        "with open('/content/drive/MyDrive/msvd_split/vocab_rev.json', 'r') as f:\n",
        "    vocab_rev = json.load(f)\n",
        "\n",
        "test_dataset = VideoCaptionDataset(\n",
        "    feature_dir='/content/drive/MyDrive/msvd_split/test/features',\n",
        "    json_path='/content/drive/MyDrive/msvd_split/test/test_captions.json',\n",
        "    vocab=vocab,\n",
        "    max_caption_length=45\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = S2VTModel(\n",
        "    vocab_size=len(vocab),\n",
        "    max_len=45,\n",
        "    dim_hidden=1024,\n",
        "    dim_word=512,\n",
        "    dim_vid=2048,\n",
        "    sos_id=vocab['<SOS>'],\n",
        "    eos_id=vocab['<EOS>'],\n",
        "    n_layers=2,\n",
        "    rnn_cell='lstm',\n",
        "    rnn_dropout_p=0.2\n",
        ")\n",
        "\n",
        "ckpt = torch.load('/content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_30.pt', map_location='cpu')\n",
        "model.load_state_dict(ckpt['model_state_dict'])\n",
        "model = model.to(device)\n"
      ],
      "metadata": {
        "id": "PxYyQEi_eZ-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def generate_predictions(model, dataset, vocab_rev, device):\n",
        "    model.eval()\n",
        "    predictions = {}\n",
        "\n",
        "    for idx in tqdm(range(len(dataset)), desc=\"Generating Captions\"):\n",
        "        video_feats, _ = dataset[idx]\n",
        "        video_feats = video_feats.unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            _, output_seq = model(video_feats, mode='inference')\n",
        "\n",
        "        tokens = output_seq.squeeze().tolist()\n",
        "        caption = []\n",
        "        for tok in tokens:\n",
        "            word = vocab_rev.get(str(tok), '<UNK>')\n",
        "            if word in ['<EOS>', '<PAD>']:\n",
        "                break\n",
        "            if word != '<SOS>':\n",
        "                caption.append(word)\n",
        "\n",
        "        video_id = dataset.video_ids[idx]\n",
        "        predictions[video_id] = ' '.join(caption)\n",
        "\n",
        "    return predictions\n",
        "\n",
        "preds = generate_predictions(model, test_dataset, vocab_rev, device)\n",
        "\n",
        "with open('/content/drive/MyDrive/msvd_split/test/predicted_captions.json', 'w') as f:\n",
        "    json.dump(preds, f, indent=4)\n",
        "\n",
        "print(\" Saved predictions.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDNLoujqeeIl",
        "outputId": "31eff811-e771-498d-ef5d-c7a4e9eea0be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Captions: 100%|| 296/296 [06:19<00:00,  1.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Saved predictions.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from rouge_score import rouge_scorer\n",
        "from pycocoevalcap.cider.cider import Cider\n",
        "from collections import defaultdict\n",
        "\n",
        "def evaluate_all_metrics(pred_path, ref_json_path):\n",
        "    with open(pred_path, 'r') as f:\n",
        "        preds = json.load(f)\n",
        "\n",
        "    with open(ref_json_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    refs = defaultdict(list)\n",
        "    for item in data['sentences']:\n",
        "        refs[item['video_id']].append(item['caption'].lower())\n",
        "\n",
        "    gt, pr = [], []\n",
        "    meteor_scores = []\n",
        "    rouge_scores = []\n",
        "    cider_refs, cider_hyps = [], []\n",
        "\n",
        "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "    smooth_fn = SmoothingFunction().method1\n",
        "\n",
        "    for video_id, pred_caption in preds.items():\n",
        "    pred_caption = pred_caption.lower()\n",
        "    references = refs.get(video_id, [])\n",
        "    if not references:\n",
        "        continue\n",
        "\n",
        "    gt.append([ref.split() for ref in references])\n",
        "    pr.append(pred_caption.split())\n",
        "\n",
        "    #  FIX HERE\n",
        "    meteor_scores.append(meteor_score(references, pred_caption.split()))\n",
        "\n",
        "    rouge_scores.append(scorer.score(' '.join(references), pred_caption)['rougeL'].fmeasure)\n",
        "    cider_refs.append({'image_id': video_id, 'captions': references})\n",
        "    cider_hyps.append({'image_id': video_id, 'caption': pred_caption})\n",
        "\n",
        "    bleu1 = corpus_bleu(gt, pr, weights=(1, 0, 0, 0), smoothing_function=smooth_fn)\n",
        "    bleu2 = corpus_bleu(gt, pr, weights=(0.5, 0.5, 0, 0), smoothing_function=smooth_fn)\n",
        "    bleu3 = corpus_bleu(gt, pr, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smooth_fn)\n",
        "    bleu4 = corpus_bleu(gt, pr, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smooth_fn)\n",
        "    avg_meteor = sum(meteor_scores) / len(meteor_scores)\n",
        "    avg_rouge = sum(rouge_scores) / len(rouge_scores)\n",
        "    cider_scorer = Cider()\n",
        "    cider_score, _ = cider_scorer.compute_score(cider_refs, cider_hyps)\n",
        "\n",
        "    print(\"\\n Evaluation Results:\")\n",
        "    print(f\"BLEU-1  : {bleu1:.4f}\")\n",
        "    print(f\"BLEU-2  : {bleu2:.4f}\")\n",
        "    print(f\"BLEU-3  : {bleu3:.4f}\")\n",
        "    print(f\"BLEU-4  : {bleu4:.4f}\")\n",
        "    print(f\"METEOR  : {avg_meteor:.4f}\")\n",
        "    print(f\"ROUGE-L : {avg_rouge:.4f}\")\n",
        "    print(f\"CIDEr   : {cider_score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "M8OkdLU5gGXu",
        "outputId": "ce985117-0112-4182-bdcc-bda5b4c69141"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "expected an indented block after 'for' statement on line 26 (<ipython-input-32-6a348d12c513>, line 27)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-32-6a348d12c513>\"\u001b[0;36m, line \u001b[0;32m27\u001b[0m\n\u001b[0;31m    pred_caption = pred_caption.lower()\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after 'for' statement on line 26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from rouge_score import rouge_scorer\n",
        "from pycocoevalcap.cider.cider import Cider\n",
        "from collections import defaultdict\n",
        "import json\n",
        "\n",
        "def evaluate_all_metrics(pred_path, ref_json_path):\n",
        "    with open(pred_path, 'r') as f:\n",
        "        preds = json.load(f)\n",
        "\n",
        "    with open(ref_json_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    refs = defaultdict(list)\n",
        "    for item in data['sentences']:\n",
        "        refs[item['video_id']].append(item['caption'].lower())\n",
        "\n",
        "    gt, pr = [], []\n",
        "    meteor_scores = []\n",
        "    rouge_scores = []\n",
        "    cider_refs, cider_hyps = []\n",
        "\n",
        "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "    smooth_fn = SmoothingFunction().method1\n",
        "\n",
        "    for video_id, pred_caption in preds.items():\n",
        "        pred_caption = pred_caption.lower()\n",
        "        references = refs.get(video_id, [])\n",
        "        if not references:\n",
        "            continue\n",
        "\n",
        "        gt.append([ref.split() for ref in references])\n",
        "        pr.append(pred_caption.split())\n",
        "\n",
        "        meteor_scores.append(meteor_score(references, pred_caption.split()))\n",
        "        rouge_scores.append(scorer.score(' '.join(references), pred_caption)['rougeL'].fmeasure)\n",
        "        cider_refs.append({'image_id': video_id, 'captions': references})\n",
        "        cider_hyps.append({'image_id': video_id, 'caption': pred_caption})\n",
        "\n",
        "    bleu1 = corpus_bleu(gt, pr, weights=(1, 0, 0, 0), smoothing_function=smooth_fn)\n",
        "    bleu2 = corpus_bleu(gt, pr, weights=(0.5, 0.5, 0, 0), smoothing_function=smooth_fn)\n",
        "    bleu3 = corpus_bleu(gt, pr, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smooth_fn)\n",
        "    bleu4 = corpus_bleu(gt, pr, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smooth_fn)\n",
        "    avg_meteor = sum(meteor_scores) / len(meteor_scores)\n",
        "    avg_rouge = sum(rouge_scores) / len(rouge_scores)\n",
        "    cider_scorer = Cider()\n",
        "    cider_score, _ = cider_scorer.compute_score(cider_refs, cider_hyps)\n",
        "\n",
        "    print(\"\\n Evaluation Results:\")\n",
        "    print(f\"BLEU-1  : {bleu1:.4f}\")\n",
        "    print(f\"BLEU-2  : {bleu2:.4f}\")\n",
        "    print(f\"BLEU-3  : {bleu3:.4f}\")\n",
        "    print(f\"BLEU-4  : {bleu4:.4f}\")\n",
        "    print(f\"METEOR  : {avg_meteor:.4f}\")\n",
        "    print(f\"ROUGE-L : {avg_rouge:.4f}\")\n",
        "    print(f\"CIDEr   : {cider_score:.4f}\")\n"
      ],
      "metadata": {
        "id": "kjmnKnAsgylQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "from collections import defaultdict\n",
        "\n",
        "def evaluate_bleu(pred_path, ref_json_path):\n",
        "    with open(pred_path, 'r') as f:\n",
        "        preds = json.load(f)\n",
        "\n",
        "    with open(ref_json_path, 'r') as f:\n",
        "        refs_data = json.load(f)\n",
        "\n",
        "    # Build references dict\n",
        "    references = defaultdict(list)\n",
        "    for item in refs_data['sentences']:\n",
        "        vid = item['video_id']\n",
        "        cap = item['caption'].lower().split()  # tokenize\n",
        "        references[vid].append(cap)\n",
        "\n",
        "    # Prepare ground truth and predicted lists\n",
        "    gt, pr = [], []\n",
        "    for vid, pred_caption in preds.items():\n",
        "        pred_tokens = pred_caption.lower().split()  # tokenize\n",
        "        if vid in references:\n",
        "            gt.append(references[vid])  # List of token lists\n",
        "            pr.append(pred_tokens)\n",
        "\n",
        "    # Compute BLEU\n",
        "    smooth_fn = SmoothingFunction().method1\n",
        "    bleu1 = corpus_bleu(gt, pr, weights=(1, 0, 0, 0), smoothing_function=smooth_fn)\n",
        "    bleu2 = corpus_bleu(gt, pr, weights=(0.5, 0.5, 0, 0), smoothing_function=smooth_fn)\n",
        "    bleu3 = corpus_bleu(gt, pr, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smooth_fn)\n",
        "    bleu4 = corpus_bleu(gt, pr, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smooth_fn)\n",
        "\n",
        "    print(\"\\n BLEU Evaluation:\")\n",
        "    print(f\"BLEU-1 : {bleu1:.4f}\")\n",
        "    print(f\"BLEU-2 : {bleu2:.4f}\")\n",
        "    print(f\"BLEU-3 : {bleu3:.4f}\")\n",
        "    print(f\"BLEU-4 : {bleu4:.4f}\")\n",
        "\n",
        "# === Run It ===\n",
        "evaluate_bleu(\n",
        "    pred_path='/content/drive/MyDrive/msvd_split/test/predicted_captions.json',\n",
        "    ref_json_path='/content/drive/MyDrive/msvd_split/test/test_captions.json'\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckpG1ZJqh5d3",
        "outputId": "2172cb7c-4915-43fa-d82c-ef499a0adf44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " BLEU Evaluation:\n",
            "BLEU-1 : 0.7170\n",
            "BLEU-2 : 0.5550\n",
            "BLEU-3 : 0.4579\n",
            "BLEU-4 : 0.3424\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge-score\n",
        "!git clone https://github.com/tylin/coco-caption\n",
        "!python3 setup.py install --cwd coco-caption\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZurWK9EjiCq",
        "outputId": "960ca7e6-c56e-4759-a301-509d1cdcb5a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge-score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (4.67.1)\n",
            "fatal: destination path 'coco-caption' already exists and is not an empty directory.\n",
            "python3: can't open file '/content/setup.py': [Errno 2] No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_all_metrics(\n",
        "    pred_path='/content/drive/MyDrive/msvd_split/test/predicted_captions.json',\n",
        "    ref_json_path='/content/drive/MyDrive/msvd_split/test/test_captions.json'\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "YgFmc_tHj4H9",
        "outputId": "02d24448-cdf3-4019-f870-4c03dd2e6cbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "\"hypothesis\" expects pre-tokenized hypothesis (Iterable[str]): a woman is cutting a potato",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-c1885571512d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m evaluate_all_metrics(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mpred_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/drive/MyDrive/msvd_split/test/predicted_captions.json'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mref_json_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/drive/MyDrive/msvd_split/test/test_captions.json'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n",
            "\u001b[0;32m<ipython-input-43-cb1b2fed86be>\u001b[0m in \u001b[0;36mevaluate_all_metrics\u001b[0;34m(pred_path, ref_json_path)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# METEOR: average of all refs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         meteor_scores.append(\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeteor_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mref\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mref_caps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref_caps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         )\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-cb1b2fed86be>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# METEOR: average of all refs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         meteor_scores.append(\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeteor_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mref\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mref_caps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref_caps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         )\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/translate/meteor_score.py\u001b[0m in \u001b[0;36mmeteor_score\u001b[0;34m(references, hypothesis, preprocess, stemmer, wordnet, alpha, beta, gamma)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlevel\u001b[0m \u001b[0mMETEOR\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \"\"\"\n\u001b[0;32m--> 397\u001b[0;31m     return max(\n\u001b[0m\u001b[1;32m    398\u001b[0m         single_meteor_score(\n\u001b[1;32m    399\u001b[0m             \u001b[0mreference\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/translate/meteor_score.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    396\u001b[0m     \"\"\"\n\u001b[1;32m    397\u001b[0m     return max(\n\u001b[0;32m--> 398\u001b[0;31m         single_meteor_score(\n\u001b[0m\u001b[1;32m    399\u001b[0m             \u001b[0mreference\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0mhypothesis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/translate/meteor_score.py\u001b[0m in \u001b[0;36msingle_meteor_score\u001b[0;34m(reference, hypothesis, preprocess, stemmer, wordnet, alpha, beta, gamma)\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlevel\u001b[0m \u001b[0mMETEOR\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \"\"\"\n\u001b[0;32m--> 326\u001b[0;31m     enum_hypothesis, enum_reference = _generate_enums(\n\u001b[0m\u001b[1;32m    327\u001b[0m         \u001b[0mhypothesis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreference\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/translate/meteor_score.py\u001b[0m in \u001b[0;36m_generate_enums\u001b[0;34m(hypothesis, reference, preprocess)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \"\"\"\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhypothesis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         raise TypeError(\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0;34mf'\"hypothesis\" expects pre-tokenized hypothesis (Iterable[str]): {hypothesis}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         )\n",
            "\u001b[0;31mTypeError\u001b[0m: \"hypothesis\" expects pre-tokenized hypothesis (Iterable[str]): a woman is cutting a potato"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pRQoonbFkIlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "from collections import defaultdict\n",
        "\n",
        "def evaluate_bleu(pred_path, ref_json_path):\n",
        "    with open(pred_path, 'r') as f:\n",
        "        preds = json.load(f)\n",
        "\n",
        "    with open(ref_json_path, 'r') as f:\n",
        "        refs_data = json.load(f)\n",
        "\n",
        "    # Build references dict\n",
        "    references = defaultdict(list)\n",
        "    for item in refs_data['sentences']:\n",
        "        vid = item['video_id']\n",
        "        cap = item['caption'].lower().split()  # tokenize\n",
        "        references[vid].append(cap)\n",
        "\n",
        "    # Prepare ground truth and predicted lists\n",
        "    gt, pr = [], []\n",
        "    for vid, pred_caption in preds.items():\n",
        "        pred_tokens = pred_caption.lower().split()  # tokenize\n",
        "        if vid in references:\n",
        "            gt.append(references[vid])  # List of token lists\n",
        "            pr.append(pred_tokens)\n",
        "\n",
        "    # Compute BLEU\n",
        "    smooth_fn = SmoothingFunction().method1\n",
        "    bleu1 = corpus_bleu(gt, pr, weights=(1, 0, 0, 0), smoothing_function=smooth_fn)\n",
        "    bleu2 = corpus_bleu(gt, pr, weights=(0.5, 0.5, 0, 0), smoothing_function=smooth_fn)\n",
        "    bleu3 = corpus_bleu(gt, pr, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smooth_fn)\n",
        "    bleu4 = corpus_bleu(gt, pr, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smooth_fn)\n",
        "\n",
        "    print(\"\\n BLEU Evaluation:\")\n",
        "    print(f\"BLEU-1 : {bleu1:.4f}\")\n",
        "    print(f\"BLEU-2 : {bleu2:.4f}\")\n",
        "    print(f\"BLEU-3 : {bleu3:.4f}\")\n",
        "    print(f\"BLEU-4 : {bleu4:.4f}\")\n",
        "\n",
        "# === Run It ===\n",
        "evaluate_bleu(\n",
        "    pred_path='/content/drive/MyDrive/msvd_split/test/predicted_captions.json',\n",
        "    ref_json_path='/content/drive/MyDrive/msvd_split/test/test_captions.json'\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2172cb7c-4915-43fa-d82c-ef499a0adf44",
        "id": "Ea0JfIFqkI5m"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " BLEU Evaluation:\n",
            "BLEU-1 : 0.7170\n",
            "BLEU-2 : 0.5550\n",
            "BLEU-3 : 0.4579\n",
            "BLEU-4 : 0.3424\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from rouge_score import rouge_scorer\n",
        "from pycocoevalcap.cider.cider import Cider\n",
        "from collections import defaultdict\n",
        "\n",
        "def evaluate_all_metrics(pred_path, ref_json_path):\n",
        "    with open(pred_path, 'r') as f:\n",
        "        preds = json.load(f)\n",
        "\n",
        "    with open(ref_json_path, 'r') as f:\n",
        "        refs_data = json.load(f)\n",
        "\n",
        "    # Build references dict\n",
        "    references = defaultdict(list)\n",
        "    for item in refs_data['sentences']:\n",
        "        vid = item['video_id']\n",
        "        cap = item['caption'].lower().split()\n",
        "        references[vid].append(cap)\n",
        "\n",
        "    # Evaluation lists\n",
        "    gt, pr = [], []\n",
        "    meteor_scores, rouge_scores = [], []\n",
        "    cider_refs, cider_hyps = [], []\n",
        "\n",
        "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "    smooth_fn = SmoothingFunction().method1\n",
        "\n",
        "    for vid, pred_caption in preds.items():\n",
        "        if vid not in references:\n",
        "            continue\n",
        "\n",
        "        ref_caps = references[vid]\n",
        "        pred_tokens = pred_caption.lower().split()\n",
        "\n",
        "        gt.append(ref_caps)\n",
        "        pr.append(pred_tokens)\n",
        "\n",
        "        # METEOR: average of all refs\n",
        "        meteor_scores.append(\n",
        "            sum(meteor_score([' '.join(ref)], ' '.join(pred_tokens)) for ref in ref_caps) / len(ref_caps)\n",
        "        )\n",
        "\n",
        "        # ROUGE-L: best among refs\n",
        "        rouge_scores.append(\n",
        "            max(scorer.score(' '.join(ref), ' '.join(pred_tokens))['rougeL'].fmeasure for ref in ref_caps)\n",
        "        )\n",
        "\n",
        "        cider_refs.append({'image_id': vid, 'captions': [' '.join(ref) for ref in ref_caps]})\n",
        "        cider_hyps.append({'image_id': vid, 'caption': ' '.join(pred_tokens)})\n",
        "\n",
        "    # BLEU\n",
        "    bleu1 = corpus_bleu(gt, pr, weights=(1, 0, 0, 0), smoothing_function=smooth_fn)\n",
        "    bleu2 = corpus_bleu(gt, pr, weights=(0.5, 0.5, 0, 0), smoothing_function=smooth_fn)\n",
        "    bleu3 = corpus_bleu(gt, pr, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smooth_fn)\n",
        "    bleu4 = corpus_bleu(gt, pr, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smooth_fn)\n",
        "\n",
        "    avg_meteor = sum(meteor_scores) / len(meteor_scores)\n",
        "    avg_rouge = sum(rouge_scores) / len(rouge_scores)\n",
        "    cider_score, _ = Cider().compute_score(cider_refs, cider_hyps)\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\n Evaluation Results:\")\n",
        "    print(f\"BLEU-1  : {bleu1:.4f}\")\n",
        "    print(f\"BLEU-2  : {bleu2:.4f}\")\n",
        "    print(f\"BLEU-3  : {bleu3:.4f}\")\n",
        "    print(f\"BLEU-4  : {bleu4:.4f}\")\n",
        "    print(f\"METEOR  : {avg_meteor:.4f}\")\n",
        "    print(f\"ROUGE-L : {avg_rouge:.4f}\")\n",
        "    print(f\"CIDEr   : {cider_score:.4f}\")\n"
      ],
      "metadata": {
        "id": "lYzmUgAWjl52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2jUJdYZ8kKGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sSWQJX6RkKe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "from collections import defaultdict\n",
        "\n",
        "def evaluate_bleu(pred_path, ref_json_path):\n",
        "    with open(pred_path, 'r') as f:\n",
        "        preds = json.load(f)\n",
        "\n",
        "    with open(ref_json_path, 'r') as f:\n",
        "        refs_data = json.load(f)\n",
        "\n",
        "    # === Build references dict ===\n",
        "    references = defaultdict(list)\n",
        "    for item in refs_data['sentences']:\n",
        "        video_id = item['video_id']\n",
        "        caption_tokens = item['caption'].lower().split()\n",
        "        references[video_id].append(caption_tokens)\n",
        "\n",
        "    # === Prepare reference (gt) and hypothesis (pr) lists ===\n",
        "    gt, pr = [], []\n",
        "    for vid, pred_caption in preds.items():\n",
        "        if vid in references:\n",
        "            gt.append(references[vid])  # List of reference token lists\n",
        "            pr.append(pred_caption.lower().split())  # Predicted token list\n",
        "\n",
        "    # === Compute BLEU scores ===\n",
        "    smooth_fn = SmoothingFunction().method1\n",
        "    bleu1 = corpus_bleu(gt, pr, weights=(1, 0, 0, 0), smoothing_function=smooth_fn)\n",
        "    bleu2 = corpus_bleu(gt, pr, weights=(0.5, 0.5, 0, 0), smoothing_function=smooth_fn)\n",
        "    bleu3 = corpus_bleu(gt, pr, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smooth_fn)\n",
        "    bleu4 = corpus_bleu(gt, pr, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smooth_fn)\n",
        "\n",
        "    # === Output ===\n",
        "    print(\"\\n BLEU Evaluation Results:\")\n",
        "    print(f\"BLEU-1 : {bleu1:.4f}\")\n",
        "    print(f\"BLEU-2 : {bleu2:.4f}\")\n",
        "    print(f\"BLEU-3 : {bleu3:.4f}\")\n",
        "    print(f\"BLEU-4 : {bleu4:.4f}\")\n",
        "\n",
        "# === Run BLEU Evaluation ===\n",
        "evaluate_bleu(\n",
        "    pred_path='/content/drive/MyDrive/msvd_split/test/predicted_captions.json',\n",
        "    ref_json_path='/content/drive/MyDrive/msvd_split/test/test_captions.json'\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcsAZtCdkKiW",
        "outputId": "6d7cf6bd-25bc-4d0b-e9cf-5ae8a7f25d0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " BLEU Evaluation Results:\n",
            "BLEU-1 : 0.7170\n",
            "BLEU-2 : 0.5550\n",
            "BLEU-3 : 0.4579\n",
            "BLEU-4 : 0.3424\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import os\n",
        "import json\n",
        "\n",
        "class VideoCaptionDataset(Dataset):\n",
        "    def __init__(self, feature_dir, json_path, vocab, max_caption_length=45):\n",
        "        self.feature_dir = feature_dir\n",
        "        self.vocab = vocab\n",
        "        self.max_caption_length = max_caption_length\n",
        "\n",
        "        with open(json_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        self.samples = data['sentences']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "        video_id = sample['video_id']\n",
        "        caption = sample['caption'].lower().split()\n",
        "\n",
        "        # Convert caption to indices\n",
        "        caption_ids = [self.vocab['<SOS>']] + \\\n",
        "                      [self.vocab.get(word, self.vocab['<UNK>']) for word in caption] + \\\n",
        "                      [self.vocab['<EOS>']]\n",
        "        caption_ids = caption_ids[:self.max_caption_length]\n",
        "        caption_ids += [self.vocab['<PAD>']] * (self.max_caption_length - len(caption_ids))\n",
        "\n",
        "        caption_tensor = torch.tensor(caption_ids, dtype=torch.long)\n",
        "\n",
        "        # Load pre-extracted I3D features\n",
        "        feat_path = os.path.join(self.feature_dir, video_id + '.npy')\n",
        "        video_feat = torch.tensor(np.load(feat_path), dtype=torch.float32)\n",
        "\n",
        "        return video_feat, caption_tensor\n"
      ],
      "metadata": {
        "id": "kR7blPoWmBpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "from collections import defaultdict\n",
        "from drive.MyDrive.S2VTModel import S2VTModel\n",
        "from VideoCaptionDataset import VideoCaptionDataset  #  Make sure this class is imported properly\n",
        "\n",
        "# === Paths ===\n",
        "vocab_path = '/content/drive/MyDrive/msvd_split/vocab.json'\n",
        "test_json_path = '/content/drive/MyDrive/msvd_split/test/test_captions.json'\n",
        "test_feature_dir = '/content/drive/MyDrive/msvd_split/test/features'\n",
        "checkpoint_path = '/content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_30.pt'\n",
        "\n",
        "# === Load vocab ===\n",
        "with open(vocab_path, 'r') as f:\n",
        "    vocab = json.load(f)\n",
        "vocab_rev = {v: k for k, v in vocab.items()}\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# === Load model ===\n",
        "model = S2VTModel(\n",
        "    vocab_size=vocab_size,\n",
        "    max_len=45,\n",
        "    dim_hidden=1024,\n",
        "    dim_word=512,\n",
        "    dim_vid=2048,\n",
        "    sos_id=vocab['<SOS>'],\n",
        "    eos_id=vocab['<EOS>'],\n",
        "    n_layers=1,\n",
        "    rnn_cell='lstm',\n",
        "    rnn_dropout_p=0.3\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# === Load test dataset ===\n",
        "test_dataset = VideoCaptionDataset(test_feature_dir, test_json_path, vocab, max_caption_length=45)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# === Inference + BLEU computation ===\n",
        "print(\"Evaluating on test set...\")\n",
        "all_references = []\n",
        "all_hypotheses = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for idx, (video_feats, captions) in enumerate(test_loader):\n",
        "        video_feats, captions = video_feats.to(device), captions.to(device)\n",
        "\n",
        "        # Generate prediction\n",
        "        _, predicted_ids = model(video_feats, mode='inference')\n",
        "\n",
        "        # Decode predicted caption\n",
        "        pred_tokens = []\n",
        "        for tok in predicted_ids[0]:\n",
        "            word = vocab_rev.get(int(tok), '<UNK>')\n",
        "            if word == '<EOS>': break\n",
        "            if word not in ['<SOS>', '<PAD>']:\n",
        "                pred_tokens.append(word)\n",
        "\n",
        "        # Decode ground truth\n",
        "        ref_tokens = []\n",
        "        for tok in captions[0]:\n",
        "            word = vocab_rev.get(int(tok), '<UNK>')\n",
        "            if word == '<EOS>': break\n",
        "            if word not in ['<SOS>', '<PAD>']:\n",
        "                ref_tokens.append(word)\n",
        "\n",
        "        all_hypotheses.append(pred_tokens)\n",
        "        all_references.append([ref_tokens])\n",
        "\n",
        "        if idx < 5:\n",
        "            print(f\"\\nExample {idx + 1}\")\n",
        "            print(f\"Predicted    : {' '.join(pred_tokens)}\")\n",
        "            print(f\"Ground Truth : {' '.join(ref_tokens)}\")\n",
        "\n",
        "# === Compute BLEU Scores ===\n",
        "smooth_fn = SmoothingFunction().method1\n",
        "bleu1 = corpus_bleu(all_references, all_hypotheses, weights=(1, 0, 0, 0), smoothing_function=smooth_fn)\n",
        "bleu2 = corpus_bleu(all_references, all_hypotheses, weights=(0.5, 0.5, 0, 0), smoothing_function=smooth_fn)\n",
        "bleu3 = corpus_bleu(all_references, all_hypotheses, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smooth_fn)\n",
        "bleu4 = corpus_bleu(all_references, all_hypotheses, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smooth_fn)\n",
        "\n",
        "# === Display ===\n",
        "print(\"\\n BLEU Evaluation:\")\n",
        "print(f\"BLEU-1 : {bleu1:.4f}\")\n",
        "print(f\"BLEU-2 : {bleu2:.4f}\")\n",
        "print(f\"BLEU-3 : {bleu3:.4f}\")\n",
        "print(f\"BLEU-4 : {bleu4:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "EdSc4Xq7lE7s",
        "outputId": "73c1aaa3-2bee-4794-902e-69762bcf880c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'VideoCaptionDataset'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-6c45d8a54bae>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMyDrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mS2VTModel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mS2VTModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mVideoCaptionDataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVideoCaptionDataset\u001b[0m  \u001b[0;31m#  Make sure this class is imported properly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# === Paths ===\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'VideoCaptionDataset'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge-score\n",
        "!pip install git+https://github.com/tylin/coco-caption.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoOrfWKamiA3",
        "outputId": "570652ee-9cd4-41a4-8fd1-9af3147221b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge-score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (4.67.1)\n",
            "Collecting git+https://github.com/tylin/coco-caption.git\n",
            "  Cloning https://github.com/tylin/coco-caption.git to /tmp/pip-req-build-gwfu9rmh\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/tylin/coco-caption.git /tmp/pip-req-build-gwfu9rmh\n",
            "  Resolved https://github.com/tylin/coco-caption.git to commit 3a9afb2682141a03e1cdc02b0df6770d2c884f6f\n",
            "\u001b[31mERROR: git+https://github.com/tylin/coco-caption.git does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from rouge_score import rouge_scorer\n",
        "from pycocoevalcap.cider.cider import Cider\n",
        "from collections import defaultdict\n",
        "\n",
        "def evaluate_all_metrics(pred_path, ref_json_path):\n",
        "    with open(pred_path, 'r') as f:\n",
        "        preds = json.load(f)\n",
        "\n",
        "    with open(ref_json_path, 'r') as f:\n",
        "        refs_data = json.load(f)\n",
        "\n",
        "    # Build references dictionary\n",
        "    references = defaultdict(list)\n",
        "    for item in refs_data['sentences']:\n",
        "        vid = item['video_id']\n",
        "        cap = item['caption'].lower().split()  # Tokenized reference\n",
        "        references[vid].append(cap)\n",
        "\n",
        "    gt, pr = [], []\n",
        "    meteor_scores = []\n",
        "    rouge_scores = []\n",
        "    cider_refs, cider_hyps = [], []\n",
        "\n",
        "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "    smooth_fn = SmoothingFunction().method1\n",
        "\n",
        "    for vid, pred in preds.items():\n",
        "        if vid not in references:\n",
        "            continue\n",
        "        pred_tokens = pred.lower().split()\n",
        "        ref_tokens = references[vid]\n",
        "\n",
        "        # BLEU\n",
        "        gt.append(ref_tokens)\n",
        "        pr.append(pred_tokens)\n",
        "\n",
        "        # METEOR\n",
        "        references_joined = [' '.join(ref) for ref in ref_tokens]\n",
        "        meteor_scores.append(meteor_score(references_joined, ' '.join(pred_tokens)))\n",
        "\n",
        "        # ROUGE\n",
        "        rouge_scores.append(scorer.score(' '.join(references_joined), ' '.join(pred_tokens))['rougeL'].fmeasure)\n",
        "\n",
        "        # CIDEr format\n",
        "        cider_refs.append({'image_id': vid, 'captions': references_joined})\n",
        "        cider_hyps.append({'image_id': vid, 'caption': ' '.join(pred_tokens)})\n",
        "\n",
        "    # BLEU\n",
        "    bleu1 = corpus_bleu(gt, pr, weights=(1, 0, 0, 0), smoothing_function=smooth_fn)\n",
        "    bleu2 = corpus_bleu(gt, pr, weights=(0.5, 0.5, 0, 0), smoothing_function=smooth_fn)\n",
        "    bleu3 = corpus_bleu(gt, pr, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smooth_fn)\n",
        "    bleu4 = corpus_bleu(gt, pr, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smooth_fn)\n",
        "\n",
        "    avg_meteor = sum(meteor_scores) / len(meteor_scores)\n",
        "    avg_rouge = sum(rouge_scores) / len(rouge_scores)\n",
        "\n",
        "    cider_scorer = Cider()\n",
        "    cider_score, _ = cider_scorer.compute_score(cider_refs, cider_hyps)\n",
        "\n",
        "    print(\"\\n Evaluation Metrics:\")\n",
        "    print(f\"BLEU-1   : {bleu1:.4f}\")\n",
        "    print(f\"BLEU-2   : {bleu2:.4f}\")\n",
        "    print(f\"BLEU-3   : {bleu3:.4f}\")\n",
        "    print(f\"BLEU-4   : {bleu4:.4f}\")\n",
        "    print(f\"METEOR   : {avg_meteor:.4f}\")\n",
        "    print(f\"ROUGE-L  : {avg_rouge:.4f}\")\n",
        "    print(f\"CIDEr    : {cider_score:.4f}\")\n",
        "\n",
        "#  Call this function\n",
        "evaluate_all_metrics(\n",
        "    pred_path='/content/drive/MyDrive/msvd_split/test/predicted_captions.json',\n",
        "    ref_json_path='/content/drive/MyDrive/msvd_split/test/test_captions.json'\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "xTrABXPTmjaA",
        "outputId": "334e74dd-c77d-4f8a-b831-5f431d11e7ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "\"hypothesis\" expects pre-tokenized hypothesis (Iterable[str]): a woman is cutting a potato",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-e8e399a8427f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;31m#  Call this function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m evaluate_all_metrics(\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0mpred_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/drive/MyDrive/msvd_split/test/predicted_captions.json'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mref_json_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/drive/MyDrive/msvd_split/test/test_captions.json'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-51-e8e399a8427f>\u001b[0m in \u001b[0;36mevaluate_all_metrics\u001b[0;34m(pred_path, ref_json_path)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# METEOR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mreferences_joined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mref\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mref_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mmeteor_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeteor_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreferences_joined\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# ROUGE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/translate/meteor_score.py\u001b[0m in \u001b[0;36mmeteor_score\u001b[0;34m(references, hypothesis, preprocess, stemmer, wordnet, alpha, beta, gamma)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlevel\u001b[0m \u001b[0mMETEOR\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \"\"\"\n\u001b[0;32m--> 397\u001b[0;31m     return max(\n\u001b[0m\u001b[1;32m    398\u001b[0m         single_meteor_score(\n\u001b[1;32m    399\u001b[0m             \u001b[0mreference\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/translate/meteor_score.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    396\u001b[0m     \"\"\"\n\u001b[1;32m    397\u001b[0m     return max(\n\u001b[0;32m--> 398\u001b[0;31m         single_meteor_score(\n\u001b[0m\u001b[1;32m    399\u001b[0m             \u001b[0mreference\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0mhypothesis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/translate/meteor_score.py\u001b[0m in \u001b[0;36msingle_meteor_score\u001b[0;34m(reference, hypothesis, preprocess, stemmer, wordnet, alpha, beta, gamma)\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlevel\u001b[0m \u001b[0mMETEOR\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \"\"\"\n\u001b[0;32m--> 326\u001b[0;31m     enum_hypothesis, enum_reference = _generate_enums(\n\u001b[0m\u001b[1;32m    327\u001b[0m         \u001b[0mhypothesis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreference\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/translate/meteor_score.py\u001b[0m in \u001b[0;36m_generate_enums\u001b[0;34m(hypothesis, reference, preprocess)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \"\"\"\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhypothesis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         raise TypeError(\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0;34mf'\"hypothesis\" expects pre-tokenized hypothesis (Iterable[str]): {hypothesis}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         )\n",
            "\u001b[0;31mTypeError\u001b[0m: \"hypothesis\" expects pre-tokenized hypothesis (Iterable[str]): a woman is cutting a potato"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from collections import defaultdict\n",
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "\n",
        "def evaluate_bleu_only(pred_path, ref_json_path):\n",
        "    with open(pred_path, 'r') as f:\n",
        "        preds = json.load(f)\n",
        "\n",
        "    with open(ref_json_path, 'r') as f:\n",
        "        refs_data = json.load(f)\n",
        "\n",
        "    references = defaultdict(list)\n",
        "    for item in refs_data['sentences']:\n",
        "        vid = item['video_id']\n",
        "        cap = item['caption'].lower().split()\n",
        "        references[vid].append(cap)\n",
        "\n",
        "    gt, pr = [], []\n",
        "    smooth_fn = SmoothingFunction().method1\n",
        "\n",
        "    for vid, pred in preds.items():\n",
        "        if vid not in references:\n",
        "            continue\n",
        "        gt.append(references[vid])\n",
        "        pr.append(pred.lower().split())\n",
        "\n",
        "    bleu1 = corpus_bleu(gt, pr, weights=(1, 0, 0, 0), smoothing_function=smooth_fn)\n",
        "    bleu2 = corpus_bleu(gt, pr, weights=(0.5, 0.5, 0, 0), smoothing_function=smooth_fn)\n",
        "    bleu3 = corpus_bleu(gt, pr, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smooth_fn)\n",
        "    bleu4 = corpus_bleu(gt, pr, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smooth_fn)\n",
        "\n",
        "    print(\"\\n BLEU Score Evaluation:\")\n",
        "    print(f\"BLEU-1 : {bleu1:.4f}\")\n",
        "    print(f\"BLEU-2 : {bleu2:.4f}\")\n",
        "    print(f\"BLEU-3 : {bleu3:.4f}\")\n",
        "    print(f\"BLEU-4 : {bleu4:.4f}\")\n",
        "\n",
        "#  Run it\n",
        "evaluate_bleu_only(\n",
        "    pred_path='/content/drive/MyDrive/msvd_split/test/predicted_captions.json',\n",
        "    ref_json_path='/content/drive/MyDrive/msvd_split/test/test_captions.json'\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nUCUnsEnDM4",
        "outputId": "93cd0e6a-e59a-4aeb-93c5-b437d0dd26c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " BLEU Score Evaluation:\n",
            "BLEU-1 : 0.7170\n",
            "BLEU-2 : 0.5550\n",
            "BLEU-3 : 0.4579\n",
            "BLEU-4 : 0.3424\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z6K6BssFneAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jDAPmRA8neGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fiE1paMYneJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYSwP0HYneNR",
        "outputId": "279ca2b3-27a6-4e24-a068-76877eaf63b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "import cv2\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "def extract_resnet_features(video_path):\n",
        "    model = models.resnet152(pretrained=True)\n",
        "    model = torch.nn.Sequential(*list(model.children())[:-1])  # remove final fc\n",
        "    model.eval()\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                             std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    features = []\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "        tensor = transform(img).unsqueeze(0)  # [1, 3, 224, 224]\n",
        "        with torch.no_grad():\n",
        "            feat = model(tensor)  # [1, 2048, 1, 1]\n",
        "        features.append(feat.squeeze().numpy())\n",
        "\n",
        "    cap.release()\n",
        "    return torch.tensor(features).unsqueeze(0)  # shape: [1, T, 2048]\n"
      ],
      "metadata": {
        "id": "swG3vEWGn8cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from drive.MyDrive.S2VTModel import S2VTModel\n",
        "\n",
        "# Load vocab\n",
        "import json\n",
        "with open('/content/drive/MyDrive/msvd_split/vocab.json') as f:\n",
        "    vocab = json.load(f)\n",
        "vocab_rev = {v: k for k, v in vocab.items()}\n",
        "\n",
        "# Load model\n",
        "model = S2VTModel(\n",
        "    vocab_size=len(vocab),\n",
        "    max_len=45,\n",
        "    dim_hidden=1024,\n",
        "    dim_word=512,\n",
        "    dim_vid=2048,\n",
        "    sos_id=vocab['<SOS>'],\n",
        "    eos_id=vocab['<EOS>'],\n",
        "    n_layers=1,\n",
        "    rnn_cell='lstm',\n",
        "    rnn_dropout_p=0.3\n",
        ")\n",
        "\n",
        "checkpoint = torch.load('/content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_30.pt', map_location='cpu')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "vekpbmNooBCJ",
        "outputId": "e2a47da8-3ca3-4c16-e0e6-47b243d21b44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for S2VTModel:\n\tUnexpected key(s) in state_dict: \"rnn1.weight_ih_l1\", \"rnn1.weight_hh_l1\", \"rnn1.bias_ih_l1\", \"rnn1.bias_hh_l1\", \"rnn2.weight_ih_l1\", \"rnn2.weight_hh_l1\", \"rnn2.bias_ih_l1\", \"rnn2.bias_hh_l1\". ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-fee58b181256>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_30.pt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2581\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   2582\u001b[0m                 \"Error(s) in loading state_dict for {}:\\n\\t{}\".format(\n\u001b[1;32m   2583\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\\t\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for S2VTModel:\n\tUnexpected key(s) in state_dict: \"rnn1.weight_ih_l1\", \"rnn1.weight_hh_l1\", \"rnn1.bias_ih_l1\", \"rnn1.bias_hh_l1\", \"rnn2.weight_ih_l1\", \"rnn2.weight_hh_l1\", \"rnn2.bias_ih_l1\", \"rnn2.bias_hh_l1\". "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from drive.MyDrive.S2VTModel import S2VTModel  #  Adjust if path differs\n",
        "\n",
        "# === Parameters ===\n",
        "vocab_path = '/content/drive/MyDrive/msvd_split/vocab.json'\n",
        "checkpoint_path = '/content/drive/MyDrive/msvd_split/checkpoints/checkpoint_epoch_30.pt'\n",
        "video_feature_dir = '/content/drive/MyDrive/msvd_split/test/features'  # Adjust if needed\n",
        "max_len = 45\n",
        "\n",
        "# === Load Vocabulary ===\n",
        "with open(vocab_path, 'r') as f:\n",
        "    vocab = json.load(f)\n",
        "vocab_rev = {v: k for k, v in vocab.items()}\n",
        "\n",
        "# === Define Dataset ===\n",
        "class VideoFeatureDataset(Dataset):\n",
        "    def __init__(self, video_dir):\n",
        "        self.video_dir = video_dir\n",
        "        self.video_files = [f for f in os.listdir(video_dir) if f.endswith('.npy')]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        fname = self.video_files[idx]\n",
        "        path = os.path.join(self.video_dir, fname)\n",
        "        feature = np.load(path)\n",
        "        return torch.tensor(feature, dtype=torch.float32), fname.split('.')[0]\n",
        "\n",
        "# === Load Dataset ===\n",
        "dataset = VideoFeatureDataset(video_feature_dir)\n",
        "loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# === Load Model ===\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = S2VTModel(\n",
        "    vocab_size=len(vocab),\n",
        "    max_len=max_len,\n",
        "    dim_hidden=1024,\n",
        "    dim_word=512,\n",
        "    dim_vid=2048,\n",
        "    sos_id=vocab['<SOS>'],\n",
        "    eos_id=vocab['<EOS>'],\n",
        "    n_layers=2,  #  Fix: match your checkpoint\n",
        "    rnn_cell='lstm',\n",
        "    rnn_dropout_p=0.3\n",
        ").to(device)\n",
        "\n",
        "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()\n",
        "\n",
        "# === Generate Captions ===\n",
        "print(\"\\n Generating Captions:\\n\")\n",
        "with torch.no_grad():\n",
        "    for video_feats, vid_id in loader:\n",
        "        video_feats = video_feats.to(device)\n",
        "\n",
        "        _, predicted_ids = model(video_feats, mode='inference')\n",
        "        pred_tokens = []\n",
        "\n",
        "        for tok in predicted_ids[0]:\n",
        "            word = vocab_rev.get(int(tok), '<UNK>')\n",
        "            if word == '<EOS>':\n",
        "                break\n",
        "            if word not in ['<SOS>', '<PAD>']:\n",
        "                pred_tokens.append(word)\n",
        "\n",
        "        caption = ' '.join(pred_tokens)\n",
        "        print(f\"{vid_id[0]}: {caption}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYlP29_qohCM",
        "outputId": "edf9861c-84ee-4422-a932-46a3beca3417"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Generating Captions:\n",
            "\n",
            "qeKX-N1nKiM_68_72: a woman is cutting a potato\n",
            "Sg5rTYrkpnU_35_48: a man is walking\n",
            "C_DDjCRxTxQ_1_4: a woman is riding a horse\n",
            "-wa0umYJVGg_271_276: a woman is cutting a potato\n",
            "9LHg5RUGukI_58_63: a man is cutting a potato\n",
            "PCXHuseKwDc_68_76: a man is playing a guitar\n",
            "tBj4Ny19vfQ_54_59: a dog is playing\n",
            "jW77z3-SrO4_56_63: a woman is cutting a potato\n",
            "8PQiaurIiDM_247_255: a man is riding a horse\n",
            "5XxshEdcfAM_96_107: a man is riding a horse\n",
            "9zXqkHvs0po_59_65: a woman is cutting a potato\n",
            "ok4cM6WTA5E_142_150: a woman is cutting a potato\n",
            "FwCmcZpkk-k_22_32: a man is riding a horse\n",
            "FWzsXeXCwuc_111_116: a man is playing a guitar\n",
            "0lh_UWF9ZP4_82_87: a woman is cutting a potato\n",
            "dJCtOz32dnw_40_60: a man is riding a horse\n",
            "Ixw6wmoC_xg_116_126: a man is riding a horse\n",
            "lcu-DwrnYY8_2_5: a man is walking\n",
            "ZlX_Gy4HP2E_38_55: a man is riding a horse\n",
            "5U3xz9Ovmhk_214_222: a woman is cutting a potato\n",
            "BVilbVCo9sU_1_11: a cat is playing\n",
            "ACOmKiJDkA4_130_144: a woman is cutting a potato\n",
            "L6dEUQ6WiHY_136_142: a man is riding a horse\n",
            "XEOH3t5AYY0_22_29: a man is riding a horse\n",
            "GPT4vUwlrXY_1_10: a man is riding a horse\n",
            "2YhDTpzxd3c_223_232: a man is riding a horse\n",
            "wpUT0DNB3qs_9_19: a cat is playing\n",
            "tJHUH9tpqPg_113_118: a woman is cutting a potato\n",
            "vpR0L3sIvoo_15_49: a cat is riding a horse\n",
            "v_tGw5gwlEw_136_152: a man is playing\n",
            "Hd-NeIhbYGc_28_31: a man is riding a horse\n",
            "EA-J-Y6QjUo_40_61: a cat is playing a guitar\n",
            "d1zdJO3CqVw_1_35: a dog is playing\n",
            "zHy7pM0U49w_103_109: a man is cutting a potato\n",
            "QhRqJp-CgEs_7_14: a dog is playing\n",
            "o4OsYxsNGMI_77_82: a man is playing a guitar\n",
            "xxHx6s_DbUo_121_128: a man is playing a guitar\n",
            "ptHkvYrH9fY_2_10: a man is playing a guitar\n",
            "Kf8oZGHDTt4_4_13: a man is riding a horse\n",
            "cR2yi-JnGcQ_8_16: a dog is playing\n",
            "PeUHy0A1GF0_105_112: a woman is cutting a potato\n",
            "tzd3AYTZq0U_0_6: a cat is playing\n",
            "c_XV7nPoRg8_2_12: a man is playing a guitar\n",
            "SzEbtbNSg04_71_93: a woman is cutting a potato\n",
            "NjCqtzZ3OtU_62_67: a woman is cutting a potato\n",
            "Cx1We6GX4IM_0_8: a cat is playing\n",
            "UXs3eq68ZjE_498_504: a woman is cutting a potato\n",
            "yyxtyCaEVqk_329_360: a man is cutting a potato\n",
            "iLr7ZHAq1ro_7_11: a man is riding a horse\n",
            "32_A0-er4Ws_16_20: a man is cutting a potato\n",
            "FH8vgAp4VDU_10_14: a man is riding a horse\n",
            "3I9tSDU-OSk_17_22: a woman is playing a guitar\n",
            "Nd45qJn61Dw_0_10: a cat is playing\n",
            "n_Z0-giaspE_62_78: a cat is playing a guitar\n",
            "wsj_dzz33Ko_31_41: a woman is cutting a potato\n",
            "F1BwUJ4--Qw_21_38: a man is playing a guitar\n",
            "VxM96IYzw0Q_2_15: a man is walking\n",
            "z0zb--BOhDY_16_22: a man is riding a horse\n",
            "kKGehSw5ht8_39_45: a man is walking\n",
            "0ONJfp95yoE_1_10: a cat is playing\n",
            "fEsrO_poIUg_161_168: a man is playing a guitar\n",
            "UJKythlXAIY_17_21: a man is playing a guitar\n",
            "jMO3jGQeqyk_3_6: a man is riding a horse\n",
            "hJuqBDw_TT4_105_112: a man is riding a horse\n",
            "nLz0QuerH1c_8_15: a cat is playing\n",
            "27mxaePkYB4_8_14: a man is riding a horse\n",
            "rVFJzN20jhQ_27_32: a man is playing a guitar\n",
            "6eokrw6_bjU_1_9: a man is playing a guitar\n",
            "6t0BpjwYKco_46_53: a woman is cutting a potato\n",
            "oFUsn1owAbs_158_168: a man is riding a horse\n",
            "UXs3eq68ZjE_250_255: a woman is cutting a potato\n",
            "GlW6n43pUQA_0_12: a man is dancing\n",
            "uAaWVeaYLdQ_1_12: a man is riding a horse\n",
            "0I4VKj4d0WI_24_40: a man is playing a guitar\n",
            "MF88IYF2MHY_25_59: a cat is playing\n",
            "Xxcv5jvRmcg_39_46: a man is playing a guitar\n",
            "OIjsSu_I4So_6_10: a man is walking\n",
            "Ms3QdGIzltU_10_20: a cat is playing\n",
            "Okg8QYhdtvU_73_92: a man is cutting a potato\n",
            "HkpUWzNNVt4_20_30: a man is playing a guitar\n",
            "j4dMnAPZu70_12_17: a dog is playing\n",
            "qeKX-N1nKiM_123_130: a woman is cutting a potato\n",
            "Li3umATsU0E_1_5: a cat is playing\n",
            "02Z-kuB3IaM_2_13: a man is walking\n",
            "0lh_UWF9ZP4_27_31: a woman is cutting a potato\n",
            "TF6LNmwDr_0_2_10: a cat is playing\n",
            "WPG-BIWOrG4_683_689: a man is cutting a guitar\n",
            "zlS1_zBYluY_15_21: a dog is playing\n",
            "AX38yo7Wuws_81_91: a man is cutting a potato\n",
            "8e0yXMa708Y_24_33: a man is riding a horse\n",
            "aQWCKcg0VRU_321_339: a woman is cutting a potato\n",
            "h0JvF9vpqx8_213_223: a woman is cutting a potato\n",
            "mmSQTI6gMNQ_15_21: a man is walking\n",
            "qRoxNXm7INc_58_66: a cat is playing\n",
            "BVjvRpmHg0w_47_51: a woman is cutting a potato\n",
            "7xNG7qGY9rw_14_29: a woman is cutting a potato\n",
            "r2oI9Y-3wAo_21_28: a dog is playing\n",
            "v-9Gx0gJmfo_26_32: a man is riding a horse\n",
            "JXSkYkiKO0I_112_116: a man is playing a guitar\n",
            "4JVpbYmqfcI_5_19: a woman is cutting a potato\n",
            "QzcTlEpAigQ_0_14: a cat is playing\n",
            "yYA7YXIKbg4_9_14: a cat is playing\n",
            "yId1rg5-ac0_40_50: a cat is playing\n",
            "ogcqFaNbah4_475_487: a man is dancing\n",
            "kWLNZzuo3do_24_31: a woman is cutting a potato\n",
            "INDhUwDMg-4_220_227: a man is riding a horse\n",
            "aN0WsBcja_E_0_15: a cat is playing\n",
            "-7KMZQEsJW4_205_208: a man is walking\n",
            "rV5VmhY3-Sw_65_76: a man is riding a horse\n",
            "Uc63MFVwfrs_360_372: a woman is cutting a potato\n",
            "OE6H_M52Sng_89_97: a woman is cutting a potato\n",
            "gGDtPJzh_0s_30_45: a woman is cutting a potato\n",
            "BAf3LXFUaGs_28_38: a man is playing a guitar\n",
            "m1NR0uNNs5Y_123_129: a man is cutting a potato\n",
            "nc0fVlaTYEs_34_44: a woman is cutting a potato\n",
            "J_evFB7RIKA_104_120: a man is cutting a potato\n",
            "Gw0FU733zzg_58_66: a dog is riding a horse\n",
            "PmrTDZy3f2M_35_39: a man is riding a horse\n",
            "ACOmKiJDkA4_67_74: a man is cutting a potato\n",
            "d7Gs0uGFLh0_5_13: a man is riding a horse\n",
            "0lh_UWF9ZP4_215_226: a woman is cutting a potato\n",
            "kSzS_lFtJDk_55_70: a man is playing a guitar\n",
            "TdYN7cePiRI_6_13: a man is playing a guitar\n",
            "nau1vCzyFQ4_37_54: a woman is cutting a potato\n",
            "4D1o4FC8YbA_29_39: a man is riding a horse\n",
            "tcxhOGyrCtI_15_21: a cat is playing\n",
            "uH8ObB_dyOk_159_166: a man is riding a horse\n",
            "05Gtb7_9tLU_0_9: a man is walking\n",
            "tYh4iDFgmEE_10_14: a woman is cutting a potato\n",
            "0lh_UWF9ZP4_183_190: a woman is cutting a potato\n",
            "YJ2aGe7CLBo_25_35: a man is playing a guitar\n",
            "Tm1HjqK0ABg_9_15: a man is playing a guitar\n",
            "2jkExrrm_sQ_19_25: a man is walking\n",
            "sRKQfxxEP4M_117_125: a man is walking\n",
            "BVjvRpmHg0w_121_130: a woman is cutting a potato\n",
            "-vg3vR86fu0_1_6: a man is walking\n",
            "gyOVZz7kXyM_1_10: a cat is playing\n",
            "O06WXS_XZN0_0_6: a man is walking\n",
            "vDhEpqfhVI0_0_8: a cat is playing\n",
            "N6SglZopfmk_97_111: a man is playing a guitar\n",
            "V4q5dQmpxIE_20_30: a man is playing a guitar\n",
            "OvRmRN1-O0Q_21_25: a man is riding a horse\n",
            "8MVo7fje_oE_108_113: a man is cutting a potato\n",
            "62I8I4sCPPQ_12_30: a man is riding a horse\n",
            "fqly5kyO2MI_10_20: a man is walking\n",
            "q8t7iSGAKik_57_74: a man is riding a horse\n",
            "lv8d_qLLqsk_1_20: a man is playing a guitar\n",
            "K-KVz3eqbnA_1_10: a woman is cutting a potato\n",
            "25NmudB2fqg_0_7: a man is playing\n",
            "xxHx6s_DbUo_82_86: a man is riding a horse\n",
            "4-1gpwkD7Hc_7_15: a dog is playing\n",
            "zfQOH4UGU_I_1_10: a dog is riding a horse\n",
            "2LgrGHWSy6k_122_135: a man is playing a guitar\n",
            "2G9fkvBzzQE_10_20: a man is riding a horse\n",
            "gHyXstpe_N8_140_150: a woman is cutting a potato\n",
            "4PayIIIOeIs_230_238: a man is playing a guitar\n",
            "CHFXTeQWXjo_206_212: a man is riding a horse\n",
            "nhm_APPwhWk_6_12: a man is walking\n",
            "Ugb_uH72d0I_8_17: a man is playing a guitar\n",
            "ItFqogTmAvQ_48_52: a man is dancing\n",
            "OUII8_aOfNM_60_69: a man is cutting a potato\n",
            "PiyoeFC31kE_9_27: a man is walking\n",
            "uB9zRlV47qA_17_23: a dog is playing\n",
            "H1Bok7Dg7g8_21_26: a dog is playing\n",
            "D1tTBncIsm8_248_254: a man is playing a guitar\n",
            "_O9kWD8nuRU_16_23: a woman is cutting a potato\n",
            "TFJ6oR89Vb8_90_107: a woman is cutting a potato\n",
            "XtQdAPV2UZs_4_11: a man is riding a horse\n",
            "3_51FhosiVY_93_101: a man is riding a horse\n",
            "7uOiiA4Kxbo_23_35: a man is riding a horse\n",
            "q8t7iSGAKik_11_31: a man is riding a horse\n",
            "rwHT2SuNOi8_195_201: a man is cutting a potato\n",
            "kquB3rIgfGk_537_544: a man is playing a guitar\n",
            "5zkCnHUnoYY_69_74: a woman is playing a guitar\n",
            "_o1UXSxTjfo_68_80: a man is riding a horse\n",
            "_WRC7HXBJpU_414_425: a woman is cutting a potato\n",
            "6t0BpjwYKco_59_69: a woman is cutting a potato\n",
            "W8l_ezoU8Lc_76_86: a man is playing a guitar\n",
            "omGWjiwxcTE_18_23: a man is riding a horse\n",
            "UoPU8F9mus8_258_262: a man is playing a guitar\n",
            "kWLNZzuo3do_228_232: a woman is cutting a potato\n",
            "fkONJEgTNJY_25_35: a dog is playing\n",
            "D1tTBncIsm8_198_205: a man is walking\n",
            "idRc_KkInds_0_6: a cat is playing\n",
            "m1NR0uNNs5Y_88_94: a man is cutting a potato\n",
            "EVqc7e8JCco_5_15: a dog is playing\n",
            "JsmHZlUgf1w_0_21: a cat is playing\n",
            "_7nP9z6T9m8_11_17: a man is riding a horse\n",
            "RiglOfJon8I_3_18: a man is riding a horse\n",
            "zYcY4mjLpxU_104_118: a man is riding a horse\n",
            "ymC2bNi6-Is_9_19: a man is playing a guitar\n",
            "aA05I5QaZnM_66_75: a man is playing a guitar\n",
            "DN7jwyL1Xgg_1_19: a man is walking\n",
            "zpgW7m7_LZw_2_15: a man is riding a horse\n",
            "JIKaIriiK8w_0_15: a cat is playing\n",
            "cUW_bXll6YM_390_395: a woman is cutting a potato\n",
            "FOOM-wA2rOY_77_86: a man is walking\n",
            "u9prcUCHlqM_503_511: a man is playing a guitar\n",
            "NbOlG7THecM_25_34: a man is playing a guitar\n",
            "C1MNCR1o9lU_0_7: a cat is playing\n",
            "dq7agmFWkq0_5_9: a cat is playing\n",
            "UbmZAe5u5FI_60_70: a woman is cutting a potato\n",
            "O9yLRzpPp44_36_55: a woman is cutting a potato\n",
            "hoinj6vyQ2g_8_16: a man is playing a guitar\n",
            "2YhDTpzxd3c_22_27: a man is riding a horse\n",
            "idXJu0BQRvo_2_6: a woman is riding a horse\n",
            "YAud4eS3DoA_54_65: a woman is cutting a potato\n",
            "n_Z0-giaspE_270_278: a man is walking\n",
            "1RchFBCT9JU_43_53: a cat is playing\n",
            "bXsKw3TOQXs_30_55: a woman is cutting a potato\n",
            "qqYysi3qotc_148_195: a woman is playing a guitar\n",
            "8sARjdhSvpo_167_176: a woman is cutting a potato\n",
            "htry5uxX0-Y_45_52: a man is walking\n",
            "KeSQ5Rv7eH8_73_79: a man is playing a guitar\n",
            "JAFGw0WRHHU_108_116: a man is playing a guitar\n",
            "hJFBXHtxKIc_225_230: a man is cutting a potato\n",
            "WTf5EgVY5uU_218_236: a woman is cutting a potato\n",
            "yU5sxW9bErQ_0_17: a man is riding a horse\n",
            "9Q0JfdP36kI_19_27: a man is cutting a potato\n",
            "Pgm-mOQu2MM_48_62: a man is riding a horse\n",
            "4lXdVS697DQ_10_25: a man is riding a horse\n",
            "OaBmAg6gz30_4_18: a man is cutting a potato\n",
            "hNECyt6Bo0A_5_10: a man is dancing\n",
            "ItFqogTmAvQ_240_246: a man is riding a horse\n",
            "gHzws6FpuNE_10_12: a man is walking\n",
            "6gQu8PWhFoQ_30_35: a man is walking\n",
            "4zPSShqsq-o_3_16: a cat is playing\n",
            "PeUHy0A1GF0_62_66: a woman is cutting a potato\n",
            "C_1cnNdMwxY_0_8: a man is playing\n",
            "73vUksucPz8_12_20: a man is playing a guitar\n",
            "JIoiUNOXGaI_8_13: a man is playing a guitar\n",
            "s6QwbmWbSmw_18_24: a man is riding a horse\n",
            "SzEbtbNSg04_214_224: a woman is cutting a potato\n",
            "0lh_UWF9ZP4_199_207: a woman is cutting a potato\n",
            "zSPBC8EO6dY_122_126: a man is riding a horse\n",
            "4VLrxtf7Z_8_0_7: a man is riding a horse\n",
            "BIqVvRh_cEY_143_149: a man is walking\n",
            "g1Gldu1KS44_8_14: a man is riding a horse\n",
            "urXDqw3S34I_12_17: a man is walking\n",
            "MWvCcwTw7Ac_154_181: a dog is playing\n",
            "m4D72WXFd8s_557_564: a man is playing a guitar\n",
            "05gNigkqfNU_78_84: a woman is cutting a potato\n",
            "dtn0PuxgfkM_0_5: a man is walking\n",
            "ZK4W-2ifl6I_1_28: a cat is playing a guitar\n",
            "O9cOSO9L8Zs_1_16: a man is walking\n",
            "_ZwwKOzpt2I_69_76: a man is cutting a potato\n",
            "xtbsD3PUua4_84_94: a woman is cutting a potato\n",
            "e-j59PqJjSM_163_173: a man is cutting a potato\n",
            "Y4gaJn0wlDE_0_10: a cat is playing\n",
            "UXs3eq68ZjE_246_250: a woman is cutting a potato\n",
            "Je3V7U5Ctj4_558_563: a man is cutting a potato\n",
            "tn-Hoz4KbkE_89_93: a dog is playing\n",
            "YZ0-6hdQ0mU_0_3: a man is playing a guitar\n",
            "wgrrQwLdME8_66_74: a man is playing a guitar\n",
            "G-M78KIy19E_315_330: a man is playing a guitar\n",
            "9HTUcMjWB3g_134_138: a woman is cutting a potato\n",
            "VahnQw2gTQY_274_280: a man is cutting a potato\n",
            "rXZy-PHtnxg_8_42: a man is playing a guitar\n",
            "qBFSt85-xqk_15_20: a woman is cutting a potato\n",
            "FCjpuJaUec0_19_26: a cat is playing\n",
            "WTf5EgVY5uU_108_116: a woman is cutting a potato\n",
            "TZ860P4iTaM_10_24: a cat is playing a guitar\n",
            "BOLKaQeB6j0_101_111: a woman is cutting a potato\n",
            "_WRC7HXBJpU_451_464: a man is cutting a potato\n",
            "jcRCn7MeSbo_71_82: a dog is playing\n",
            "Puv_4NtflqE_26_34: a woman is cutting a potato\n",
            "HZ-BuDDmvVk_0_10: a cat is playing\n",
            "MdOAr_4FJvc_5_15: a man is playing a guitar\n",
            "nV3Wv8iHp4U_0_38: a cat is playing\n",
            "edqyq4Q-7uU_103_109: a man is riding a horse\n",
            "0xx13BuvVmo_25_36: a woman is cutting a potato\n",
            "dtwXtwJByYk_5_14: a man is playing a guitar\n",
            "GyIs9B3A1Z0_0_7: a man is riding a horse\n",
            "PeUHy0A1GF0_114_121: a woman is cutting a potato\n",
            "dQmaVQZz7EE_1_18: a man is playing a guitar\n",
            "s_ldnx8_etY_32_54: a woman is cutting a potato\n",
            "L6dEUQ6WiHY_171_186: a man is cutting a potato\n",
            "g2IYQq7IkXc_23_32: a dog is walking\n",
            "UXs3eq68ZjE_313_318: a woman is cutting a potato\n",
            "BR4yQFZK9YM_101_110: a man is riding a horse\n",
            "iyAoiWeD53k_120_127: a woman is cutting a potato\n",
            "GWQTAe64m-0_160_166: a man is playing\n",
            "4cgzdXlJksU_83_90: a man is cutting a potato\n",
            "6t0BpjwYKco_200_205: a woman is cutting a potato\n",
            "Vg1jyL3cr60_163_186: a man is playing a guitar\n",
            "GMuijLIJH-U_1_10: a man is dancing\n",
            "8mSUD7JGNO0_198_204: a man is walking\n",
            "YXixZrSxefk_17_48: a man is cutting a potato\n",
            "dP15zlyra3c_0_10: a cat is playing\n",
            "lAznAeFFldg_6_10: a man is riding a horse\n",
            "vub04F8CWng_32_41: a man is playing\n",
            "n_Z0-giaspE_168_193: a man is riding a horse\n",
            "39Ce7I6nXIw_96_104: a man is riding a horse\n",
            "5fDBl1wT2Lk_34_39: a woman is cutting a potato\n",
            "s-XjRDsYuzU_0_12: a man is cutting a potato\n",
            "NFxWwI0J3As_78_84: a man is walking\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_caption_from_video(video_path):\n",
        "    feats = extract_resnet_features(video_path)  # [1, T, 2048]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _, pred_ids = model(feats, mode='inference')\n",
        "\n",
        "    caption = []\n",
        "    for idx in pred_ids[0]:\n",
        "        word = vocab_rev.get(int(idx), '<UNK>')\n",
        "        if word == '<EOS>':\n",
        "            break\n",
        "        if word not in ['<SOS>', '<PAD>']:\n",
        "            caption.append(word)\n",
        "    return ' '.join(caption)\n"
      ],
      "metadata": {
        "id": "5rJn3kIFoMwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = \"/content/sample_data/t1.mp4\"\n",
        "caption = generate_caption_from_video(video_path)\n",
        "print(\" Caption:\", caption)\n"
      ],
      "metadata": {
        "id": "7Cy6-SzCoNyV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "ee06dc3b-883e-438c-bdaa-0b3b193ffd97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "not enough values to unpack (expected 3, got 2)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-3f314e9b271f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvideo_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/sample_data/t1.mp4\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcaption\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_caption_from_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" Caption:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaption\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-428325bfb724>\u001b[0m in \u001b[0;36mgenerate_caption_from_video\u001b[0;34m(video_path)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'inference'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mcaption\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/S2VTModel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, vid_feats, target_variable, mode, beam_width)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvid_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_variable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvid_feats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvid_feats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for file in os.listdir(\"/content/sample_data\"):\n",
        "    if file.endswith(\".mp4\") or file.endswith(\".avi\"):\n",
        "        cap = generate_caption_from_video(os.path.join(\"/content/sample_data\", file))\n",
        "        print(f\"{file}  {cap}\")\n"
      ],
      "metadata": {
        "id": "AaiALdKToPjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S3Ln2OqDpHsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2uEruyaVpHzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python-headless\n",
        "!pip install torch torchvision torchaudio\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85kcYHvZpH2i",
        "outputId": "5edcfaa2-4a53-4bb6-ea69-01ef4099d477"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python-headless) (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "from torch import nn\n",
        "from moviepy.editor import VideoFileClip\n",
        "\n",
        "# Assuming your S2VTModel.py is in the same directory or mounted from Drive\n",
        "from drive.MyDrive.S2VTModel import S2VTModel  # Adjust path as needed\n"
      ],
      "metadata": {
        "id": "Gzdt7QfJpLB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_path = '/content/drive/MyDrive/msvd_split/vocab.json'\n",
        "with open(vocab_path, 'r') as f:\n",
        "    vocab = json.load(f)\n",
        "vocab_rev = {v: k for k, v in vocab.items()}\n"
      ],
      "metadata": {
        "id": "5cRPJjN7pOtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "resnet = models.resnet152(pretrained=True)\n",
        "resnet = nn.Sequential(*list(resnet.children())[:-1])\n",
        "resnet.eval().to(device)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n"
      ],
      "metadata": {
        "id": "PhbE8_vTpThr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features_from_video(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frame_features = []\n",
        "    frame_count = 0\n",
        "    while cap.isOpened() and frame_count < 40:  # Limit to 40 frames\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        img = transform(Image.fromarray(img)).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            feat = resnet(img).squeeze().cpu().numpy()\n",
        "        frame_features.append(feat)\n",
        "        frame_count += 1\n",
        "    cap.release()\n",
        "\n",
        "    # Pad or truncate to 40\n",
        "    frame_features = frame_features[:40]\n",
        "    while len(frame_features) < 40:\n",
        "        frame_features.append(np.zeros(2048))\n",
        "    return torch.tensor(frame_features).unsqueeze(0).float().to(device)\n"
      ],
      "metadata": {
        "id": "jtoqJCtLpXPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features_from_video(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frame_features = []\n",
        "    frame_count = 0\n",
        "    while cap.isOpened() and frame_count < 40:  # Limit to 40 frames\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        img = transform(Image.fromarray(img)).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            feat = resnet(img).squeeze().cpu().numpy()\n",
        "        frame_features.append(feat)\n",
        "        frame_count += 1\n",
        "    cap.release()\n",
        "\n",
        "    # Pad or truncate to 40\n",
        "    frame_features = frame_features[:40]\n",
        "    while len(frame_features) < 40:\n",
        "        frame_features.append(np.zeros(2048))\n",
        "    return torch.tensor(frame_features).unsqueeze(0).float().to(device)\n"
      ],
      "metadata": {
        "id": "QPbD6ZpDpdSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "video_path = '/content/sample_data/t1.mp4'  # Adjust as needed\n",
        "video_feats = extract_features_from_video(video_path)\n",
        "\n",
        "with torch.no_grad():\n",
        "    _, predicted_ids = model(video_feats, mode='inference')  # [1, T]\n",
        "\n",
        "# Decode prediction\n",
        "generated_caption = []\n",
        "for idx in predicted_ids[0]:\n",
        "    word = vocab_rev.get(int(idx), '<UNK>')\n",
        "    if word == '<EOS>':\n",
        "        break\n",
        "    if word not in ['<SOS>', '<PAD>']:\n",
        "        generated_caption.append(word)\n",
        "\n",
        "caption = ' '.join(generated_caption)\n",
        "print(\" Generated Caption:\", caption)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgSHctPYpgTl",
        "outputId": "535b863d-bebc-437f-94cf-1ad1bd63c149"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Generated Caption: a man is playing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d8ogPZ1wuPWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "video_path = '/content/sample_data/t1.mp4'  # Adjust as needed\n",
        "video_feats = extract_features_from_video(video_path)\n",
        "\n",
        "with torch.no_grad():\n",
        "    _, predicted_ids = model(video_feats, mode='inference')  # [1, T]\n",
        "\n",
        "# Decode prediction\n",
        "generated_caption = []\n",
        "for idx in predicted_ids[0]:\n",
        "    word = vocab_rev.get(int(idx), '<UNK>')\n",
        "    if word == '<EOS>':\n",
        "        break\n",
        "    if word not in ['<SOS>', '<PAD>']:\n",
        "        generated_caption.append(word)\n",
        "\n",
        "caption = ' '.join(generated_caption)\n",
        "print(\" Generated Caption:\", caption)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a70888f2-85dc-42b2-dfbd-a4356520eb73",
        "id": "78zwqenVuPvJ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Generated Caption: a man is playing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sCLCo3WGppKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "video_path = '/content/sample_data/04.mp4'  # Adjust as needed\n",
        "video_feats = extract_features_from_video(video_path)\n",
        "\n",
        "with torch.no_grad():\n",
        "    _, predicted_ids = model(video_feats, mode='inference')  # [1, T]\n",
        "\n",
        "# Decode prediction\n",
        "generated_caption = []\n",
        "for idx in predicted_ids[0]:\n",
        "    word = vocab_rev.get(int(idx), '<UNK>')\n",
        "    if word == '<EOS>':\n",
        "        break\n",
        "    if word not in ['<SOS>', '<PAD>']:\n",
        "        generated_caption.append(word)\n",
        "\n",
        "caption = ' '.join(generated_caption)\n",
        "print(\" Generated Caption:\", caption)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "433624e8-70aa-4444-bf56-4d27d2530466",
        "id": "Dl7ZBjDcppcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Generated Caption: a man is playing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = '/content/_6OTzzK7t9Y_73_78.avi'\n"
      ],
      "metadata": {
        "id": "_ApAK8H3qL7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_feats = extract_features_from_video(video_path)\n",
        "\n",
        "with torch.no_grad():\n",
        "    _, predicted_ids = model(video_feats, mode='inference')\n",
        "\n",
        "generated_caption = []\n",
        "for idx in predicted_ids[0]:\n",
        "    word = vocab_rev.get(int(idx), '<UNK>')\n",
        "    if word == '<EOS>':\n",
        "        break\n",
        "    if word not in ['<SOS>', '<PAD>']:\n",
        "        generated_caption.append(word)\n",
        "\n",
        "caption = ' '.join(generated_caption)\n",
        "print(\" Caption for AVI video:\", caption)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acyITMeZqpw-",
        "outputId": "40a720c7-e4ba-4157-df4a-e4d3229cafb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Caption for AVI video: a woman is cutting a potato\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = '/content/_1vy2HIN60A_32_40.avi'\n",
        "\n",
        "video_feats = extract_features_from_video(video_path)\n",
        "\n",
        "with torch.no_grad():\n",
        "    _, predicted_ids = model(video_feats, mode='inference')\n",
        "\n",
        "generated_caption = []\n",
        "for idx in predicted_ids[0]:\n",
        "    word = vocab_rev.get(int(idx), '<UNK>')\n",
        "    if word == '<EOS>':\n",
        "        break\n",
        "    if word not in ['<SOS>', '<PAD>']:\n",
        "        generated_caption.append(word)\n",
        "\n",
        "caption = ' '.join(generated_caption)\n",
        "print(\" Caption for AVI video:\", caption)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iiv6r4CdrJZQ",
        "outputId": "fbac42dd-f567-41ca-c13c-66abe0c748d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Caption for AVI video: a man is playing a guitar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ByGCPT9crfeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = '/content/04.mp4'\n",
        "\n",
        "video_feats = extract_features_from_video(video_path)\n",
        "\n",
        "with torch.no_grad():\n",
        "    _, predicted_ids = model(video_feats, mode='inference')\n",
        "\n",
        "generated_caption = []\n",
        "for idx in predicted_ids[0]:\n",
        "    word = vocab_rev.get(int(idx), '<UNK>')\n",
        "    if word == '<EOS>':\n",
        "        break\n",
        "    if word not in ['<SOS>', '<PAD>']:\n",
        "        generated_caption.append(word)\n",
        "\n",
        "caption = ' '.join(generated_caption)\n",
        "print(\" Caption for AVI video:\", caption)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPOAawQvrf-d",
        "outputId": "6e993f65-23af-4bf0-d854-7295baa47a8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Caption for AVI video: a man is riding a horse\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6Ptozg8frtKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = '/content/_6OTzzK7t9Y_73_78.avi'\n",
        "\n",
        "video_feats = extract_features_from_video(video_path)\n",
        "\n",
        "with torch.no_grad():\n",
        "    _, predicted_ids = model(video_feats, mode='inference')\n",
        "\n",
        "generated_caption = []\n",
        "for idx in predicted_ids[0]:\n",
        "    word = vocab_rev.get(int(idx), '<UNK>')\n",
        "    if word == '<EOS>':\n",
        "        break\n",
        "    if word not in ['<SOS>', '<PAD>']:\n",
        "        generated_caption.append(word)\n",
        "\n",
        "caption = ' '.join(generated_caption)\n",
        "print(\" Caption for AVI video:\", caption)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0766e24-c877-4398-e409-ef67ac1f2343",
        "id": "o4l2cNZBrvrg"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Caption for AVI video: a woman is cutting a potato\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GDvF2r2vsD4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = '/content/aa.mp4'\n",
        "\n",
        "video_feats = extract_features_from_video(video_path)\n",
        "\n",
        "with torch.no_grad():\n",
        "    _, predicted_ids = model(video_feats, mode='inference')\n",
        "\n",
        "generated_caption = []\n",
        "for idx in predicted_ids[0]:\n",
        "    word = vocab_rev.get(int(idx), '<UNK>')\n",
        "    if word == '<EOS>':\n",
        "        break\n",
        "    if word not in ['<SOS>', '<PAD>']:\n",
        "        generated_caption.append(word)\n",
        "\n",
        "caption = ' '.join(generated_caption)\n",
        "print(\" Caption for AVI video:\", caption)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff5c8bcc-5580-45bf-c1c6-e9e9eb9ae3c3",
        "id": "ZPlSTE7rsEMy"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Caption for AVI video: a man is dancing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s2dD4ipTsIW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = '/content/-_hbPLsZvvo_19_26.avi'\n",
        "\n",
        "video_feats = extract_features_from_video(video_path)\n",
        "\n",
        "with torch.no_grad():\n",
        "    _, predicted_ids = model(video_feats, mode='inference')\n",
        "\n",
        "generated_caption = []\n",
        "for idx in predicted_ids[0]:\n",
        "    word = vocab_rev.get(int(idx), '<UNK>')\n",
        "    if word == '<EOS>':\n",
        "        break\n",
        "    if word not in ['<SOS>', '<PAD>']:\n",
        "        generated_caption.append(word)\n",
        "\n",
        "caption = ' '.join(generated_caption)\n",
        "print(\" Caption for AVI video:\", caption)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rhIZ4gEsLJi",
        "outputId": "9621f9ad-c0a6-4176-ad19-24c6f180ec75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Caption for AVI video: a woman is cutting a potato\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tRFBlbiVuY3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = '/content/-_hbPLsZvvo_19_26.avi'\n",
        "\n",
        "video_feats = extract_features_from_video(video_path)\n",
        "\n",
        "with torch.no_grad():\n",
        "    _, predicted_ids = model(video_feats, mode='inference')\n",
        "\n",
        "generated_caption = []\n",
        "for idx in predicted_ids[0]:\n",
        "    word = vocab_rev.get(int(idx), '<UNK>')\n",
        "    if word == '<EOS>':\n",
        "        break\n",
        "    if word not in ['<SOS>', '<PAD>']:\n",
        "        generated_caption.append(word)\n",
        "\n",
        "caption = ' '.join(generated_caption)\n",
        "print(\" Caption for AVI video:\", caption)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9320ac3b-95c4-4689-b910-9d3b4aa82417",
        "id": "ng-Ec-PDuZPc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Caption for AVI video: a man is playing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kJ_O-CUmub39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = '/content/6t0BpjwYKco_118_127.avi'\n",
        "\n",
        "video_feats = extract_features_from_video(video_path)\n",
        "\n",
        "with torch.no_grad():\n",
        "    _, predicted_ids = model(video_feats, mode='inference')\n",
        "\n",
        "generated_caption = []\n",
        "for idx in predicted_ids[0]:\n",
        "    word = vocab_rev.get(int(idx), '<UNK>')\n",
        "    if word == '<EOS>':\n",
        "        break\n",
        "    if word not in ['<SOS>', '<PAD>']:\n",
        "        generated_caption.append(word)\n",
        "\n",
        "caption = ' '.join(generated_caption)\n",
        "print(\" Caption for AVI video:\", caption)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46acc88f-7415-4997-d97e-ebdbdce5d8ec",
        "id": "tMQBqeZlucJS"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Caption for AVI video: a woman is playing a guitar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "axDA2vazsLz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = '/content/video0.mp4'\n",
        "\n",
        "video_feats = extract_features_from_video(video_path)\n",
        "\n",
        "with torch.no_grad():\n",
        "    _, predicted_ids = model(video_feats, mode='inference')\n",
        "\n",
        "generated_caption = []\n",
        "for idx in predicted_ids[0]:\n",
        "    word = vocab_rev.get(int(idx), '<UNK>')\n",
        "    if word == '<EOS>':\n",
        "        break\n",
        "    if word not in ['<SOS>', '<PAD>']:\n",
        "        generated_caption.append(word)\n",
        "\n",
        "caption = ' '.join(generated_caption)\n",
        "print(\" Caption for AVI video:\", caption)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M86UY0CbsMFQ",
        "outputId": "82eecfa4-5072-4aea-fbf2-35172aed360c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Caption for AVI video: a man is riding a horse\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = '/content/video1.mp4'\n",
        "\n",
        "video_feats = extract_features_from_video(video_path)\n",
        "\n",
        "with torch.no_grad():\n",
        "    _, predicted_ids = model(video_feats, mode='inference')\n",
        "\n",
        "generated_caption = []\n",
        "for idx in predicted_ids[0]:\n",
        "    word = vocab_rev.get(int(idx), '<UNK>')\n",
        "    if word == '<EOS>':\n",
        "        break\n",
        "    if word not in ['<SOS>', '<PAD>']:\n",
        "        generated_caption.append(word)\n",
        "\n",
        "caption = ' '.join(generated_caption)\n",
        "print(\" Caption for AVI video:\", caption)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kf4IE3sZsMmX",
        "outputId": "1e840364-7176-48e9-c716-ea0e42cf0692"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Caption for AVI video: a man is cutting a potato\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = '/content/video2.mp4'\n",
        "\n",
        "video_feats = extract_features_from_video(video_path)\n",
        "\n",
        "with torch.no_grad():\n",
        "    _, predicted_ids = model(video_feats, mode='inference')\n",
        "\n",
        "generated_caption = []\n",
        "for idx in predicted_ids[0]:\n",
        "    word = vocab_rev.get(int(idx), '<UNK>')\n",
        "    if word == '<EOS>':\n",
        "        break\n",
        "    if word not in ['<SOS>', '<PAD>']:\n",
        "        generated_caption.append(word)\n",
        "\n",
        "caption = ' '.join(generated_caption)\n",
        "print(\" Caption for AVI video:\", caption)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90YXadfKswns",
        "outputId": "0e249f78-ce1d-4aac-f76f-3662499f4d13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Caption for AVI video: a man is playing a guitar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = '/content/video3.mp4'\n",
        "\n",
        "video_feats = extract_features_from_video(video_path)\n",
        "\n",
        "with torch.no_grad():\n",
        "    _, predicted_ids = model(video_feats, mode='inference')\n",
        "\n",
        "generated_caption = []\n",
        "for idx in predicted_ids[0]:\n",
        "    word = vocab_rev.get(int(idx), '<UNK>')\n",
        "    if word == '<EOS>':\n",
        "        break\n",
        "    if word not in ['<SOS>', '<PAD>']:\n",
        "        generated_caption.append(word)\n",
        "\n",
        "caption = ' '.join(generated_caption)\n",
        "print(\" Caption for AVI video:\", caption)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAogmKwKs17f",
        "outputId": "25a425e1-65ad-4bce-bf32-253c1b229e3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Caption for AVI video: a man is walking\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = '/content/video4.mp4'\n",
        "\n",
        "video_feats = extract_features_from_video(video_path)\n",
        "\n",
        "with torch.no_grad():\n",
        "    _, predicted_ids = model(video_feats, mode='inference')\n",
        "\n",
        "generated_caption = []\n",
        "for idx in predicted_ids[0]:\n",
        "    word = vocab_rev.get(int(idx), '<UNK>')\n",
        "    if word == '<EOS>':\n",
        "        break\n",
        "    if word not in ['<SOS>', '<PAD>']:\n",
        "        generated_caption.append(word)\n",
        "\n",
        "caption = ' '.join(generated_caption)\n",
        "print(\" Caption for AVI video:\", caption)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jz5Ht2T2s33y",
        "outputId": "f92586bf-592f-4987-e1ec-d51ea93549b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Caption for AVI video: a woman is cutting a potato\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = '/content/video10.mp4'\n",
        "\n",
        "video_feats = extract_features_from_video(video_path)\n",
        "\n",
        "with torch.no_grad():\n",
        "    _, predicted_ids = model(video_feats, mode='inference')\n",
        "\n",
        "generated_caption = []\n",
        "for idx in predicted_ids[0]:\n",
        "    word = vocab_rev.get(int(idx), '<UNK>')\n",
        "    if word == '<EOS>':\n",
        "        break\n",
        "    if word not in ['<SOS>', '<PAD>']:\n",
        "        generated_caption.append(word)\n",
        "\n",
        "caption = ' '.join(generated_caption)\n",
        "print(\" Caption for AVI video:\", caption)"
      ],
      "metadata": {
        "id": "nF4xtaAisM2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6qjYIlcos2W2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Iej2A4X1s2ay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PNzSET0qs2e2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yJ7d09uZs2iy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "video_path = '/content/6t0BpjwYKco_118_127.avi'  # Adjust as needed\n",
        "video_feats = extract_features_from_video(video_path)\n",
        "\n",
        "with torch.no_grad():\n",
        "    _, predicted_ids = model(video_feats, mode='inference')  # [1, T]\n",
        "\n",
        "# Decode prediction\n",
        "generated_caption = []\n",
        "for idx in predicted_ids[0]:\n",
        "    word = vocab_rev.get(int(idx), '<UNK>')\n",
        "    if word == '<EOS>':\n",
        "        break\n",
        "    if word not in ['<SOS>', '<PAD>']:\n",
        "        generated_caption.append(word)\n",
        "\n",
        "caption = ' '.join(generated_caption)\n",
        "print(\" Generated Caption:\", caption)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "c2448115-d396-4454-d825-1c2e38bbd8ee",
        "id": "L-pMvCx7s3Jp"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'extract_features_from_video' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0553e9c4d49f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvideo_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/6t0BpjwYKco_118_127.avi'\u001b[0m  \u001b[0;31m# Adjust as needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mvideo_feats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features_from_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'extract_features_from_video' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fAiv6dgHtibS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = '/content/6t0BpjwYKco_118_127.avi'\n",
        "\n",
        "video_feats = extract_features_from_video(video_path)\n",
        "\n",
        "with torch.no_grad():\n",
        "    _, predicted_ids = model(video_feats, mode='inference')\n",
        "\n",
        "generated_caption = []\n",
        "for idx in predicted_ids[0]:\n",
        "    word = vocab_rev.get(int(idx), '<UNK>')\n",
        "    if word == '<EOS>':\n",
        "        break\n",
        "    if word not in ['<SOS>', '<PAD>']:\n",
        "        generated_caption.append(word)\n",
        "\n",
        "caption = ' '.join(generated_caption)\n",
        "print(\" Caption for AVI video:\", caption)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "a83b3b6e-a076-421c-adab-a22629c7fbd5",
        "id": "qRv_hcfpti6i"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:<ipython-input-12-592e6418fbd0>:21: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
            "  return torch.tensor(frame_features).unsqueeze(0).float().to(device)\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-d587e127fdc8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'inference'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mgenerated_caption\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "11x3Zj5Fujir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rXY6s6u4ukLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TUHl9ErwukP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = '/content/6t0BpjwYKco_118_127.avi'\n",
        "\n",
        "video_feats = extract_features_from_video(video_path)\n",
        "\n",
        "with torch.no_grad():\n",
        "    _, predicted_ids = model(video_feats, mode='inference')\n",
        "\n",
        "generated_caption = []\n",
        "for idx in predicted_ids[0]:\n",
        "    word = vocab_rev.get(int(idx), '<UNK>')\n",
        "    if word == '<EOS>':\n",
        "        break\n",
        "    if word not in ['<SOS>', '<PAD>']:\n",
        "        generated_caption.append(word)\n",
        "\n",
        "caption = ' '.join(generated_caption)\n",
        "print(\" Caption for AVI video:\", caption)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07dc6ddf-cd9b-4877-dcef-bb16bd454b1d",
        "id": "7pt5EnWYuklO"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Caption for AVI video: a woman is playing a guitar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mtJM2T7mumJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = '/content/_0nX-El-ySo_83_93.avi'\n",
        "\n",
        "video_feats = extract_features_from_video(video_path)\n",
        "\n",
        "with torch.no_grad():\n",
        "    _, predicted_ids = model(video_feats, mode='inference')\n",
        "\n",
        "generated_caption = []\n",
        "for idx in predicted_ids[0]:\n",
        "    word = vocab_rev.get(int(idx), '<UNK>')\n",
        "    if word == '<EOS>':\n",
        "        break\n",
        "    if word not in ['<SOS>', '<PAD>']:\n",
        "        generated_caption.append(word)\n",
        "\n",
        "caption = ' '.join(generated_caption)\n",
        "print(\" Caption for AVI video:\", caption)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3dc16af-dc30-4282-ef4c-60a9f4c02497",
        "id": "3hBRKfW_umcB"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Caption for AVI video: a man is cutting a potato\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6virXPbHurBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = '/content/1.avi'\n",
        "\n",
        "video_feats = extract_features_from_video(video_path)\n",
        "\n",
        "with torch.no_grad():\n",
        "    _, predicted_ids = model(video_feats, mode='inference')\n",
        "\n",
        "generated_caption = []\n",
        "for idx in predicted_ids[0]:\n",
        "    word = vocab_rev.get(int(idx), '<UNK>')\n",
        "    if word == '<EOS>':\n",
        "        break\n",
        "    if word not in ['<SOS>', '<PAD>']:\n",
        "        generated_caption.append(word)\n",
        "\n",
        "caption = ' '.join(generated_caption)\n",
        "print(\" Caption for AVI video:\", caption)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8f10cc1-1986-4ebf-d11d-efa66702f43a",
        "id": "Mph2jMG-urTI"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Caption for AVI video: a man is riding a horse\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CyFS2kRVu8Wm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = '/content/2.avi'\n",
        "\n",
        "video_feats = extract_features_from_video(video_path)\n",
        "\n",
        "with torch.no_grad():\n",
        "    _, predicted_ids = model(video_feats, mode='inference')\n",
        "\n",
        "generated_caption = []\n",
        "for idx in predicted_ids[0]:\n",
        "    word = vocab_rev.get(int(idx), '<UNK>')\n",
        "    if word == '<EOS>':\n",
        "        break\n",
        "    if word not in ['<SOS>', '<PAD>']:\n",
        "        generated_caption.append(word)\n",
        "\n",
        "caption = ' '.join(generated_caption)\n",
        "print(\" Caption for AVI video:\", caption)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75a782be-3446-4926-eb67-8604d9cd4d98",
        "id": "d_W1gxJbu8pq"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Caption for AVI video: a man is riding a horse\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lCmofodzu-Uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = '/content/3.avi'\n",
        "\n",
        "video_feats = extract_features_from_video(video_path)\n",
        "\n",
        "with torch.no_grad():\n",
        "    _, predicted_ids = model(video_feats, mode='inference')\n",
        "\n",
        "generated_caption = []\n",
        "for idx in predicted_ids[0]:\n",
        "    word = vocab_rev.get(int(idx), '<UNK>')\n",
        "    if word == '<EOS>':\n",
        "        break\n",
        "    if word not in ['<SOS>', '<PAD>']:\n",
        "        generated_caption.append(word)\n",
        "\n",
        "caption = ' '.join(generated_caption)\n",
        "print(\" Caption for AVI video:\", caption)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6ca8710-83a9-4578-c444-950269ba39dd",
        "id": "ZchZmKrzu-tn"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Caption for AVI video: a cat is playing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y-iVkHP_vBu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = '/content/4.avi'\n",
        "\n",
        "video_feats = extract_features_from_video(video_path)\n",
        "\n",
        "with torch.no_grad():\n",
        "    _, predicted_ids = model(video_feats, mode='inference')\n",
        "\n",
        "generated_caption = []\n",
        "for idx in predicted_ids[0]:\n",
        "    word = vocab_rev.get(int(idx), '<UNK>')\n",
        "    if word == '<EOS>':\n",
        "        break\n",
        "    if word not in ['<SOS>', '<PAD>']:\n",
        "        generated_caption.append(word)\n",
        "\n",
        "caption = ' '.join(generated_caption)\n",
        "print(\" Caption for AVI video:\", caption)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54950b27-bba9-4394-cf3b-2fcba164aaf4",
        "id": "GqRO1pBqvCLM"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Caption for AVI video: a dog is playing a guitar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kURkCOo_vGGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = '/content/5.avi'\n",
        "\n",
        "video_feats = extract_features_from_video(video_path)\n",
        "\n",
        "with torch.no_grad():\n",
        "    _, predicted_ids = model(video_feats, mode='inference')\n",
        "\n",
        "generated_caption = []\n",
        "for idx in predicted_ids[0]:\n",
        "    word = vocab_rev.get(int(idx), '<UNK>')\n",
        "    if word == '<EOS>':\n",
        "        break\n",
        "    if word not in ['<SOS>', '<PAD>']:\n",
        "        generated_caption.append(word)\n",
        "\n",
        "caption = ' '.join(generated_caption)\n",
        "print(\" Caption for AVI video:\", caption)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d81bbeef-28cb-4890-cc6e-edd1635b5892",
        "id": "eLImY1J1vG0E"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Caption for AVI video: a man is cutting a potato\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jgq0IX8CvJjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = '/content/video1.mp4'\n",
        "\n",
        "video_feats = extract_features_from_video(video_path)\n",
        "\n",
        "with torch.no_grad():\n",
        "    _, predicted_ids = model(video_feats, mode='inference')\n",
        "\n",
        "generated_caption = []\n",
        "for idx in predicted_ids[0]:\n",
        "    word = vocab_rev.get(int(idx), '<UNK>')\n",
        "    if word == '<EOS>':\n",
        "        break\n",
        "    if word not in ['<SOS>', '<PAD>']:\n",
        "        generated_caption.append(word)\n",
        "\n",
        "caption = ' '.join(generated_caption)\n",
        "print(\" Caption for AVI video:\", caption)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4smGcTtpvSXE",
        "outputId": "dbce42e1-e1fc-4bf7-bc54-629ddb661cef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Caption for AVI video: a man is cutting a potato\n"
          ]
        }
      ]
    }
  ]
}